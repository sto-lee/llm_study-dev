[Music] You can kind of think of an AI model as a vegetable you want to grow in your garden.  Before you buy the seeds, you need to research the weather and water requirements for that plant, or else it might die before it ever flowers.  And as it grows you need to evaluate and reoptimize the care you’re providing so that it can thrive and bloom.  And for an entire garden, you need to do that with every vegetable and make sure that none of them interact with each other harmfully.  Which is important because you need all of those different vegetables in order to survive.  You can’t live on carrots alone.  Welcome to AI Academy. I’m Nicholas Renotte and I’m not a gardener, but I am Chief AI Engineer at IBM Client Engineering.  If you want your AI garden to grow, you need to ensure that you’re using a variety of vegetables, or in this case, multiple models.  That’s what we call a multi-model approach, where you have a variety of models for your AI use cases.  This means you can pick and choose from different models to find the right one for the right use case, ...which gives you the opportunity to look at how each of those models is designed as you find the right fit.  You need to ask specific important questions. Who built it? What data was it trained on? What guardrails are in place for it? What risks and regulations do you need to consider and account for?  The other challenge when it comes to finding the right model for the right use case is identifying the best use case to fit your business needs.  And that begins with a prompt.  A prompt is a textual input or instruction that goes into a large language model to set up the basics of the AI.  What a good prompt does is clearly articulate your use case and the problem you’re solving with AI.  So, the first step in the process of choosing a model for your use case is writing a very specific prompt, ...that captures use case; user problem; the ask of the technology; and the guardrails for what good looks like.  Next, you’ll research the available models, looking at things like model size, performance, costs, risks and deployment methods.  You can then use the information you’ve collected to evaluate those models, ...against your prompt and identify which of them you first want to test.  Start with a large model and work with it until you satisfy your original prompt.  Then, try to duplicate the result using smaller models.  You’re essentially passing the same prompt through different models to experiment and see which works best.  That enables you to choose the best model for the use case, but it’s not the end of the process.  You’re going to want to continually evaluate and govern that model with ongoing testing, ...so you assess how it’s working based on performance and cost benchmarks.  It’s like that idea of the garden. You need to tend to it, not just plant the seeds and hope for the best.  And part of that ongoing care is to continually update the data and the prompt, where needed, to keep it relevant, ...and also test new models as they become available.  You don’t want to stick to just one model forever and get locked in as situations change, both inside and outside your business.  Now all throughout this process you want to be constantly thinking about the factors that affect your choice of model.  In addition to the three elements of performance; accuracy; reliability; and speed, ...you also want to consider size; deployment method; transparency; and any potential risks.  All of those need to be considered as you choose the right mode for your use case and then start to implement it.  That implementation is going to require a team that not only crosses disciplines, but also crosses lines of business.  Don’t think of it as proprietary to any one department, ...but treat it as a distinctly collaborative project that takes multiple teams to get up and running.  Ensure that this team is ready and able to diagnose performance benchmarks, ...each of which measures something unique and produces a dataset that shows how everything is being calculated.  Without this you can’t make informed decisions about this and future models and use cases.  And remember, even once your little AI model crop is growing happily, you need to keep taking care of it.  In this case that means continuous testing, governance, and optimization, ...all of which are essential to keep that model up to date and running optimally.  Remember, models evolve. So your strategy and choices need to do the same.  You want to keep growing towards the sun instead of withering on the vine.  Check out the AI Academy archives for deep dives into other aspects of AI for Business, and watch this space for future episodes.  [Music]