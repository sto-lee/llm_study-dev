{
  "video_url": "https://www.youtube.com/watch?v=TabyC8otFY8",
  "video_id": "TabyC8otFY8",
  "title": "AI and Large Language Models Boost Language Translation",
  "upload_date": "20231117",
  "channel": "IBM Technology",
  "duration": "6:19",
  "caption": "Hello.  [foreign language...]  Unless you know Hindi,\nyou wouldn't understand what I just said. What I   said was I wanted to tell you something very \nimportant-- unless you know my language, you   can't understand it. All of you must know and must \nhave experienced LLMs (large language models) in   the recent times. Large language models are \nvery popularly known for generating text, but   it is also important to know that LLMs can also \ndo a very good job of translating languages. Why   is this important? It seems only about 25% of the \nInternet users--their primary language is English.   And more than 65% of the users on the Internet \nprefer to be provided information in their   primary languages--respective primary languages. \nAlso, more than 70% of Internet users would like   to receive support, issue resolution, etc. in \ntheir preferred languages. Now, because they do   not receive the help in their primary languages, \nmore than 65% of these Internet users are using   machine translations to get the help that they \nneed. So it seems that machine translations are   essential for us to do business. So I'm going to \nexplain machine translations in two parts. First,   I will talk about how we have been doing machine \ntranslation so far, and then I will jump to the   advantage of the large language models and how we \nare going to do translations from them. So let's   see how machine translations are done. Machine \ntranslations use artificial intelligence to   translate between languages automatically without \nany human help. So how do they do that? Let's take   an example here: English, Spanish, and Japanese. \nTo translate between any of these languages,   you need linguistic rules and you need \ndictionaries for each of these languages.   And the machine translation are done in multiple \napproaches. The rule based approach--the rule   based approach is the one that predominantly \nuses the linguistic rules and the dictionaries   and also the parallel dictionaries that have the \nmeanings of two different languages, the source   language as well as the target language. And then \nwe have the second approach called the statistical   approach. It takes a totally different approach \nof leveraging the human translations and learning   the patterns from them and making very smart \nguesses of those translations and delivering   those translations. Both approaches work very, \nvery well by the way. We take it one notch up   with the neural approach where, as in rule \nbased and statistical, it actually is looking   at each word to get to the translations. Neural \ntakes it one notch up because it is actually   looking at the sentence constructions to do the \ntranslations. Now, as in any other approach,   you can take a combination of these approaches and \nmake it a hybrid approach. So as we discussed, the   traditional way makes use of the linguistic rules \nas well as the dictionaries. And it goes through   the supervised learning into one. Large language \nmodels do the translations differently. They make   use of the content that is already available in \ndifferent languages. We call it the large corpus   of parallel text. That is the examples of the same \ntext in different languages like English, Spanish,   Japanese and so on. And we feed it to the models. \nSo the large language models, as you all know,   use the transformer models and they have both the \nencoder and decoder capabilities. On top of that,   the large languages models typically make use of \ntwo approaches in doing the translations. Number   one is the sequence-to-sequence approach. \nAnd the sequence-to-sequence approach,   you can take an input text and feed it to the \nencoder. \"Hello, how are you?\" And the encoder   goes through the text and creates the semantic \nrepresentation of the text and also captures   the meaning of the text and passes it on to the \ndecoder. Now the decoder is capturing the semantic   representation and the meaning and translating \nit to the representative target language. You   say \"Hello, how are you?\" in English, and if \nyour target language is going to be Spanish,   you will get \"Hola, cómo estás\". The second \napproach-- also interesting --is the attention   model. The attention model is a little bit of a \nlazy model compared to the sequence-to-sequence   one. The attention model is focusing on the \nmain relevant vocabulary of the sentence. It   is not going through the entire sentence. So \nfor example, it can pick up \"hello\" and \"how   are you\" and focus on the \"hola\" and \"cómo \nestás\". But it is still going to capture the   meaning and the semantic representation essence \nthrough the encoded and decoder. As you can see,   the larger language models, instead of using \nthe linguistic rules and the dictionaries are   really focusing on capturing the patterns and the \nrelationships between the data and translating it.   It's quite obvious now that everybody wants to be \ncommunicated in their own language, including our   customers. Let's go leverage large language models \nto meet them at the table and communicate in their   own language. Thank you for watching. Before \nyou leave, please click subscribe and like."
}