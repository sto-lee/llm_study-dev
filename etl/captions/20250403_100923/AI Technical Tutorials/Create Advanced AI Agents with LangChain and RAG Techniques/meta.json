{
  "video_url": "https://www.youtube.com/watch?v=Y1PaM3edYoI",
  "video_id": "Y1PaM3edYoI",
  "title": "Create Advanced AI Agents with LangChain and RAG Techniques",
  "upload_date": "20241021",
  "channel": "IBM Technology",
  "duration": "6:47",
  "caption": "Hi, I'm Anna and this is how to perform\nagentic  retrieval augmented generation. In other words, agentic RAG. We'll need a few packages\nfor this tutorial. Make sure to install the following\nlibraries. Next, we'll import the following packages. Then we input our API key and project ID. We've set up our credentials\nusing a .env file. For this tutorial we're using IBM's granite 3.08b instruct model, but you're free to use any AI model of your choice. The purpose of these models is to serve as the reasoning engine that decides which actions to take. We'll set up a prompt template to ask multiple questions, and now we can set up a chain\nwith our prompt and our alarm. This allows the LLM to produce a response. Let's test our agent response to a basic question like what sport is played at the US open? Our agent responded correctly. Let's try something a little harder. Like where was the 2024 U.S. Open? The LLM is unable\nto answer this question. The training data used for this\nmodel is from before the 2024 U.S. Open happened, and without the appropriate tools, the agent doesn't have access\nto this information. The first step in creating our knowledge\nbase is listing the URLs. We will be getting content from. Here are a list of URLs summarizing IBM's involvement in the 2024 U.S. Open. Let's put them in a list called URLs. Next, weâ€™ll load the URLs as documents\nusing LangChain's web based loader. We'll also print a sample document\nto see how it loaded. Looks good. In order to split the data\nin these documents into chunks that can then be processed by the LLM,\nwe can use a text splitter. The embedding model that we are using is an IBM slate model through the watsonx.ai embedding service. Let's initialize it. In order to store or embedded documents. We will use Chroma DB,\nan open source vector store to access information in the vector store. We must set up a retriever. Let's define the get IBM US Open context function and tool. Our agent will be using the tools description helps the agent know when to call it. This tool can be used for routing questions to our vector store. If they're related to IBM's involvement\nin the 2024 US Open. Next, let's set up a new prompt template to ask multiple questions. This template is more complex. It's known as a structured chat prompt\nand can be used for creating agents\nto have multiple tools available. Our structured chat prompt\nwill be made up of a system prompt, a human prompt, and our RAG tool. First, we'll set up the system prompt. This prompt tells the agent to print\nits thought process, the tools that were used,\nand the final output. In the following code,\nwe're establishing the human prompt. This prompt tells the agent\nto display the user input, followed by the intermediate steps taken by the agent\nas part of the agent scratchpad. Next, we'll establish the order of our newly defined prompts in the prompt template. Now let's finalize our  prompt template\nby adding the tool names, descriptions, and arguments using a partial prompt template. An important feature of AI agents is\ntheir memory. Agents are able to store\npast conversations and past findings in their memory\nto improve the accuracy and relevance of their responses\ngoing forward. In our case, we'll be using LangChain conversation buffer memory as a means of memory storage. And now we can set up a chain\nwith our agents scratchpad memory prompt and the LLM. The agent executor class\nis used to execute the agent. We're now able to ask the agent questions. Remember how the agent\nwas previously unable to provide us with the information\nrelated to our queries? Now that the agent has its RAG tool\navailable to use, let's try asking the same questions again. Let's start with where was the 2020 for US Open? Great. The agent used its available RAG tool\nto return the location of the 2024 U.S. Open. We even get to see the exact document\nthat the agent is retrieving its information from. Now, let's try something harder. This time, our query will be about IBM's involvement in the 2024 US Open. Again, the agent was able\nto successfully retrieve the relevant information\nrelated to our question. Additionally, the agent is successfully\nupdating its knowledge base as it learns new information\nand experiences new interactions as seen by the history output. Now, let's test if the agent can determine when to calling isn't necessary to answer the user query. We can test this by asking the wrong agent a question\nthat is not about the US Open. Like what is the capital of France? As seen in the agent executor chain, the agent recognized that it had\nthe information in its knowledge base. To answer this question\nwithout using any of its tools. And that's it. In this tutorial, we created a RAG agent\nusing LangChain in Python with watsonx.ai. The LLM we worked with was an IBM granite 3.08 the Instruct model. The AI agent was successfully able\nto retrieve relevant information via a rogue tool, update each memory with each interaction and output responses. It's also important\nto note the agent's ability to discern whether tool calling is appropriate\nfor each specific task. For instance, when the agent already had the relevant information needed to answer a question about the capital of France, it didn't use any tool calling for question answering."
}