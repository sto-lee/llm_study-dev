{
  "video_url": "https://www.youtube.com/watch?v=b0brvO-rrMI",
  "video_id": "b0brvO-rrMI",
  "title": "Modernizing code with AI Code Assistants",
  "upload_date": "20240813",
  "channel": "IBM Technology",
  "duration": "21:47",
  "caption": "Everyone is under pressure to\nproduce faster and to do more with less, which often means\nthat programmers are behind the scenes reprograming millions of lines of code. And no biggie, but if they get it wrong,\nthe whole house of cards might just collapse. So how can you use generative\nAI to modernize your applications and make sure you don't break\nanything in the process? Let's see today on AI in action. In this series, we're going to explore\nwhat generative AI can and can't do, how it actually gets built. Responsible ways to put it into practice\nand the real business problems and solutions\nwe’ll encounter along the way. So welcome to AI in action,\nbrought to you by IBM. I'm Albert Lawrence. Now today we're going to talk about\nhow generative AI is transforming application development\nand making sure that what you spent years building doesn't break\nwhen you try to upgrade it. Today\nI'm joined by Miha Kralj and David Levy. Now, Miha is a senior partner\nof Cloud Build and Modernization at IBM and self-defined software\nengineering nerd. Welcome, Miha. Hey, Albert.\nHow are you today? I'm so good. I'm excited to get into this. And our other guest is David Lev, an advisory technical engineer\nfor IBM Client Engineering. He builds things, y'all. So why did I choose you to\nfor today's episode? Well,\nbecause you both geek out about the build. And today I want to explore\nhow to modernize your applications faster, at lower cost,\nand with less risk using generative AI. Plus, the different ways developers\ncan use it to make life easier. . So we're going to get into all of that. Let's start off with our first question\nthough. I want to direct this one to you, Miha. How does generative AI change\nthe application development cycle? And for the folks who haven't dipped their\ntoe in yet, what are they missing? Well,\nI typically compare how job of a software developer is changing the same way\nhow the job of car manufacturing changed. If you remember, in 50s,\ncar plants were typical conveyor belts and blue collar workers were welding on it\nand painting and putting glass in, and at the end, to have a human\nto do the inspection of the car. If you're looking at any modern car\nplant today, they're all robots, right? Robots are welding, robots are painting,\nrobots are putting glass in there, and robots are doing the final inspection\nbefore car is sold off. Now the question is, where are humans now? Well, humans are making robots\nthat make cars, and that's very similar to what's happening\nto a software development profession. Instead of directly making code,\nwe are going to make instructions, prompts to make code in the future. The second part is we actually sees that\nyou can use generative AI in the software development lifecycle, either\nfor good things or for silly things. Let me explain. Generative AI can generate new code,\nbut then if we let the generative AI to do that, somebody else probably needs to write the documentation,\nwrite the tests, do all of the things. And I saw in early stages\nthat in a worst case scenario, developers\njust became butlers for generative AI. There were literally copy paste\nin one direction. Generative AI takes that as a prompt,\nspits out some code. You copy paste code back,\nyou try to compile, doesn’t work. That’s not how we do it these days at all. In general,\nall of the tasks that developers have to do in a traditional life\ncycle because of, engineering excellence,\ncoding standards and discipline, typically we prefer to offload those like let's\ncall them the toil type of the tasks. Definitely, for example, a developer\nthese days is just going to write the whole class or function\nor routine or whatever the language is, typically without massive amount\nof comments in there, and then ask generative\nAI to comment out the code, to actually explain in human language\nwhat the flow of the code is. Very, very frequently, we actually see\nthat the lazy developer is going to use a single letter variable names, you know,\nnot just iterators, but in general. Inside your class you’re going to\njust say, I don’t know C for a customer. And then at the end you say to generative\nAI, please rename all of that stuff, and it’s just going to go and put intelligent names\nto the variables, to the fields and so on. These are just a couple of examples\nwhere generative AI can be used as a very useful power tool, but is not replacing the creativity that we still expect from human developers\nto shine through. The creativity thing is huge. Like,\nyou're not going to get the generative AI to be creative about a solution,\nbut the documentation like that is, that's it for me\nbecause the documentation was tedious. And if you want to make something\nreally well documented and you're sharing your code\namongst a team, or you're making an asset for other people\nto use, and you have a complex pipeline. And when you look at the code, you're seeing a function call\nwith a bunch of parameters. With modern IDEs, vim\neven, like you could just hover over it and get a detailed explanation\nof what that function does by writing good documentation. And that's what I use almost exclusively. Generative AI is unbelievable for that. It's such a boon for developers. So then with with all of this\nand with that tool truly being so useful. But now I'm curious about how do you actually use generative\nAI to transform an application. So there are languages that are performant\nbut old like COBOL, right. And to modernize an application\nthat uses COBOL and mainframe, you could have a plug in in your VS code\nwhere you're able to have two panes, one with the COBOL and one with, let's say\nthey're converting it to Java, and you're able to do the whole suite\nof like transformation and testing and LPAR\nand all of this stuff. And it's just like Miha was saying,\nthe human is still in the loop. You cannot you're not asking it to do it\nall on its own. You're doing it and you're testing it\nand you're making sure it's working and you're understanding what the COBOL\nprograming language is doing, and then you're able to convert it\nto something in Java. And you just have like a first step\nin the transformation. And that is a huge leap forward\nfrom just trying to write it on your own. Well then is this a separate set of tooling\nthen that's required in order to do this? Or does it integrate into somebody's\ncurrent existing environment? You can integrate it into VS code. Most programmers are running\nsome kind of IDE, and VS code is probably the most popular. And they're able to integrate\nthis kind of tooling into VS code. So the way how toolchains work\nfor developers, it's extremely hard to say what's a separate tool\nand what's integrated tool. So for all developers will use,\nlet's say they open Visual Studio Code\nas a as their coding environment, and then they're going to load\nsometimes up to 20 plugins. And every plugin is a separate\ntool, sort of. You want the plugin\nthat color codes your code. You want the plugin\nthat is actually linting your code. So properly aligning it, you can have a plugin that is looking into\nyour syntactical errors. Or you can have a plugin for AI. Okay, okay, I'm getting it now.\nIt's all coming together. But but with all these different codes\nwith the building, I'm kind of curious\nabout some potential pitfalls. I actually want to give you an interesting example\nthat is not necessarily code conversion. And it can start, let's say, with Java\nor with any of the current languages or JavaScript. Typically, developers are going to create a relatively simple flow of code. The way how brains work and the way\nhow logical parser needs to be captured. And then typically after it works,\nthen you start teasing AI and you’re asking,\ncan you make that more performant? Can you optimize? Can you make it even more optimized? And if you do that too much, you actually\nget a very interesting side results. For example, I saw a couple of examples\nwhere a developer writes a fully functional class, the object\nin in object orientation programing. And when you start to push,\na large language model to optimize one of the changes that LLM does, it turns the code\ninto functional programing code. So it drops objects. Because functional programing\nis generally more optimized for speeds\nthan the object oriented coding is. And when you keep pushing it further,\nsuddenly it starts to do everything in async calls\nand it's just throwing awaits everywhere. So at the end, what I'm trying to explain\nis that you get a code that flies, but regular human brain can't even debug that properly\nbecause you now need to start to trace. If you have a paralyzed pass that is extremely hard\nfor a regular developer to do the async programing, debugging,\nand especially in a functional programing paradigm, suddenly\nall of those things come together. And yes, it works,\nbut it's no longer for human consumption. It looks like complete cryptic hieroglyphs\nthat maybe you need to put it to another LLM\nand say rewrite for humans to understand. It's really interesting, like the\nI guess if you prompted to say to keep it class, keep it object oriented, and you could try to make it\nmore performant that way. But I, I have seen that in action. Like exactly what you're saying, where it just starts\nconverting a object oriented program that I'm writing into the async/await,\nnot async/await, but like await chains like this\ncallback insanity that I had to be like,\nall right, you know what? Let me just start over, write it myself,\nand then ask it to document it for me. The other example would be over going. It's not that it goes wrong, it just goes\nweird for regular humans to understand. To try to give you the analogy here,\nwould it be like when you want to optimize some nice writing,\nsome nice narrative in English, and you would get a complete legalese out\nthat actually means the same, but you just can't choose for that specs. A good example would be where\nmost of the LLMs. when you ask them to optimize regular\nif/then chain statements, it turns into ternary operators,\nwhich most languages support ternary operators,\nbut they are really hard to comprehend gor a regular human,\nit just the syntax is not trivial, and once you start adding the whole lambda\nanonymous functions in it. So very much what I'm trying to say here\nis that LLM can almost start to write assembler level weird constructs,\nwhich yes, they will work, but they will make code\nunmaintainable uncomprehensible and it is not a good code to then actually\ncommit. Code needs to be understandable\nfor generations. It's not just something\nthat one nerd puts together and then, you know, everybody’s\ngoing to admire that piece. Masterpiece, monument on GitHub or GitLab\nor wherever it's published. The code is extremely human\ncollaborative thing. And if you use somebody that is smarter,\nnot really, but makes code that looks smarter, that's actually\na very, very bad anti-pattern. Well, then how do you keep the human in the loop\nthen when you are coding with Gen AI? Is it possible? It is with a very long prompt,\nright? Okay. But even like like avoiding anti-patterns,\navoiding relying on generative AI to write your code for you. And I have, you know, nephews who are interested in\ncomputer science or programing. They're young and with ChatGPT, they're like,\noh, I'm just going to have this write this Lewis script for me in Roblox\nor whatever it is. And they're not understanding\nthe principles of programing. Like there's like, you know, they didn't\nread the Art of computer program. They don't understand the principles\nof what they're doing. And so they're relying on, like Miha was saying, this really hard\nto understand, like complex solutions to problems that work well,\nbut they're not understandable. And even learning from Gen\nAI in that way is pretty terrible. So you have to really go into it\nunderstanding what you're trying to do and then using Gen\nAI to augment your workflow, augment the stuff that you're writing documented\nin my case, and stuff like that, that that's where it really shines. And that's,\nI think its most profound benefit. It seems like the power of really learning\nand understanding is key here. It's about more than just being able to to make a thing\nor know a thing or quote a thing. It's about that understanding. Understandability is definitely the key. Yes. Well, look, here's something else\nI'm wondering about in terms of key. When I think about governance,\nwhen I think about monitoring, did those things have any sort of a place\nhere in this conversation? Yes. And we didn't solve them all yet. And lots of a discussion how to do that. One of the projects that we are doing\nrecently is repeatability. So we always expect in\ncomputer science industry that when we do something that it can be very much\nrepeated again and again and again. And that's why very much infrastructures,\ncode exist. And, you know, we commit all of those\nthings into source control, the way how we actually code with generative AI,\nbecause it's generative, it makes things and it makes them\nslightly different each and every time. So when you give it a spec, let's say\nI want a class that is going to do blah and it will generate code\nthat might be perfectly fine and it works. But every developer that I know\nis going to ask for another generation and another one and another one,\nand then it's going to stare at 4 or 5 of those different variations\nthat came out. And all of them are slightly different\nor sometimes significantly different. And then human just kind of arbitrarily\ngoes, oh, I like this one most, which is all great, right? Then the code goes into a commit\nand everything is fine. Here is the core problem\nwhen it comes to governance. How do you repeat that process? Because the next person,\neven if you feed it exactly the same prompt, was going\nto get another five different variations, and none of them is the one\nthat is currently in our source control. So if somebody wants to rewalk the same steps again, it can't. We need to find\na new way of governance to say, well when you asked it blah\nand it generated material based on your prompt,\nwe need to start tracking these things. So we need to start tracking\nthe temperature number or something that makes the process repeatable. Otherwise at the moment\nit just goes one direction. At the end you get the code,\nbut you cannot repeat the process the same way again.\nThat's really interesting. And it's also like a little bit\nphilosophical, right? Like you're saying that it can't repeat\nexactly the same solution, but it gives you a solution. So you're trying to you're asking Gen AI to give you a solution,\nit succeeds in that. But it's not exactly the same. What's the benefit of having identical responses\nto the process, like understanding that there are many solutions\nin all computer science to figure out a solution for a problem? What's the benefit of having the same\nsolution over and over again, though? Let's say that you want to add\none feature into the code. Are you going to? As a typical modern processes\ngo, you want to prevent the drift. So you are removing\nthe current branch of the things. And you want to start\nwith a new additional requirement. But you start from the beginning\nand you want to regenerate everything. You just can’t, right? Because you are not going\nto get the same thing out. When we're thinking about that, even with, you know, the difficulties\nwith governance there, how can somebody get started\nwith using Gen AI tools in the coding\nor in the modernization process then? If we're talking technical\nand we're talking computer science, data science, stuff like this, having\na background, at least and a foundational understanding of what you're doing\nbefore you start using these tools, I think is necessary. Otherwise it's not a good way to learn. If you're given the answer to something\nthat you don't understand how they got to like the process\nactually is really interesting. How do you how did you get to this? If you’re just taking a block of code\nthat’s generated by, you know, an AI and then you’re plugging in and it works, but you don’t understand\nwhy, that is an issue in learning. And so having a fundamental understanding of programing in this use\ncase is necessary. But to get started using it\nI mean easy peasy. It’s you could find you can find tools\nanywhere and good tools almost everywhere. You just have to learn how how to use it. And it’s simple. Just go into it, start doing it. It's almost a maturity scale. So I would say that once you install\nthe generative AI tool of choice that is using the back end\nmodel of choice and, you know, different, models and different\nvendors are doing different things. We can talk about it later,\nbut the usual step wise, the first thing that typically developers\nshould start doing is doing a very simple slash explain, which is ask a generative\nAI what is this code? So through that, the generative\nAI typical is going to write in English language how it interprets\nwhatever code you are pointing to. The next step is typically slash fix,\nwhich is find the error and fix that error. Third one would be slash test,\nwhich kind of, generates tests for me and make sure that all of the boundary conditions\nare properly validated and tested. And then the last one is of course\ngenerate new code. So if you're going through that, it's\nkind of almost the easiest one is just to ask,\nalmost like some very senior developer. Can you tell me what this does? That is very much the easy first step\nfor developers to start. So how do you even decide between the\ngenerative AI options that are out there? You actually try multiple models\nand that's one of that level of complexity and lack of governance\nthat we are trying to address these days. If you're looking, for example,\nhow developers and we interviewed a whole bunch of that, there is a joint project\nbetween IBM and Red Hat right now where we are really intently looking into\nhow that is, how new software is created. With these tools, they're typically going\nto create the prompt and throw to 2 or 3 different LLMs\nand see results coming back. Based on that, change the prompt and maybe make a different selection of LLMs\nand then change the prompt again. And then maybe they're going to say to one\nLLM, can you test the output from the other LLM\nand then create the chain of those agents. And then at the end,\nvery much using them like a power tools and agentic approach to, generative AI,\nwhich means that they become agents, that they start to talk to each other,\none generates code, the next one does the semantic parsing. The third one does the static code\nanalysis, and the last one, for example, does the full end to end blackbox testing. These are all different, potentially different LLM\nengines, different LLM models, each one specialized in being based\nin one of those specific tasks. We have models that are not great at all\nin generating new code, but they're absolutely amazing in verifying\nand testing existing code, for example. How to choose which one, at the moment\nit is still a little bit of, you know, dark arts. And that separates a good, advanced\nAI software developers from beginners. Okay. Well, look, we've been kind of talking\nabout things, an ideal scenario. Right. So we've been kind of imagining\nthat if you're that your organization has all the resources in the world\nand can test all these different tools on out and find the perfect one,\nthat's for you. But what if your organization just doesn't\nhave the tools that you need, or that you want to use? Are you just out of luck? It's a very typical approach. Until we understand it,\nwe are not going to allow it. but there are two separate problems here\nthat we need to address. Why are companies\nso, reserved and risk averse when it comes to using LLMs\nfor, let's say, some, you know, important code,\nlet's say mission critical system code. And so the first problem is, of course,\nas you mentioned, giving that code to LLM until there is a very thorough\nlegal review, if you push code to some SaaS service or API endpoints\nthat is outside of your organization, what happens with that data\nthat you gave out from there? So the first problem is\nhow are they going to treat the data. But the second problem is\nwhat was the model trained on. And that’s\nwhy those two things are super important. Where it runs, it could run\ninside the walls of a data center or in a trusted environment\nthat has a very strict and very well defined legal limitations. But the other part,\nwhich is, is the model open sourced\nand published and fully vetted by broader audience and scientific community,\nor is it commercial, closed source, dark, arcane art\nand you should just trust it, right? So in closing for today, here's\nwhat I'm thinking. I've got a few takeaways here. Generative\nAI won't replace you as a coder, but it will seriously help you code. Code should be built\nto last for generations, not just for a moment in time. And having a fundamental\nunderstanding of code is still critical to code\nusing generative AI. So basically, just keep playing with all of your approved options\nand emphasis on the approval. All right. So Miha, David, thank you so much\nfor both of you for being here. This has been a fantastic conversation. That’s it for this episode. But everybody please keep on listening. Don't worry. There are a ton more good bites,\ngood nuggets where these came from. And we'll see you again\nhere soon. All right, friends."
}