{
  "video_url": "https://www.youtube.com/watch?v=rn1sjMFRzTQ",
  "video_id": "rn1sjMFRzTQ",
  "title": "AI Hardware: Training, Inference, Devices and Model Optimization",
  "upload_date": "20240703",
  "channel": "IBM Technology",
  "duration": "38:25",
  "caption": "Welcome everyone to this\nweek's Mixture of Experts. I am your host, Brian Casey. Um, today we're doing all\nhardware all the time. Uh, we are going to start with a\ndeep dive really of the industry and space right now, specifically\nabout, um, how the training and inference stacks are diverging. Model architectures still. evolve over time. So how do you build hardware that, you\nknow, you put in the data center and you, you know, depreciate over like five, six\nyears so that you can still run the model, which is active in six years with today's\nhardware, you know, you're putting down. From there, we're going to move to Apple\nand talk a little bit about how The architecture patterns that we see there\nthat are combining on device and cloud could be a pattern for the industry. The composability of these\nchips, the different models is going to become important. And I liked the way Apple did this, right? Here's some of the stuff\nthat you run on device. But when I want to do something a\nlittle bit more complicated than reasoning, I'm going to come off\ndevice and I'm going to push that on the cloud to perform that action. Finally today, we're going to talk\nabout model optimization and the things that people are doing with models. to better take advantage of the\nhardware that's available to them. But I see there's a lot of energy there\nand a lot of work and documentation and, uh, which make it very easy,\nyou know, for the end users and the developers to use these techniques. Joining us on the show today,\nwe have Volkmar Uhlig, who is the VP of AI Infrastructure. We have Chris Hay, who is the CTO of\nCustomer Transformation and we have Kaoutar El Maghraoui, who is the Principal\nResearch Scientist for AI Hardware. We're going to start like\na little bit big picture. Um, and Volkmar, I'm going to\nthrow this first one over to you. Um, but obviously like NVIDIA has been,\nhas absorbed a lot of like the oxygen and perhaps money in the room, uh, in\nthe hardware space over the last 18 months in a sort of like classic, Um,\nwhen there's a gold rush picks and shovels, um, toward a sort of investment. Uh, but I think one of the thing that's\nbecoming like more discussed and kind of more clear is that the ways that like\nthe training and inference stacks might diverge, um, over time, the extent to\nwhich that's kind of already happening. Um, because I think some of the\nplayers that are involved in each one of those things, and some of the\nlike, the interests of the consumers are, um, you know, a bit different. Yeah. Even at some of the use cases obviously\nare very different, but I wonder if you might just like start us off and just like\nmaybe give, like, I don't want to say an overview of the whole landscape, but maybe\nstart with that kind of piece of talking about the, like the two stacks of like\ntraining and inference, uh, kind of maybe some of what you see going on in each\nof those and just like how you see them kind of diverging both now and over time. Yeah. Okay, cool. So I think the, uh, uh, the\nmain difference between. inference and training stack is to\nscale, uh, these GPUs get connected. And so we like, there's a bunch\nof accelerators happening, but let's just stay in the GPU market. On inference. You're primarily limited by, you\nknow, the throughput you want to achieve the token latency you want\nto achieve, um, and then there's a certain amount of memory you need. And so the bigger the models get,\nthen usually, you know, you start spanning across multiple GPUs just\nto maintain, like you have enough memory and maintain the throughput,\num, and then achieve latencies, which are reasonable for human beings. So if you look at the big\nGPUs, which is often what... what people are using, A100s, H100s,\nand, you know, what's coming out, um, you connect, you know, two to four of\nthem to just get into latency bands, um, where, uh, you know, the human can not\nread as fast as the tokens get produced. Um, and then you have the, the other area,\num, we're just training and training is really, uh, how many GPUs can you connect\ninto a single system, and it's much more like the HPC space, you know, very\nlike, you know, for 20 years, we tried to figure out how to build networks to\nconnect thousands or tens of thousands of computers, and now we're doing the same\nthing again, it's just called the GPU. And here it's really how fast can you\nexchange information between these GPUs? So you have these all gather operations\nor weight redistribution operations, um, but you may have 10, 000 GPUs behind it. So the real value here is how fast\ncan you get data out of these GPUs to the other GPUs and how fast\ncan you distribute them to all. And so from a cost perspective, um, in,\nin basic inferencing is really like, you know, you cram something in a box and you\nwant to have a power efficient, um, and then you want to achieve those latencies. And in the, in the case of training,\nuh, at least for like the very large training sets, uh, it's like how\nmany computers can you fit into a room and connect them with a super\nlatency, uh, super low latency network? And many cases like You know, the\nhardware cost in the network is 30 percent of your overall system\ncost just to interconnect them. And Chris, maybe a question to you is,\num, on the inferencing side, like in particular, have, have you seen like costs\non like an absolute basis already become like a real issue for clients, or is it\nthe type of thing where, you know, they're concerned about like the unit economics\nand as it scales, it will obviously be an issue, like how real is the cost\nproblem today um, on just like the production workloads people are working\nwith when it comes to work with LLMs? Is that like a theoretical\nproblem or real problem? Um, a very obviously in the future\nproblem, uh, real problem, but how would you, how would you describe that? I honestly think from a client\nperspective, and this is going to sound really harsh, they don't really care. Because most of the time they're\ngetting inference from the cloud, right? And they're playing cost per token, so\nthey're not really provisioning GPUs. I don't know many clients\nwho are provisioning GPUs? Maybe some are doing that for\nfine tuning, but they're certainly not doing that for inference. In fact, clients are actively avoiding\nanywhere where they have to have a GPU for inference themselves. And the reason they're avoiding that\nis they're not going to shove enough workload through that for inference\nto be able to justify the cost. So they really want to be paying\nmore in a kind of SaaS model where it's kind of paying for the\ntokens as opposed to GPU time. And if you think about it sensibly, GPUs\nwould just sit around doing nothing most of the time in a client's estate, so I\nthink this is a big technology company problem, not necessarily a client problem. Now, don't get me wrong. There are some clients who have\nto run on premise workloads. So think of financial institutions,\ngovernment institutions where they don't want that data going to the cloud, and\nthey have that problem where they have to think about provision their own GPUs. But, but regular clients, I don't\nthink they think about that too much. But I think I, you know, this is very\ntrue, Chris, but I think especially for startups and like small and medium\nbusinesses, this becomes a big challenge, especially being able to maybe either\ntrain or fine tune models for their whatever business that they care about. This becomes a real issue. So cost reductions for\nthem will be significant. I think like what Chris said is, is right. To a certain extent, the\ncustomer doesn't really care. Um, you know, they don't want to know\nthat there's a GPU behind it, ultimately. Um, they only care that\nthey get the service. And they get the service at a certain\ncost and at a certain latency. Because in the end, you're modeling\nsomething where, let's say, I have an interactive workload with a customer and\nthe customer doesn't want to wait like 20 seconds until they get an answer back. Um, we want to hide that in the data\ncenter in the inferencing service. Now, from a hardware perspective\nfrom, you know, companies which are actually invested in those hardware\nsystems, that's the optimization, right? And that's, I think, where we will\nsee much more custom hardware going into the market, um, which is highly\noptimized for these specific workloads. And it's interesting to see like, you\nknow, we have a trend of like, okay, there's, you know, more common models\nand model architectures, but model architectures still evolve over time. So how do you build hardware that, you\nknow, you put in the data center and you, you know, depreciate over like five, six\nyears so that you can still run the model, which is active in six years with today's\nhardware, you know, you're putting down. So I think there is a, there's an\noptimization game probably played by the big players, you know, the AWSs\nand you know, the Azures and you know, the IBMs who are putting specialized\nhardware, highly optimized influencing engines with a lot of variety for these\ndifferent models into the data center. And I think, and for companies,\nit'll be hard because, like, how many GPUs do you wanna host? Like you want to homogenize,\nbut now you may overpay. And so the cloud may be your answer,\nuh, to get cost reductions in place. To that point about maybe optimizing\nfor five or six year time horizons. Like one of the themes I've\nseen come up, um, more recently, I've heard Jan Leike.... Um, make versions of the statement,\na few others, um, that like LLMs, even within the research and academia\ncommunity have like absorbed all of the oxygen, um, in, in the room and\nthat there's other, there's other areas and like ways that people could\npotentially even imagine getting to AGI. But like right now, it's just\neverybody's on this one path and so I think Jan even like famously was like\ntelling New Research like don't work with LLMs like go do something else\nwith your life, uh, basically which like the reaction in the market was\nlike sort of predictable to that but it was like very Jan on some level. Um, but I'm curious like do you see like\nthe hardware players like almost following that same sort of dynamic where like\neverybody sort of like is looking what's happening with LLMs right now and they're\njust sort of all in like the transformer has been pretty resistant up to up to this\npoint with some like tweaks to it so... are people kind of looking at the model\narchitectures that are around today and just sort of like assuming that those\nare going to hold for at least at least the medium term or, you know, do you\nsee places where people are like, you know, Exploring, you know, what other,\num, um, you know, maybe what other alternative architectures might play\na role at some point in the future? I think that's a great question,\nand there's a lot of research actually exploring alternative\narchitectures as transformers. Transformers, of course, it's\na pretty solid architecture. Uh, the example, for\nexample, the MatMal3 LLMs. Which is… which is I think it’s a WAG paper that has demonstrated the viability of MatMal3 architectures. And the research opens up a new frontier in LLM design. Also encouraging the exploration of alternative operations and computational paradigms. So this is actually kind of opening\nthe frontier here for a new wave of innovations with researchers\ndeveloping novel model architectures. So this particular paper, Uh, really\nkind of looked at using these trinary operations, replacing the, uh, the math\nbehind the matrix multiplication with much simpler operations, and that's\nactually the biggest bottleneck you see in these LLMs computationally. And there's also translated also into\nreduction, huge reductions into the memory and the computational, uh, requirements\nneeded for these LLMs, also, they really showed some very interesting benchmarks\nand result that shows you could get almost same accuracy or the same performance\nas you would get with transformers. So and I think this is just the beginning. So there is a huge need to look\nat alternative architectures, uh, either using, you know, these novel\narchitectures or using things like, uh, neural architecture search where\nyou're trying also to You using AI to generate efficient architectures\nfor for hardware for to target, you know, special special platforms. And that's are much more energy efficient. The reaction to that paper\nwhen it came out in the market was like pretty substantial. Um, I think the original tweet that\ngot shared as well, like announcing the paper, either just like summarizing it\neven, um, I think had a couple million views associated with it and people\nwere very excited and, um, in the way that Twitter gets, um, around things\nlike, oh, this is, you know, a big change, it's like, we're not going to\ndo matrix math on GPUs, uh, anymore. And obviously, like there's the initial\nhype and then, you know, there's reality that sort of comes after that. Um, but in, in a space like that\nwhere you're doing that sort of fundamental research and a lot of\nthe indicators were looking like, you know, this actually could be a thing... um, what do you, what are like the next\nsteps for an approach like that going from something that's just like a novel\npiece of research to something that is like approaching a place where it\nmight have some commercial relevance? Like, how does it go, and like,\nyou know, does it end up having any impact on the hardware ecosystem? Maybe before we get to that hardware\npoint, you know, maybe just would love to hear from the group of, you know, what\nare the things that you would want to see that would give you some indication\nthat like a different approach like this, um, might have some, some relevance and\nsome staying power, um, in the industry? So I think that we go through functional\nplateaus with these different models. So if you go back, we had CNNs,\nand suddenly computers could see. And then we got transformers,\nand suddenly they can reason. And we are going through that, through\nthose phases, I think there will be new model architectures coming, you know. Like this is almost like\na scientific discovery. Suddenly, The model exhibits\na certain behavior, which we haven't seen before, right? And we had the similar effects when CNNs\ncame out and suddenly, you know, all the image processing algorithms were out the\nwindow and now all the, the NLP systems are out of the window because we don't\ndo entity extraction anymore because we can directly reason in a network. And so I think there will be new\narchitectures coming with new capabilities, and I think this will\nbe a lot of the effort, like the people will start experimenting with\nthese alternative model architectures. At the same time, we are seeing that,\nyou know, people are trying to rewrite the existing capabilities because now\nyou have that new plateau where you are, you know, we know how Chat GPT needs\nto behave and we, we can benchmark it. Before we could benchmark the vision\nalgorithms and we could say, you know, what's the detection rate. And so I believe that we will\nconstantly go through these loops. And if these fundamental changes happen,\nsuddenly you got, you know, a 10 X performance improvement, but, and then\nthe, the hardware will catch up, but you will not take something which is\nin production and necessarily say, Oh, let's just throw it out because these\nmodels have been tuned for, for ages. So if you look at the introduction\nfrom discovery until it goes into the industry, you know, if you are like at. Three years now, like, you know, it's\nlike Chat GPT already I mean, it's already aged, like the transformer model is aged. And so what's the lifetime? It's probably not like twenty years,\nbut it's probably like five, six years until the new model shows up\nand things, you know, get tweaked, and each of these 10 X performance\nimprovements now allow you to make the model 10 X more complex, because now you\nsuddenly get all the computation free. And so I think those things\nwill happen over and over again. We are just now in this era of discovery,\nright before a true architecture or a lasting architecture was found. So, and I think we will have\nthis for a couple more years. Do you think that basically every, every\nhardware player in the space right now is up is like building out what they're\ndoing, optimizing almost entirely for like the existing transformer and LLM stack? That's the safe bet, right? So if you look from a, from a\nmanufacturer, if you build a card, like you go like all the cards,\nif you go back five years, cards don't have enough memory bandwidth. Then suddenly training of these LLMs\ncomes on online and it's like everybody's like oh I need two terabytes of memory\nbandwidth per second and now I need four and you know if you're growing these cards\nif you would have optimized the card five years ago you would have done that for\nencoder models which are totally compute bound and not that, not memory bound. And so we are seeing the shift towards\nthese models as they come online. And then you go from, oh, I\nneed 32 GPUs to I need one. But that's the architectural shift because\nthe hardware gets like gets modified. And you are like, Okay, I'm willing to\npay 10 times as much for memory bandwidth. And so but overall, I'm\nstill getting a better deal. And so I think those things will happen\nover and over again, you know, like effectively what NVIDIA did is like\nit took the computation of the CPU and said, okay, I give you 100x for\nthis different architecture, right? And now we are going through a\ncycle of, okay, we need more memory. And so then there will be\nspecialized operations. We already see it with CUDA cores, right? So do matrix multiplications in\nhardware and, you know, you just do this over and over again. I, I kind of disagree. Yeah. Sorry. I kind of disagree.\nGood. I think, I think last best model wins. That's it. So how many people here\nis still using LLAMA 1? How many people here\nis still using LLAMA 2? Nobody. Why? Because we're using LLAMA 3. How many people is\nusing Mistral version 1? Zero. Cause we've all moved on to Mistral 0.3.\nLast best model wins. So therefore, if somebody comes up with\na good new architecture that beats the old architecture, Everybody's going\nto move to that super, super quickly. Consumer doesn't care because consumers\nare not training models, so they will take the first model that works,\nthey'll run it, as long as it works on their consumer hardware, they're good. Slightly different for data centers,\nbut data centers are not going to want to be left behind, so\nthey're going to run towards it. So hardware wise, if I'm honest,\nas long as it runs on my Mac, which runs MLX, I don't care, right? So I'm not going to go out and buy\na GPU, it's going to run on my MLX. Now that's different for, uh, folks who\nrun data centers because they're going to want the most efficient inference\nthat they can and they're going to invest in the hardware to be able to do that. But me as a consumer, last best\nmodel wins and, you know, it's got to run on my consumer grade hardware. Good, but now you just defined\nwhat the constraint is and it needs to run on my consumer hardware. So there is effectively a,\nthere are two sides of this. There's like, to what hardware do\nI build, which you are going to buy with your next laptop in three years? And what's the model architecture? And so you have a deployment,\ndeployment issue, right? So CNNs, for example, haven't\nfundamentally changed. So CNNs, the hardware is the same, right? It's an encoder model. It's not a decoder model. And so you, you, Still run the same\nhardware, but now we said, Oh, and by the way, we have this new workloads, but fund\nfundamentally CNN's all kind of stuck where they were a couple of years ago To a point, because if there's a\nbrand new model with a completely different architecture, and it could be. It could be a 5 trillion parameter\nmodel for all I care, right? If the latency is fast, then again,\nI'm putting more constraints on it, and it's SaaS and I can't run it on\nmy machine, but it smokes whatever I can run on my machine by 100x, and\nthe price is so cheap, I don't care. I don't need to run it on my machine\nat that point, because That model there is the new best model, and it\nwins, and therefore me as a consumer is going to run right towards it. I think on the hardware space it is,\nit has been constantly a challenge for these hardware vendors to\nfigure out what do we optimize for. There's always this trade offs between\nbuilding this general purpose accelerator that can run all these different\narchitectures Or this really super optimized things for like the LLMs or\nfor the CNN, so for, you know, the LSTMs or all of these different architectures. So that I think that's going to continue\nto evolve and because it takes a long time to design hardware, so it's not a\nsoftware that's, you know, every day you have new architectures, new, new releases\nand things like that for hardware. It is a much longer timeframe, so. so this really, you know, becomes a\nchallenge for the hardware vendors. What do we optimize for? And I think maybe that's going to\neven change the way these hardware vendors and the architecture and so\nforth think about hardware moving into these composable systems. Maybe I don't need to build these\nmonolithic chips that are built to design everything, you know, to optimize for\none specific model or architectures. Can we compose things and\nthen plug plug them or compose decompose in a dynamic fashion? So I think this is maybe something a\nchiplet, for example, it's something trying to do something similar where\nwe we don't want to have, you know, these one big chip, but having these,\nuh, uh, chiplets where that you can compose, uh, very easily and then scale,\nespecially for different use cases. I think the, the, this race for the\nbest model is going to continue the, the, also the race on the AI hardware... how do we optimize,\nwhat do we optimize for? How do we build the next\nroadmap for the next five years? That's also going to be a challenge,\nbut it's going to force, I think, a much closer hardware software co\ndesign kind of cycle that we need to shorten so we can reduce cost\nefficiencies and win in the market share. And I think you're right. There is, I think there's two\ndifferent driving factors, right? So if you're a consumer, there are\nthree different driving factors. If you're a consumer, you care\nabout what is cheap for you, what can run on your machine, etc. That's kind of one factor. If you are a company who is\ntraining these models, then the biggest factor in your case is\ngoing to be, can I train the model? Can I pump the data in, how\nquickly can I get my model out? And how can I prove it and\nkeep up with the market? If you're a cloud company serving up\ninference or a data center serving up inference, then you're trying to maximize\nthe cost per token or minimize the cost per token for your architecture. And that becomes really\nimportant because you want to be competitive with everybody else. So if you're charging, you know 0. 05 cents per token or something\nper million token, but somebody else is doing it for 0.01, you need to cut your inference\ncosts get it as bare as possible so that you can be as cheap as possible. But that's not true when training, Right? When training you are trying to get the\nbest possible model that you can and get it done as as quickly as possible. So I think these different driving\nforces, uh, really affect it. And I totally agree with you, Kaoutar,\nthat, that the composability of these shapes, the different models\nis going to become important. And I like the way Apple did this, right? Here's some of the stuff\nthat you run on device. But when I want to do something a\nlittle bit more complicated than reasoning, I'm going to come off\ndevice and I'm going to push that onto cloud to perform that action. And I think that sort of. Uh, thing is going to\nbe key in the future. There's a really good statement across\nwhich, um, what I like about the Apple approach is that they effectively\nfigured out an upgrade path, right? So you can have your five generations\nof old phones and you effectively say, whatever I can do on device, I\ndo on device and whatever I can't, or if I don't have the hardware\nacceleration, I now have an overflow bucket and the overflow bucket can be. The model is too complex or like I\nwant a complex answer to something or the other one is I have old hardware. So they effectively created for\nthemselves now an architecture where they can start innovating despite\nthat the device has a longer lifetime. So they decoupled the lifetime of\nthe device from the lifetime of or from from the model evolution. And so now they can innovate\nrapidly because they can always update the data center. But the device which the customer\nheld in their hand is, it's, you know, it's kind of fixed. And so I think it's, it's\na really interesting move to, to do that separation. I thought that story, just like everything\nthat they announced at WWDC from, from both that path of smaller, like many\nsmaller models on device to their own models in the cloud, to only a third\nparty API when they absolutely have to, and then having like the user opt in on\na per interaction basis combined with their own sort of Silicon was just like... the best example yet I've seen in the\nmarket around how to do, you know, how to optimize for, for cost for speed,\nhow to do kind of like local, um, you know, local inference, um, on, on a\nworkload, um, you know, I'm just curious, like, do the rest of y'all, Was that\nkind of your reaction to it as well? And I'm curious to just the conversations\nthat you've had with clients like did that did that stick with them at all\nabout a way to, you know, think about both like the combination of models they're\nworking with and just like at all in terms of the way that they're thinking\nabout like infrastructure and compute. Yeah, my reaction was the, I think\nit's the first architecture, which takes security and confidentiality\nbetween a device I have in my pocket and, you know, computation, which is\non the other side of a wire, seriously. And I think they really try to figure out\na way, um, to keep like the, you know, like they're selling a device and like\neverything happens on the device and they figured out, well, we cannot do that. And so they try to come up with an\narchitecture where you, you, you extend your trust domain into a\ndata center and they, they went, I think, Overboard with like, okay,\nwe, you know, we have secure boot. Okay. That's what everybody expects. And then you have, um, non introspect\nability of any, any data, which goes in by the like by an administrator,\nso you don't have privilege elevation of administrative accounts that\nI cannot extract data at all. So I think there is. They really, oh, and then we publish our\nbinaries and the binaries are digitally signed and they are only buildable\nthrough our internal build processes. And so now anybody can inspect it. So they really try to say, look what we\nguarantee you with physical isolation. We are doing also in the cloud with like. digital isolation, you know,\nthrough that, all these things. I think the second really smart move\nwas to say we are taking the same chip, we are not buying, you know, an NVIDIA\nchip or, you know, an AMD chip, but we are using our own infrastructure to run\nthose things on our, on the same device, like on the same operating system, right? So they're M3 or whatever\nthey have internally. Um, and so, And now you can\nrun the code on both ends. So, like, from a development perspective,\nit really brings the cost down. But if you look at those scales,\nthose Apple scales, with, you know, hundreds of millions of chips, like,\nit's really cheap for them, right? So, a chip costs you, like,\n60 bucks if you make it yourself, put a bunch of memory. The whole system is probably\nin the ballpark of, like, $200. And they can stamp them out. They know how to stamp\nout at scale, right? And so, if you would do that alternatively\nwith an NVIDIA card, Card and x86 servers to cost would be like super ballooning. And so I think there's a lot of really\nsmart, but design decisions in there, but they really looked at an end to end. I think they figured out and they put the\nbar there, how consumer AI trustworthy consumer AI will now look like. And everybody will be like, okay,\nif you don't do it like Apple, I will not use your source. And I think now what's unclear is what\nthe enterprise answer to that is, because there are other questions which are asked. But I don't think that's just\nin the consumer and inference. So, uh, I run a Apple M3, right? MacBook Pro. It's a 128 gig unified memory. Um, it's, this machine is a beast, right? And basically, I can run Llama\n3 70 B on my local machine... because of the unified memory, I can say\nright now, there is no consumer machine that I can afford, right, other than\nthe Apple M3 that would be able to run Llama 3 on on my machine, not only that,\nI can fine tune models just as fast as I can on Google Colab on my MLX machine. Without quantization, I can just take\na base model and I can fine tune it. So I fine tuned, I think it was 1, 000\nrows, completely unquantized, so maybe it was about 10, 000 tokens, maybe more. Um, and I did that in less than\n10 minutes on my, on my M3. I didn't need to go to cloud,\njust did it on my machine. Now, I haven't even measured that with\nquantization or Laura at that point, but that is going to be a future for fine\ntuning as well, even in the enterprise. Why would I go to a cloud to go and fine\ntune my data if I'm fine tuning maybe 100, 000 rows worth of data, right? Maybe I'm doing a couple million tokens. I can run that on MacBook, and half of\nmy enterprise is sitting with MacBooks on their machines, so I think it's, I think\nApple's making a really smart play, and I don't think it's just in the inference\nspace, I think it's in the fine tuning space as well, I think Apple has set\nthemselves up as the real developer. Uh, workstation for AI I think combining this with something\nlike what IBM is doing and Red Hat with Instruct Lab would be really powerful\nbecause it also brings that kind of, uh, creativity in terms of creating your\nown models, fine tuning them all in your local space without having to deal with\nthe complexity or the cost of the cloud. I think that's going\nto be really powerful. It's funny. I think one of the outcomes of just\nthat whole thing was that from the very start when people started to get\neducated about models, people were like, fine tune as a last resort. Like you should, you should do rag first. You should do all these things before\nyou ever think about fine tuning. And I kind of feel like Apple made fine\ntuning cool because that was like such an important part of what they're doing. And it just, it does strike me as\nlike, as it gets easier and cheaper to do this stuff that we'll see,\nwe'll see more and more fine tunes. I certainly like hugging faces just\nlike littered with fine tunes at this point, but I think, um, it does\nfeel to me like there's going to be sort of more and more of that. And I think that's actually sort of\na great segue for the last segment where we were going to just talk a\nlittle bit about, Model optimization. There's a huge amount of\nactivity in the space. I think, um, you know, the count of\nand it even seems like the emphasis on smaller models, even from the big players\nis getting to be more and more and more like even some of the messaging I've\nseen from them recently is like we want to deliver models that are actually like\nusable to the community as opposed to just like this, just ultra AGI path, which\nis only useful in like some dimensions. Um, you know, Chris, you just talked a\nlittle bit about Laura and quantization. Um, I'm just, you know, beyond like,\nwhat are you all seeing in terms of the things people are doing? And that's having like the most impact\nin the market right now in terms of Taking a combination of like small models\nand fine tuning all these techniques and actually compressing them down\nto get their inference costs lower, but still getting kind of the results\nthat they're that they're looking for, there's like a lot in this space, and\nthere's a lot of energy in the space. And I'm just curious in terms of like\nthe things you've seen, the things you've done, the clients you've worked with,\num, you know, just where you're seeing kind of the most bang for the buck and\nwhere people are kind of most excited, um, right now and, you know, happy for,\num, anyone to kind of take that one. I think there's a lot of, you\nknow, excitement around these model optimizations, you know, reducing the\ncomplexity of the models, quantizing the models, pruning the models, applying\nthings like knowledge distillation. So there is a suite of techniques right\nnow and tons of great libraries out there. Hugging Face is doing a, I think they're\ndoing a fabulous job, uh, with their Optimum, uh, uh, library, where they\nhave multiple hardware vendors, and they basically focusing on these, uh,\nhardware level optimizations, but kind of abstracting them away where you can\nhave these LLM extensions, for example, for better transformers, for things\nlike quantization with GPTGTQ and or for bits and bytes, so these are libraries\nthat make it very easy with some common APIs to benefit from quantization,\npruning, uh, better transformers or optimization and transformer like this. page attention. Uh, you know, some of these\nquantization like you, Laura, et cetera. So those are very important to kind\nof democratize and make it easy for people to consume without really\nhaving to understand the depth and the details of different hardware vendors. So that's a good example of kind\nof democratizing this, all of these optimizations and making\nthem accessible to the developers. Um, Of course, that requires a lot\nof work from different vendors to be able to implement all of these\noptimizations and then provide these common API for the end users. But I see there's a lot of energy there\nand a lot of work and documentation and, uh, which make it very easy,\nyou know, for the end users and the developers to use these techniques. Yeah, and just to sort of add into\nthat, I mean, there's obvious ones as well, like kind of caching, for example. One of the ones that I find\nquite interesting at the moment is probably batching. So there was a project somebody did\nwith MLX earlier this week, where, you know, if you use MLX out of the box,\non my machine certainly, with something like the Gemma, Model to be model. You're going to get something\nlike 100 tokens per second. That's unquantized. But they got up to just by batching\nthe inference, they got it up to 1300 tokens per second on their machine. And I've seen similar\nthings with fine tuning. So one of the things that I've\ndone in my data pipeline is the MLX pipeline wasn't fast enough for me. So I rewrote all the Apple data\npipeline so that it was just kind of loading in and The biggest thing that\nI found is actually I pre process all the tokenization for fine tuning. So rather than letting it load, um,\nyou know, as it's trying to fine tune, I just pre tokenize everything. I set up all the input and target\ntensors, get it all done in advance. But the last thing that\nI do is then I bucket. all of the batches together on\nthe same sizes, which is kind of almost like padding free. But really what I'm doing in this\ncase is bucketing based on the same pad padding size to reduce padding. And that is massively increased,\nuh, my speed of training. So I think these techniques like caching\ntraining, you know, uh, cause I mentioned things like you, Laura, et cetera. Again, even, even, even if you\nthink of the way LoRa works, um, you're really looking at\nthings like, um, freezing layers. I think at some point, if I think of the\nGolden Gate stuff, the Anthropic we're doing, I, I think you're going to get\nto the point, rather than doing a hard freeze on layers, I think you're going\nto get, start to freeze on features, because if you can understand Feature\nA and Feature B are being influenced, uh, or is, are on a certain topic, why\nnot freeze those features right in the future and then train areas only in the\nfeature area that you want to train. So I can see that happening\nin the future as well. So I think there's going to\nbe a lot of optimized patients coming in on those areas. I know we're just about out of time. So Volkmar, maybe I'll, as the first time\nguest, um, on the show, maybe I'll, I'll give it over to you to maybe close us out\nand talk, you know, of all the sort of like techniques that are in the market\nright now that are like really focused on like optimization, which ones, which\nones are you seeing, like, are Yeah. Which ones are you most excited\nabout and that you see the market kind of most excited about? So I think the similar to what Chris and\nKaoutar already said, you know, I think we get a bag of these optimizations. What's interesting, uh, what I find\ninteresting is the, uh, Uh, like anything which is giving you like a\n2, 3x speed up in whatever dimension. Like we are not yet, I mean, when you\ncompound them you may get to a 10x. One of the things that really excites\nme, um, is speculative decoding. So we are running a sim, like a simpler\nmodel, um, on top of your complex model. And the simpler model is kind of\ntrying to predict a bunch of tokens. Um. Ahead, and then you just verify it\nwith the big model because all the quantization methods, right, there's a\nquality impact and the quality impact, you know, it's really hard to quantify. And so we have these models which\nwe're testing and then we go through quantization and they kind of look good,\nbut you don't know what you don't know. And with speculation, you effectively\nverify against the full model and you get just a 3x performance improvement. And I think that's probably the one\nthing where, um, uh, you know, I, I think that's, that's the biggest\nimpact because it's also like batching makes things better, et cetera. And then you have like\na continuous batching. So, you know, requests come in and\ngo out, but there's always, um, like there's a latency impact with that one. It's really like, it's just,\nit's better and you don't have any, any reduction of quality. I think that's a great place to end on. And it's also just like thematically, I\nthink one of the most fun parts about the space, which is, it just feels like you\ncan wake up one day and it's like, Oh, I, someone struck together a couple two Xs. And now we got a 10 X improvement. Um, and something, you know, being\nat that stage of the journey is a lot more exciting and interesting than,\nSome of the later ones, potentially. So, um, it's a fun, fun time right now. And, um, I think a great,\na great place to end on. So thank you all for, for joining us\ntoday for our listeners, for our guests. And, uh, we will see you back\nnext time on mixture of experts. Thank you."
}