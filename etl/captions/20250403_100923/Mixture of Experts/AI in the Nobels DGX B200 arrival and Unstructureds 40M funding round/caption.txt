it's 2027 has an AI generated work won the Nobel Prize for literature chrisy is a distinguished let me start that again give me that pause man just starts yelling even before I ask the question and do the in it's 2027 has an AI generated work won the Nobel Prize for literature Chris Haye is a distinguished engineer and the CTO for customer transformation Chris welcome to the show what do you think absolutely and while we're at it AI is going to win a few Oscars and an em as well okay all right and uh next up Edward calbar is a vice president product management for the Watson X platform Edward welcome to the show uh what do you think no way uh I don't think the noble institution will outow work all right cool well with a difference opinion like that you know it's going to be a good show all that and more on today's mixture of [Music] experts I'm Tim hang and it's Friday which means that it's time again to take a deep dive with our experts into the week's news in AI we're going to talk about open ai's new dgx b200 the new round of funding for a company called unstructured but we're going to start today with the big news story of the week which is basically that AI has been taking the Nobel prizes by storm this year in the prize for chemistry David Baker Demis hbus and John jumper took the prize with hbus and jumper winning in part for deep minds work on Alpha fold and then in the Nobel Prize in physics one of the founding fathers of modern AI Jeff Hinton and John hopfield another uh Leading Light of the field W for their work in neural networks so I think Chris I want to start with you first is what do we make of this are the Nobel prizes basically scumming to like AI hype or is this the start of something way bigger I love it actually um I think well I think the Nobel prizes is if I'm being completely honest has been a little theoretical and not hitting with the real world for a while so actually you know recognizing Ai and the impact that it's going to have in multiple Fields such as physics and chemistry and if we think about the big innovations that are going forward in the next few years it is going to be more AI Le right so it's going to be Ai and humans in collaboration and how do you distinguish that well is it fair to say that the the people who founded AI in the first place then don't get rewarded for that work of course not so I actually I think it's a good thing because this is the Cornerstone over you know the next few years where AI is going to massively help in these areas so I'm I'm all for it go for it and while we're at it uh I think my next AI means I'm going to become the MVP of the NFL soon as well you know so Aaron Rogers watch out here I come I guess Edward to bring you into the conversation I think Chris has already taken a very strong stance that basically you know it's G to be a few years from now and you'll just be AI winning every single Nobel Prize award um I guess Edward there's two questions I think what you said in the opening question you were like well I don't know if the dobell institution will really allow it does that mean that like you don't think it's deserved or basically that you think like the institution won't really you know be into being into awarding and giving AI it's do yeah I mean I think I think um mainly the the former uh I mean I do think that that AI is going to be an incredible Tool uh in in almost every aspect of our lives and it's going to do amazing good for society and and uh well-being and quality of life and and and basically all the things that they institution stands for uh but but I I do think uh you know the human contribution to uh to to to those outputs that deliverable is really essential right so whether it's 10% AI or 90% AI I think if it's 100% AI maybe it goes a little bit too far uhuh right um I think Chris one thing I wanted to do is you know I think I I take a lot of pride in the fact that mixture of experts is like a good way for people who might not be like reading every single archive paper working their way through every single machine learning textbook to kind of learn about sort of like what's happening more deeply in the AI space and you know I think what's kind of interesting is you can often get lost with all the stuff that's happening like at the Enterprise layer around AI like I think there's a lot of people listening to this who may actually not even know Jeff Hinton right um and um I guess I'm curious if you feel comfortable you know if you want to give our listeners just kind of a quick explanation for why someone like Hinton is so kind of important to the field and and what exactly he sort of like contributed here no absolutely so I think the first thing I would say is Hinton really is kind of the he's considered as the the OG goat of uh Godfather of AI which is which is I love that term in that sense and and he's been doing AI for a very long time U machine learning specifically he has uh been there before it was cool so he was doing that right back in the sort of 1980s right so if we think really uncool basically right yeah very uncool exactly so so but if we think of the modern foundations of what we've got today so we think of things like deep learning um that all comes down into these really deep massive neural networks with with billions and even trillions of parameters these days right and I'm not going to go into the the massive details of that but if we look at the work that Hinton has done there even as far back as sort of 2011 right when uh Alex snack came out um you know and IL were were was part of that as well then that was a time where really the Deep learning Revolution kicked off which was the sort of first kind of CNN on a on a GPU for training uh against images and if we go further back uh in time there as well so if we look at the the work that he did things like back back propagation which is a key Cornerstone of what we even do today with deep learning so all of this goes back to the 80s and Jeff Hinton Wasing doing this when it was uncool right so if he hadn't done that work we wouldn't be where we are today so I I think you have to sort of recognize that fact and as I said earlier the impact that AI is having and going to have in the future is going to be incredible so I think actually the impact that it's having in these different fields he should be recognized for the work that he's done right even even in physics right because I know there's some physicists I saw on my Twitter feed grumpy about like well what's this computer science person doing in here I guess kind of ultimately you're like actually this is significant enough that like it it actually should be recognized in this context absolutely and and I think it does open this up right so and I think it's a good thing for physics as well right you don't want to be seen as this kind of boring thing here's a bunch of formulas you know oh look here's another telescope in the sky do you know what I mean it's like this is stuff exactly AI is impacting every field and and and therefore um I think I think it's a really good move by by the Nobel uh instit Edward I'd love to bring you into this because I think one of the things I I love about Jeff Hinton in particular is just kind of how sort of down to earth and like kind of open he is about his sort of views um there's a great quote that the Nobel committee had posted on Twitter about how he was like oh yeah like I'm I was just in this like low rent hotel room when I got the news and like I I had to like reschedule my medical appointments to go deal with this Nobel Prize win um I think one of the things that Hinton has been sort of very kind of um sort of strong on I would say in the last few years is kind of warning about the sort of risks of AI and I think people have taken him very seriously because he's been at this of course for a very long time as Chris explained he's sort of like the goat hipster of neural Nets um and I guess I'm kind of curious about how you sort of think about those as kind of like a Leading Light in the fields you know do you take do you take his kind of sort of dark warnings about where AI is going seriously do you think he's on the right track you know I'm curious about how you kind of think about those those kind of risks and he I think made it actually a center of like some of his comments during his some of his interviews around this prize and so I did want to make sure that we talk about it before we move on to our next topic yeah I mean I think he's he's raising the warning uh just to make sure that that that that that voice is you know always considered right that that that that risk is always kind of part of the part of the math that uh that we're you know that enterprises individuals institutions uh you know do when when they're applying AI to to the particular problem or use case that that they're applying it to and I really think that's what it comes down to right so when we think about uh risk assessments or AI governance it's really in the intersection of the technology and the use case right it's not the same thing to apply AI to do creative writing right as we mentioned this morning uh than it is to do credit underwriting uh for for a bank right or to do a hiring decision for for an organization uh so these are very different use cases very different impact on on individuals right uh on on Society uh and and they they pose totally different uh different risks so it's really not just about that technology it's really what the technolog is being applied to uh that I think uh that I think is is needs to be assessed you know the more this technology makes its way into into National Security into defense right obviously it's a much different consideration uh than uh than than poetry yeah for sure and is this are you hearing this like um you know because you work very close to the metal in some ways right like Watson X platform is something that like customers are using and relying on I mean sometimes I think I feel like a lot of the discussion about like oh AI is really dangerous kind of takes place in like a totally different domain but I I guess I'm kind of curious I mean it sounds like you're sort of suggesting that like even day-to-day you're sort of hearing from you know customers and and the market that like these kinds of risks and these kinds of concerns are are things that people are thinking about yeah I mean they're not existential risks right but but there definitely risks uh to uh to Brand to Brands right uh their their their business risks uh their Regulatory Compliance risks uh and and and managing these risks uh is definitely one of the the top considerations that enterprises are uh uh that that that's that's really acting as an inhibitor right to to more WID scaled adoption of the technology uh and and it's something you can't really do after the fact right because so much of of uh of managing this risk is is the endtoend life cycle right it starts with the data that goes into the model and the and you know what model you selected and how you customized and tuned it all the way to monitoring and guard rails um separation of Duties right between development deployment so so all these things that uh that you kind of have to start thinking about from the beginning because if you don't then at the end they become a wall or real obstacle to try to reconstitute Posta uh so so that's we've been working with clients uh uh you know in that perspective uh with that approach uh and that's what's leading to you know some of these use cases making their way into production and I wasn't being hypothetical when I was talking about credit risk underwriting and and and hiring decisions right these These are these are real real world use cases where where the risk is being assessed mitigated uh in order to implement uh these uh these work fors yeah for sure um Chris do you want to get a final shot here curious about what you think about sort of you know I guess hinton's kind of late career turn as being sort of like a voice of warning around some of these Technologies I I like to think of this as like the difference between waterfall and agile you know which is probably a weird way of putting it which is go into that more if if we think of waterfall projects nobody does waterfall projects anymore because we realize that we are too dumb and I mean this in the nicest possible way to figure out every requirement in advance and be able to plan everything because the world is too complicated I sort of feel that way about AI risks I think we are too dumb to figure out every single risk and every exploitation and be able to get ahead of everything in advance and pre-planned so therefore I kind of think like a software project I think we need to be agile which is you need to experiment and then you need to discover in a safe and controlled fashion what those risks are and let them evolve and that means we're going to do dumb things we really are right but then in the process of doing dumb things like you know sticking your fingers and you know a wall socket or whatever you realize oh I better not do that right and then you put safety things in there now I'm not saying that we should go that far with AI but I I hope that history tells us that in human existence um we've done enough dumb things that we shall do a enough dumb things before they become catastrophic dumb things so I think a little bit of agility will help us discover that stuff we need to have control but I don't think we are going to all blow ourselves up because I think we're going to do much dummer things much earlier that is my [Music] opinion I'm going to move us on to our next topic um there was an incredible photo that uh open AI uh put out onto social media it's of the team celebrating their receipt of the new Nvidia dgx b200 um and it's a great photo because it's like you can see clearly that everybody is so jazzed to be standing next to this fresh new piece of compute that it's like Christmas morning you know it's like people are so thrilled to get this computer in their hands um and I think it's a nice cook to talk a little bit about this kind of next generation of uh platform that Nvidia is rolling out um and is actually having a really material effect on the market for compute right so um there's a great chart I saw earlier in the week about how sort of the prices for NVIDIA h100s right which were sort of like last season's got to have it Hardware those compute costs are just dropping all of a sudden right as these new boards are kind of coming available and online and so I think it's a nice hook to talk a little bit about what's happening in the hardware markets and I think you know maybe Edward I'll turn to you first you know is what we're seeing here just more speed right like I guess there's one kind of point of view which is it's Christmas morning because it's really cool to be standing next to what's basically like an F1 racing car for compute but like is what we're getting here largely just faster and if not you know what's different about it yeah I mean I think I think it connects back to the first topic we talked about right the the the the the evolution of this technology and really trying to to build it in a way that uh somewhat models the way our brain works right and and and this kind of almost Infinity uh I know that's a big word but uh uh of of of nodes and connections and and relative strengths between them right so so I it's not just speed right it's it's it's it's scale right and the capacity to to consume more data and to have more nuanced uh relationships between between that data um so I'm I'm not a hardware expert uh but uh but I definitely uh I think it's I think it's a great time in technology when Hardware matters again right I think we go through Cycles where like Hardware becomes totally commoditized and then and then it matters again uh and then eventually becomes commoditized again right uh so we're definitely in a in a stage where it matters um I think that I think that's a signal right that that the Innovation right the The Innovation Frontier is is active and and moving rapidly and I think that's all very positive yeah I mean I think Chris it's it's stunning I was talking to a friend of mine who is working on some of these clusters and he's basically like the hardware is literally moving so quickly here that they can only really afford to do like one big training run on a cluster they've built and then almost immediately they start moving to building the next cluster that they're going to do training right on um I guess I'm kind of curious here as someone who kind of like thinks about this and researches in this space you know where's this all going right like are we just you know is is the cluster just going to get bigger and bigger and faster and faster you know like is there a top this this top out at some point or you know what what is the trend here in the next like 12 to 24 months I I think there's a couple of Trends going on and and I I and I think I might have said this on another episode but I'm going to say it again it's like it's almost like following the Bitcoin Trend right which is if you follow a Bitcoin Trend everything started on CPU then it moved from gpus and then it moved to fpgas and it moved to as6 right so so basically you went from kind of CP you went from you know compute being CPU to being GPU bound and then you were going to custom hardware and we we're kind of seeing the same thing again because people you need to bring the cost of compute down you need to bring the cost of training down I.E you've got bigger and better models to train but actually I think the bigger thing is on inference right so you got to run these models at a low cost and speed now if if there is one criticism I would say of Nvidia uh over anything is that the speed of tokens per second and the cost on these gpus are quite expensive and you've seen this in the marketplace already this is where folks like grock have been coming in right and they've been sort of releasing these chips that go really really fast and then IBM's got their North Pole chip as well right um and then Google's got their TPU chip so everybody's trying to bring down the cost of inference because if you're running these massive models on the cloud everybody's consuming compute you that to be as cheap and as fast as possible the big thing if you look at these new Nvidia boxes right is yes the the the training speed was much faster but actually if you look at that chart the cost of inference the speed of inference came down massively right so they've obviously put a focus on that as well because they know that if they don't improve the inference speed if you don't improve that influence cost then all of these other providers are going to start eating their lunch as well right because everybody's is going to go cheaper and I and I and I but I think this push and pull between kind of general purpose GPU and sort of custom chips is really important but again in the training point of view different from inference everybody's just focused on I need to get the biggest and fastest mod I need to get my model out really quickly and therefore you know throw away your last card put in the the latest card because I just need to get my model out all the time so there's a different Dynamic that's going on over time you know you're going to get faster architectures you're going to get different architect it's going to get cheaper and and these cost uh speed performance ratios are going to change over time yeah the architecture I think bit of this component I think is a really interesting part of the market right I think like one one theme that we've had pop up on a lot of mixture of experts episodes is customers want smaller models they want faster models uh they don't want the gigantic model that's really expensive right um and so there is that pressure there but it feels like there's kind of two ways of getting there right one of them is well we start marketing just smaller models right where we like lower our demand the other one which you're arguing is well the chips get good enough that the cost of inference finally Falls for running larger and larger models and the two are kind of like in a little bit of a race it sort of seems like um and I don't know predictions on kind of who wins that race in the end because you can imagine like the the market might eventually settle and say hey look these models do 99% of what we need them to do we don't need cre near AGI models to do this work so at a certain point you just don't need the chunkier model right I guess there's another point of view which as well but if the cost was cheap enough you would still go bigger um and I guess I'm kind of curious about like how you think about that relationship it's a little bit complex and it's unclear where it lands in the market today I think it's just going to keep pushing and pulling right because we are going to want to run our models on device right if you think of things like apple intelligence Etc so I think smaller models and faster compute are just they're you're going to need both for a while will one win it yeah and will one it win out I don't think so because the smaller that you can make the models and the faster you can make and smaller you can make the chips then the more you can put them on embedded devices which open up a whole set of other scenarios which are kind of low latency and and again you even see that like this week so what uh llama 32 was out last week and they released their their 1 billion model and their three billion model I think it was right and and again just smaller models and and I think the big thing there is folks are getting really good at taking these larger models and Distilling them down into into much smaller models and that's going to continue and I and I think we're looking at 1 billion parameter models but let's project forward maybe uh six months a year you're going to then start to be back into the million parameter models and then the chips are going to get faster and we're just going to go back and forward back and forward and it's forever yeah um yeah Edward are you seeing that in the market I mean it feels like one kind of interesting outcome of what Chris is talking about is that there's a lot of Market pressure to like have a lot of the models just more on edge devices everywhere um and it strikes me that like you know part of the idea of a platform is you're running it in the cloud and all the advantages of cloud but it does seem like there's actually really powerful kind of economic incentives eventually kind of pushing us to sort of all on device here you know not really like in the model that we're familiar with do you think that's like a real possibility going forward I mean I think it's going to be all the above uh and you know and we're we're the hybrid Cloud company right so so Edge to us you know is definitely a Continuum right uh uh the data center right compared to the hypers skater cloud is is effectively a type of edge um and then you go down to facilities and and eventually you know devices um so so yes it's going to be it's going to be all of the above and and and finding the right balance is always uh is always very specific to the requirements of the of the use case I mean I think what what what we see a lot right is clients to get started use a big model because that's a that's a way of you know accommodating a very broad range of of requirements use cases languages uh all sorts of things right so so so so you kind of prove out the business case with a with a big big model uh that's that's going to help you accelerate right but then when you're there you're like okay how can I do this as cheap cheaply and with the least latency as possible right uh and and now you start to really kind of optimize um and customize right uh once you once you've validated that uh that that business case and really want to want to scale it uh with uh with with real economics behind it so you know it's it's it's like you use the the Swiss army knife right uh it's going to give you a lot a lot of flexibility but eventually you know you're going to want to use that fit forp purpose tool to get the job done yeah that's super interesting I never really thought about it as kind of like this life cycle but it's sort of very interesting that like well just for the pilot we use the biggest baddest model because it gives us the most optionality and then as an organization kind of Tunes in the use case it gets kind of like much more discret and smaller and you're optimizing for cost and all these other sorts of things um it's very interesting is this the time to mention agents I realize we haven't mentioned the word agents in this episode yet so I mean we're not contractually obligated to talk about agents but if you want to mention agents Chris go for it you can do the final uh uh hot take before we move on to last topic this is needed for agents because you need your agents are going to be highly specialized they're going to work together and they need to have low latency Etc so actually the smaller model and being being able to run on device and being able to run in you know whether it's on data center on device and running different locations that is 100% necessary for this agentic world that we're in so uh you know so it's a good thing agents for sure agents uh a lot more to get into there for sure [Music] sure so for our final story of today I really wanted to make sure that we had a chance to talk about a company called unstructured which recently closed a $40 million round um and uh this round was led by IBM and Nvidia and a long list of kind of prominent companies and investors in the space what's most interesting about inst structured is it's a company that focuses purely on transforming unstructured data into structured data which is not something that you normally think of as being something that you'd invest $40 million in so I want to make sure that we talked about it first Edward if I want to bring you in just like if you want to resolve that mystery for some of our listeners like why is unstructured data important and why is structuring it incredibly incredibly important for AI yeah well unstructured data is most data uh nowadays right and and I think the most relatable type of unstructured data the most usable type of unstructured data today for for llms is uh is document data right so so um could be could be the the content on the Internet or or or word docs or PowerPoint presentations right but effectively document data and that is that is Enterprise knowledge right that is that is what runs the world right it's it's these documents uh in this language information um and and and that's what large language models are built on right that's what they're trained on and that's what they're excellent at processing summarizing uh and and and making making usable right um so so bringing bringing that data bringing that Enterprise and institutional knowledge to the models is really the way in which uh in which an organization can make it their own right customize it to the knowledge of their business the language of their business the tonee the entities the relationships right the the values all all all the things that you need to to put a a model uh in service of of a business right or a goal you need to do that by by effectively teaching it right uh with with uh with your data and and that's what this company uh focuses on i' I've met them uh they're very focused I think that's really been part of their strength and success they're very focused on taking that unstructured data that relies you know in different locations and and and and different formats and then make it available for for the models particularly uh in Vector stores right for uh for retrieval augmented generation as an initial uh use case which is effectively Universal at this point but then beyond that you know identifying relationships in the data for graph rag taking the data and putting into structured format to really increase the Precision uh and accuracy of some of those queries so I think I think rag is uh you know very popular really valuable uh but already kind of running out of uh out of gas a little bit for the next evolution of uh of use cases and and that's really all about kind of continuing to unlock the value of the data in those documents huh yeah that's really interesting can you go into that a little bit more is like why why is rag running out of steam it's kind of like again it feels like 12 months ago is like the new hotness right or like people were still definitely like leaning into it as the kind of key strategy for doing retrieval what's what's missing I guess what's what were the cracks appearing yeah I mean I think I think it's a great starting point and I and I think think it's I think it's essential in in most cases right but but for example graph rag um is going to give you the ability to have richer contextualization right by identifying non- obvious relationships if I prompt the model with you know a certain set of words uh it's it's really only going to limit uh its ability to to to reason right including uh retrieving the the the uh the knowledge base to to that domain right and there may be hidden relationships right there may be for example if going to search something about Facebook but I don't get a response about Instagram then I'm not really getting the whole picture right uh but the model is not necessarily going to know that Facebook and Instagram are are related right because those relationships could potentially be non-obvious right uh so so the graph rag um pattern right it's going to give you uh strength in relationships uh that are nonobvious and in doing so provides you richer contextualization that will be more relevant right to the question being asked even if it's not asked with those specific words right so it's it's again MIM mimicking a little bit of how how how our brain works uh in in in identifying those relationships that's that's one example and but even that is not necessarily going to be perfectly accurate right because there's data about transactions that may have like a skew a skew number or a particular you know ID has no semantic value it's just a bunch of characters it's like it's like your license place it doesn't really doesn't really mean anything right so so so you need to have that type of data in structured formats and really combine rag or semantic search with with SQL right was with structured query queries and that's going to give you more accurate responses to questions that have you know to do with uh transactions or or other types of data that are that are very you know very important to a particular business very important to particular domain but don't have semantic value in a conversational or language sense right so now you have to complement rag with a different dimension uh which is structured data so those are just two examples right of of how how you really need to complement kind of classic rag uh to make it more more more accurate that's really helpful Chris you know again I think there's another I think this story made me think a little bit about like the market for data structuring which I think is really interesting which like we normally think about like okay the people who generate data the people who do the training the people who offer the models to the consumer as kind of the supply chain and one Link in that chain I have really thought about is just like this layer that exists between like the data that's out there and like the data that's usable um and I guess one question I want to ask of view is that it feels like there's lots of different potential ways you could go about doing that right there's companies like unstructured where like we have a Specialized Service that does the structuring of data for you you might imagine that um you know that the the foundation models themselves become good enough that they can do the structuring kind of out of the box you don't actually have to do much additional post-processing to make it happen you could imagine that uh synthetic data gets good enough we don't even need the UN structure data because we can just generate a purely you know out of nowhere um and it feels like there's a lot of contenders to the throne of getting data that's usable um I guess how do you size that up like do you think that at some point say like synthetic data just gets good enough that you know you don't need to do this data structuring anymore or is there like always going to be a niche for this kind of structuring kind of business just kind of curious about how where you think this Market is going oh goody I get to say the word agents again my fa yes please do yeah we got to get a few more in before the episode's up so so actually I think everything is GNA move into a Marketplace in the future so I do think we're going to have a Marketplace of data we're going to have marketplaces of agents and we're going to have marketplaces and models and I think we are going to get more outcome focused so specifically on the data I think we're doing a lot of human work at the moment to curate that data and even if you look at things like stretcher Etc they do great work because they're actually taking away a lot of the complexity to get your data into your kind of vector databases to follow rack right because it's it is really hard you have to do things like chunking you're constrained by the uh the context of the model I.E the short-term memory that it can work within you have to work out which data is going to be associated with wall as Edward's saying you need to start building up things like relations and then you've got to understand okay I've got to get this data from this form I'm getting this from an S3 bucket I'm getting this from here it's really complicated but actually we are even though that's a faster process we are still humans who are figuring this out and doing Transformations Etc and doing these sort of ETL pipelines if I project a little bit forward in the future back to our earlier discussion where the models are going to be smaller they're going to be uh have latency they're going to have faster tokens per second you're then going to be able to train these smaller models to start to do that restructuring work for you and therefore uh you I think you're going to be in this world where agents are going to help you get your data into a structured format and once your data is into a structured format you're going to be able to train your model and then you're going to loop around and you're going to be in this nice virtuous circle so will there be a Marketplace for this absolutely because at the end of the day people own data right so the uh the publishing companies the media companies the they're all sitting on gold mines right um at the moment because that's data that is highly valuable highly creative there are things that are probably can be synthetically generated so things like all the math data Etc you could probably argue you know that will just be commoditized over time because that will just get generated and synthetically created and that would be the same for anything that our puzzle Games Etc so there will be this push and pull of who owns that data and and I think that human data in especially the creative spaces will still be highly valued um so um I don't see the record companies giving up their ownerships of or songwriters of yeah exactly so I think that's going to be the push and pull that we have over over time but we are going to be moving into this uh marketplace where sort of that soft IP is just going to be uh the big thing that distinguishes companies because one of the examples I like to give is if you have got a model trained and you have the data of all the kind of Spanish legal texts and you've got that structured Etc and your model can answer uh Spanish legal queries better than any general purpose model if I'm going into court you know what I want the the model that's really good at Spanish law as opposed to the model that's got a vague understanding of Spanish law because that's the difference of me getting a large fine or going to jail right so you know so there's a huge value on that locality and I and I think that will be one of the biggest Trends as models are going to get more and more specialized and we're just going to be like like we've been having with the general purpose benchmarks MML use and all that we're going to have a benchmark for everything you can imagine Tim it's going to be here's the Spanish legal Benchmark here's the car parking benchmark that you name it is going to be benchmarks everywhere and we're just going to be in this big massive Marketplace of specialization I I love the image of of of hiring an agent AI agent attorney right to defend you in a in in a case I mean I think I think that that is that is a that is a feature I can get behind I used it myself I did a an insurance claim I looked at the insurance document and I was like I have no clue what any of this means so right and it was a kind of medical condition thing and I was like I run through the llm it's like huh tell me gave me the the key points brought to the insurance company pay out and you're like you know uh that is this could go somewhere exactly that's what you want from from these things so I'm I'm yeah I but we are we are going to be in a wild ride we are we are going to be having like the kind of the Uber style marketplaces where you're matching up AIS to people AI to AIS it's going to be wild over the next few years anded do you want to final to close us out for the day uh agents agents uh absolutely I mean I I uh some of the work we're doing uh at IBM with agents is super exciting uh and and it really is going to kind of I think it's going to be a step function right in terms of the the the complexity of the of the workloads and the use cases uh the the creativity right uh to to solving problems uh um potentially Beyond even even even our approaches uh the the automation right the fact that you know you're going to have so much work happening 24/7 365 a lot of stuff already works that way right but but this is going to take it to the next level um and uh and and I think it's it's it's exciting it's productive um I think it's going to level the playing field for for uh for for for consumers in some cases uh for smaller institutions right so so uh we're we're we're excited um uh to be part of this future and and to really be co-creating it with h with our clients and uh and our community well gentlemen this is um wonderful uh Chris you're always welcome back on mixture of experts um and uh Edward I hope to have you on uh some point in the future listeners out there if you enjoyed what you heard you can get mixture of experts on Apple podcast Spotify and podcast platforms everywhere and we will see you next week for another Roundup