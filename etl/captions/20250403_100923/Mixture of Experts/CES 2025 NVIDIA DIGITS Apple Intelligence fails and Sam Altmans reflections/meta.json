{
  "video_url": "https://www.youtube.com/watch?v=KwGg5WhxuFY",
  "video_id": "KwGg5WhxuFY",
  "title": "CES 2025, NVIDIA DIGITS, Apple Intelligence fails, and Sam Altmanâ€™s reflections",
  "upload_date": "20250110",
  "channel": "IBM Technology",
  "duration": "35:31",
  "caption": "What are you most excited\nabout coming out of CES? Shobhit Varshney is Senior\nPartner Consulting on AI for US, Canada, and Latin America. Shobhit, welcome back to the show. What do you think? NVIDIA's DIGITS. The supercomputer right next to my laptop. Mwah. Love it. Great. Uh, Skyler Speakman is a\nSenior Research Scientist. Skyler, welcome back. Uh, what are you most\ninterested coming out of CES? As a long time PC gamer, absolutely the\nnew line of graphics cards coming out. And finally, last but not least,\nVolkmar Uhlig is Vice President, AI Infrastructure Portfolio Lead. Volkmar, uh, what do you\ntake out of CES this year? I'm with Shobhit, it's the DIGITS. All right, all that and more\non today's Mixture of Experts. I'm Tim Huang, and welcome\nto Mixture of Experts. Each week, MoE is dedicated to bringing\nyou the debate, news, and analysis you need to keep up with the top\nheadlines in artificial intelligence. Today, we're going to be talking\nabout a new report on developer use of AI tools, some trouble with Apple\nIntelligence, and Sam Altman's reflections on the second anniversary of ChatGPT. But first, let's get started. Let's talk a little about CES and\nShobhit, maybe I'll kick it to you first. We're all very excited by DIGITS. For those of us who are not obsessively\nwatching all the headlines coming out of CES, what is DIGITS and\nwhy are you so excited about it? So the intention here is, uh,\nshrinking their DGX, uh, all the way down to a small machine, So NVIDIA has figured out a way to\nsqueeze in a lot more, uh, firepower. Their graphics, uh, GPU card with an\ninsane memory, 120 GBs attached to it. So you can start to run some really\nlarge AI workloads on your desktop. Imagine a 200 billion parameter model,\nwhich is way bigger than what ChatGPT was when it came out two years back, right? You're able to run that locally,\nright next to your machine. Now, it comes with a flavor of Unix on\nit, but you can obviously, instead of Linux, uh, you can have your Mac and\nWindows use that as a server, and you can do some really cool things, right? But now you're talking about having\npersonal supercomputers that you can literally keep on your desk or\npotentially even carry with you. It won't be out till May. It's about $3,000, which just looking\nat the hardware that's going in that itself is a ridiculously great price\npoint to go deliver that, but this starts to move computing from the cloud\nsupercomputing all the way down to your desk, so petaflops of compute at your\ndesktop, and that just is an insane value. Yeah, absolutely. I know Volkmar, you were saying that\nyou were excited about this as well. I know, I think we've talked about\nit in the past, but if you want to give our listeners a little bit of\nan intuition of why is NVIDIA moving into this market at all, right? Like, arguably, doesn't this put them\nin competition with like, Apple and all these other kind of, you know, kind\nof desktop personal computer creators. Whereas NVIDIA's usual thing\nhas of course been data centers. Do you have a sense of why\nthey're moving into this market? Yeah, I would not say that NVIDIA\ntraditionally is a data center company. They are a gaming company. So, and the data center kind of came\nalong and hit them three, four years ago. Right in the face, yeah. Um, yeah. And you know, good for them, they\ncaptured it and was just visible in their market capitalization. Um, I think what NVIDIA is figuring\nout right now is that, um, the development market, uh, or developer\nmarket was kind of limited to, you know, buy an RTX and stick it into\nan, you know, developer machine. Uh, and now they are effectively\ngoing all in of saying we need to cover this whole value chain creation. And I think it's very, very hard, um,\nlike today, because you, in fact, you need to buy, um, like a Windows or Linux\nbox and then, you know, you, you stick in a bunch of NVIDIA cards and, you know,\nyou rake this thing up and now they are effectively coming out and saying, okay,\nhere's a ready to go system, which is optimized for that specific workload. Um, I think. When you see what Apple did with\nthe M-, you know, M1 to M4 now, they are effectively trying to\ncapture that desktop market. And that is not CUDA,\nand that is not NVIDIA. And I think NVIDIA is doing\na preventive strike here. And if you look from a pricing\nperspective, they're sitting right between the smaller Apple, you know, Apple Studio\nfor $2,000 and the bigger Apple Studio for $4,000, and so they are at $3,000\nand they have specs which are bigger. And so I think it's, um, and, and\nnow it's also, it's an attachment, but it's at the same time, you can\nuse it as your primary desktop. So I think they are- they are are\neffectively trying to cover their bases. What will be interesting to see\nas, you know, what people are now doing, if you can just, for\n$3,000, you can get that box. It's not a DGX, but you know, in\nmany cases, it may be sufficient for running small scale training jobs. Um, and so I can, you know, imagine\nthat people are just buying them by the truckload and putting them in\nup in data centers and giving their developers, not necessarily something\non the desk, but, you know, maybe it's tethered, but it's on, on premise. Um, and so it's a really\ngood way of actually getting that development loop going. And you could even use it for\nproduction use cases, right? So if you don't need a 19 inch rack\nsolver, you could use something smaller. They, I think at three different points\nin the press release from NVIDIA, they talk about how easy it is to take the\nmodels that you've trained on your small DIGITS and move it to NVIDIA's cloud. So I also think they're really pushing\nfor this hook here in order to drive more business to their data centers. And one of these is start small on your\nown personalized local system and make it extremely easy for you to then scale that\nup onto, of course, their data centers. So I think that also plays a\nlot into the strategy of why they're really pushing this. Yeah. Shobhit, maybe to turn back to you,\nWhat do you do with a petaflop? You know, it's like, it's kind of\nfunny, like, because it is very exciting, you know, a supercomputer\nliterally on your desktop, but like, with that level of computing\npower, what, what do we use it for? I mean, is it, is it just gaming? Do you anticipate people doing\na lot more homebrew AI stuff? Um, what, what, what\ndoes this unlock, right? If it just really\nbecomes super successful? So I think that the two different markets\nhere, one is enterprise, one is consumer. Right? I think, uh, from there will be\nsome enthusiasts that'll, uh, on the consumer side that'll\nobviously gravitate towards it. But I think there's a huge\npotential on the enterprise side. Uh, what that gives you is being\nable to run compute that's closer to where the action is happening. So think about industrial applications\nwhere on the factory floor, you want compute to be right next to where the\nmanufacturing of everything is happening. Or, one of my clients, uh, large\nauto industry, they have a lot of trucks and buses and things of that\nnature, and you would want to have some mobile compute that you can\nactually run a model on, right? In a lot of these use cases, there's\na lot of latency between calling a server or a cloud API and being,\nagain, getting responses back, right? Those are expensive. So imagine you're taking, say, a picture\non the manufacturing conveyor belt, right? You want to be able to process those near\nto where the images are being captured there's less latency and there's a huge\nsecurity concern here right you want to make sure that the data especially if\nit is related to something that's very sensitive you don't want that leaving\nyour premise either so you want to be able to run those closer to it same thing\ngoes for say defense applications where you are doing something more tactical\nin the field you want to be able to compute all the images coming in from\nall the drones and stuff at the, at the particular place, because you may be\nin a territory where you really don't even have a cellular connection, right? So all of those are heavy computing\nworkloads that used to traditionally take cloud environments to go scale up and run. That you're now being able to do closer\nto where the action is happening. That's a huge, huge unlock\nof value for enterprises. Today, we've been constrained by some\ncutesy little small, uh, models that'll be running on mobile devices and things of\nthat nature, but we're not quite there yet where you can run 200 billion parameter\nmodel right next to where the action is. Yeah, that's really exciting. Well, a lot more to pay attention to. Um, I'm definitely going to\nget one as it sounds like many of the folks on this call are. So we'll definitely have to compare\nnotes once they start arriving, uh, on our respective desktops. Tim, apart from the DIGITS there were some\ninsanely good things that, uh, NVIDIA, uh, released, uh, during the keynote. There were, like, three different\nareas that, uh, Jensen wanted to ensure that people realize that this\nis what NVIDIA really does, right? So one was in physical AI, figuring\nout a way in which we can model the physical universe around us. a good set of starter AI open source\nthat can understand the physics and we can start to train things around it. That leads to things, things\nlike robotics and humanoids around us in our environments. Right? The second big area of unlock was the AI. automotives. Figuring out how do we do autonomous\ndriving, and you need the whole pipeline of millions of sensor data coming in. How do you process that and make decisions\non the, on the vehicle itself, right? And then the third one was around\ndigital workers, agents doing regular day to day work as you and I do inside\nof all the softwares that we work with. Jensen spent 90 minutes on this\non stage wowing the audience. That's no easy feat, right? If you analyze the entire 90 minute\nconversation you start to realize how an incredible communicator\nhe is, breaking down a concept, complex concepts into such clarity. So in each of those different sections he\nproved that NVIDIA is in fact a leader. They are making some bold moves to ensure\nthat the ecosystem comes along with them. They just bought, run:ai for\nmaybe $700 million they turned around and open sourced it. It's such a baller move, $700 million\nand then you open source it, right? So they're trying to ensure that\nthe entire industry moves closer to this physical AI and agentic\nAI and autonomous driving era. And they want to be the backbone\nacross each one of them. Uh, last year they had, uh, in the, in\nthe gaming industry and, and Skyler's going to, uh, chuckle on this, right? The four, the, the four, the\n40 series of their, uh, of their chips used to be $1,600. They just released an equivalent\ncompute for $550, right? So just imagine, Apple will never do this. They'll never take a $1,600 thing\nand the next iteration being a third of the price, right? So you're getting to this point where,\nNVIDIA wants to make sure that the compute is as easily accessible and\ndemocratized as plugging into electricity. But they want to be the electric\nsuperpower of the entire world. And if you look at those three\ndifferent areas, my hot take, NVIDIA is undervalued right now. IBM is out with a new developer report, um,\ntaking a look at, uh, developers views on the use of AI tools in their workflow. Um, a couple of very interesting data\npoints, but I think the place I wanted to start is, uh, really, I think on this\nreally interesting result where, you know, the developers were asked, okay, so\nwhat do you want most out of an AI tool? You know, the comment was, well, we\nwant things like trustworthiness in the AI, and it should be reliable and\nall the things that you would want. And then they were asked, well,\nwhat are the current problems with the existing AI tool set? And it was exactly those same things. And so I do want to really ask this kind\nof question of the group, which is, does feel like despite all the hope- hype\naround code assistance and agents in developing and all this kind of stuff\nthat we've been talking a lot about. Um, it seems like ultimately that\nthere still is this big trust gap and it is actually preventing\nadoption of a lot of these tools. And I guess maybe Skyler, I'll\nturn it to you first is, you know, do you see that as a big problem? Like, do you think that it's\nultimately going to kind of put a ceiling on the use of these tools? Um, and, and what, what\nshould we make of this? Like I'm, I'm kind of, it was sort\nof an interesting result for me. I'm not sure about a ceiling is the\nright term, but certainly delay. Um, I spent, uh, a good time, uh,\nlast year, end of last year, um, in San Francisco at this International\nNetwork of AI Safety Institute. So this big congregation. And the topics are, of course, around\nsafety, robustness, trustworthiness, and those are the topics of the day\nin this and, uh, here when I talk to would be clients, they aren't\nconcerned about overall accuracy. That's not their concerns. It's how are these machines reaching\ntheir conclusions and can we trust them? That's the back and forth we have\nnow, not accuracy or even costs. So it's a concern at a global\nlevel and even at just kind of an individual client engagement level. So yes, it's been part of an IBM\nresearch strategy for many years now. What can we do with trust\nand governance in this space? Lots of lots of work to be done there. Yeah, that's right. And I think there's kind of one point\nof view and Volkmar, I don't know if you agree, you know, working with a lot\nof folks who are kind of in the nitty gritty of the technical aspects of this\nis, you know, I think the AI person's response also is, well, what do you care\nabout, like trust or reliability, if it just works, then it just works, right? Like, you kind of think about like\nthe early days of like Google, where it's like oh the Google Image\nSearch, there's this GPS thing that's going to tell you where to go. Yeah, sure I don't trust that. And then over time it just turns out\nlike the fastest way to get from point A to point B is just to put it into GPS\nand kind of people get over uh, Like their fear about not really knowing\nhow these systems make decisions. Do you think that'll kind of be\nthe case here with kind of these, all these developer tools that\nsay we're going to do code gen. Uh, and you're like, I don't\nreally need to understand cause it just works and I'm moving faster\nthan developers that are not. I don't think so. So the way right now the development\nworks usually is, and I hope this is how it works for most companies, is you\nuse the code generation kind of as like, okay, I know what algorithm I want. And I can, I can proofread it. So I can proofread code about 10\ntimes faster than I can write code. Right. And so if I go and I need to build\nsomething, I'm just going to an agent and then the agent produces the code. I'm still checking that the code works and\nthere is still, you know, an architecture behind it where you are saying, you\nknow, you're kind of interacting with the system and you're, you almost have a, um,\nyou know, uh, an engineer at your hand who is very fast and doesn't get tired. So, uh, and, and you still need to do\nall the engineering practices we have. You still need to write unit\ntests, you know, you still need to write integration tests. And so there is a, there is a rigor to it. Now, if you have bad engineering practices\nand you don't write unit and integration tests, then you may actually put, you\nknow, litter your code base with bugs. But that's more of an organizational,\nstructural problem, right? So do you allow code which is\nuntested in your code base? And, you know, a developer can make\nmistakes and the model can make mistakes. And we are primarily now asking,\nyou know, who has the higher likelihood to get it right? Um, in the end, confidence in your\ncode base will always come from, you know, test coverage and reviewing\nthat the tests are written well. And typically in engineering, you're\nsaying, you know, your test should be 10 times easier to understand than\nthan the code you are actually writing so that you actually know you it's\neasier to to check that the tests are correct than the code itself is correct. If you follow those practices\nI think you will discover the bugs which which get introduced\nbut if you don't yeah good luck. Yeah definitely. So do you think that the this report\nis mostly just revealing the fact that you know effectively the sort of AI\nengineering is still more buggy than humans like that effectively like the\nlack of trust is kind of well warranted. I think we are not at the point\nthat I can go blindly to a model and say produce me 10,000 lines\nof code And they will be correct. Um, I think the big challenge is\nthat Um, you know, humans are lazy. Um, and so there is, there is a\ntendency that we are overconfident what the model is doing. Uh, and if you do that and we are not\nvery skeptical about the output and we don't, you know, review it, we will\nactually get bugs into the code base. Um, I would flip it around and say the\nmore Um, like, open ended question we have right now is where we are actually putting\nthe model in the middle of the execution. So there's one is the, is the, you know,\ncode generation, but I can review this. What if the, if the model\nactually executes code? And we see this right\nnow already in ChatGPT. You ask it a random question, and it\ngoes out, and it produces actually Python code, and then it runs the\nPython code, and it gives you an answer. But then you look at the Python\ncode, it may be buggy, right? And so sometimes the code doesn't, doesn't\neven, you know, like, um, when you do data aggregations, uh, you know, you\nhave like a table and has like, you know, five values in the first column and seven\nvalues in the other column, and then says, oh, Panda, sorry, I got an exception. So that, you know, and\nthis happens in real life. And so you get these answers,\nwhich are just bogus, uh, simply because the code generation and\nthen the code execution is wrong. And so that's, I think where it\nbecomes much more scary where we are doing this on-the-fly code generation. Uh, and I do not think that with the\ncurrent accuracy, we are there yet. And so, for small things that may\nbe okay, but for large things, I think you still need human eyes. Will that go away? Yes, probably. Over the next three, four years,\nwe will get to a point that, you know, the code will be better\nthan what a human can produce. Shobhit, to bring you back into this\nconversation, I gotta believe that this is like your life, right, is like,\ncustomers and clients saying, well, I don't know if I trust this stuff, and\nthen you being like, no, the water's fine. Um, I'm curious how this is kind\nof playing out in your world, because it feels like this is\nlike a conversation that you have day in, day out, all the time. So, from an IBM consulting perspective,\nright, we have very strict, uh, guidelines and warranties and things of that nature. For any code that IBM produces for\nan end client, we have to be bound by what our master service agreement\nsays, and what will go into the code, is it copyright free, things\nof that nature as well, right? So there's a pretty high bar\nfor when our team members are producing code for our clients. And I think over time, you're\nstarting to see that the quality of the engineer that is leveraging\nthese copilots matters a lot, right? If you are a software architect,\nsomebody who's senior, who knows how to make interns work for you, right? So say we get you some brilliant software\ndevelopers and they have these parts of brilliance, they'll show you some\ncode that's like, oh my God, I don't believe that this intern wrote this. And you realize that they actually\ncopied it off from- from Stack Exchange and they modified it a little bit\nor something of that part, right? So it was brilliant, but it was\nbecause they had access to other things and stuff like that, right? But unless you know how to judge\nthat piece of code, it's very difficult for you to even think about\nputting that into production, right? So the, the bar of the manager\nfor intern is pretty high. Similarly, when you get a copilot who\nis behaving like an intern and you're trying to ensure that the person who\nis, uh, who's using that copilot should understand how code is written, right,\nto, uh, to the earlier points we've made. We need to know what good looks like,\nbut if the code is being generated 100 percent by, uh, by a copilot,\nthen it's very difficult for you to understand what logic was used. Right? Earlier you said that you can just\nproofread a code, but then you need to be really good enough and have\ndone this over and over again before to understand what to even look for. What's happening in reality\ntoday, 70 percent of the code gets generated and it works pretty well. The last 30%, the last\nmile is where we get stuck. Right?\nIt's an iterative process. It takes one step forward, but then it\nends up taking two steps backward and may introduce some other bugs, right? So unless you really know how you, how\nthe code was written, how you would have written it yourself if you had\nthe time, You're not really able to get to that 100 percent unlock of value. So this tandem between a human and\ncopilot, we also need to figure out a little bit better on how\nto ask the right questions, how to create the right test cases. And I think having an agent that's going\nto go review and be the peer reviewer for the code that's being generated,\nwe're moving towards that place. A lot of our deployments with our\nclients, when we introduce other agents to review the code, review the\nerrors, that multi agent is delivering higher quality code for our teams. Then what we got from an LLM that we\njust start spitting out the code into it. It's really interesting to think\nabout this is like part of like a maturity of the overall AI\ntool chain that needs to happen. So like the lack of trust is the fact that\nwe have this AI code gen thing, but it's really not connected to any other AI tools\naround it, sort of is what you're saying. It's a new year. We can be optimistic. One of the insights from the same,\nfrom the same study, what was the lowest item on this list of 10? The one that people, the developers\ndon't think is a problem, and I think is really interesting,\nit was the quality of the LLM. So the, these developers are, I think are,\nare correct and convinced that The LLM quality is going to continue to increase. That's not one of their concerns. Um, and it's, it's really interesting\nto see that sort of play out here as the lowest of the 10 options given here. This came up about half as often\nas the trustworthy issues did. Um, so I think that's, that's a\npretty interesting takeaway from here. LLMs will get better. How we integrate them into the\ndecision making process, that's a different story, but I think there is\nkind of a global optimism that these LLMs Are going to become stronger. For our next segment, we're going to talk\na little bit about Apple Intelligence. Uh, there was a really interesting news\nstory that popped up in the last week about how, uh, this new summarization\nfeature that was part of Apple Intelligence had been messing up, right? So this would, be a summary of your\nvoicemails, your text messages, and importantly, your news stories, your\nnews headlines that you were getting. And they found that in many cases, Apple\nwas actually summarizing incorrectly. It was hallucinating. So Apple apologized and promised\nthat they'd be doing better on the version two of this feature. Um, I wanted to bring up this topic\njust because when we talked about this earlier last year, before the feature\ncame out, you know, the opinion that we had was AI is going to be perfect for\nApple, and they're going to get this so right, and it's going to be so targeted. So I wanted to just go back\nand talk a little bit about, were we right, were we wrong? And I guess, I don't know, maybe\nShobhit, I'll throw it to you first on what your hot take is on that. I think it underperforms in\na lot of different scenarios. I think Apple, uh, is using a lot smaller\nmodels to do this on device, to dialing up on the security side of things to\nmake sure that they're, uh, they're small, can run, they don't- they're\nnot using some insanely large model to do the summaries and stuff like that. So a little bit of the performance hit,\nuh, I believe is happening because of the size of the models that they're using. And we see this in, in real world as\nwell as we're building multi agent systems and stuff like that too, right? So I think there's a little bit\nof, uh, the balance between, hey, should I, make sure that everything\nruns on device, and I'm going to constrain it only to a few things. It has to be, it cannot start draining\nthe battery and a few other, uh, uh, things that they have to solve for. Well, so do I really get a really\nintelligent model to go do these summarizations and things of that nature? Skyler, maybe do you\nhave a similar take or? I think in addition to Apple getting\nburned here, I think there's, at least from what I've seen from the\nheadlines, it's other news agencies that were using Apple technology. And so, for example, the BBC, you\nsee this BBC breaking news coming up. And it's completely made up. And so the BBC is actually\nfeeling quite burnt in this. It's not just Apple\nwith egg in their face. It's partners that they've gone with\nbecause now they're getting these headlines blasted to their customers\nwith the BBC icon next to it. Gibberish. So I think it's going to be, yeah, Apple\nreally has to really think about how. Obviously the technical challenges\nof getting these hallucinations- hallucinations taken care of. But then how do you really pass that\nmessaging on to the consumers going through another news agency the right way? Because I think, I think\nthey got hurt on this one. Yeah, that's right. Volkmar, one question I had in\nparticular for you, I was having a conversation recently where he was- a\nfriend of mine was making the argument that like Apple is ultimately like,\nthey have hardware brain, right? They like, they do hardware, um,\nand he was saying that, you know, he's a machine learning researcher\nwho is like, um, machine learning is like very different, right? It's just like, you throw a bunch\nof data at it and then the machine just sort of figures it out. And so its attitude is a lot more\njust like, you know, just try it. And then if it works, you know, then it\nwas like a basically a lot more shooting from the hip than kind of like the\nmentality of it is that is to do hardware. And so kind of from this argument, he\nwas saying, like, culturally, Apple's just like, not well positioned to\nkind of like play and win in this space because of like, kind of how\ncareful Apple is in a lot of ways. Do you think that's right? Like, is there kind of a point of view\nhere, which is like, in some ways, like Apple was slow to launch the\nproduct, and then it just can't bear like, organizationally the risk of\nthese things and so it's always kind of always kneecap that like really\nlaunching good features in the space. I don't think so. So the way- like, do you remember\nwhen Apple kicked out Google from their phone and did Apple Maps\nand it was a disaster, right? Yeah. And and they took a lot of heat for it. And now it's you know, one of the\nmain routing applications, right? I think what's happening is Apple\nwas kind of in a bind because they were late to the game. They didn't build a really strong AI team. This was very visible, like, you\nknow, I was living in Silicon Valley and Apple was just not there. Like, they were not present. Uh, and now they were effectively\nin a bind of, okay, we need to bring something out. We need to make an announcement. So they made a big splash. And they, you know, ship the product,\nthey try to keep the functionality really limited, but effectively make\na strong statement, Hey, we are going to put something on our devices. We are not like missing the whole thing. Um, and, uh, I think\nthey had to rush it out. I think fundamentally there\nis a backtesting problem. Those things could have been found. If they would have done decent\nback testing on very large scale data, they have very large scale\ndata on the devices they didn't Um, and so now they get burned. Uh, do I believe that will get fixed? Yes Yeah, I think what Apple is\ndoing is uh, it's defining on edge devices, you know, how you do deep\nintegration, I think it's still clunky, like the whole, you know, rewrite my\nemail and rewrite my text messages. It's not good. The models are not good yet. You know, we have much better models out. I think figuring out how to squeeze\nsomething into that form factor with the resource constraints you have,\nbut the power constraints you have is a really the tough challenge. Um, on the flip side, what we are\nseeing now is every generation of a new model, we pretty much get the same\ncapabilities for the next smaller model. So the 70 billion parameter model\ngets to a 20 billion and the 20 billion parameter model becomes 13\nbillion and 13 billion parameter model becomes 7 billion parameter model. And so, you know, just by waiting 6 to\n12 months, we will see capabilities which you know, have only been traditionally,\nuh, been able to do in the cloud on a, you know, like two GPUs or so\nwill be possible to run on a phone. Uh, but if they would have waited\nthe year, you know, like they would have lost the market. So I think they were in this bind. It's like, okay,\ntechnology's almost there. There's a lot of hype around it. We need to do something,\nso let's get something out. And now they burn their fingers. Mm. Yeah.\nThat's interesting. But it's cool that to think of that,\nthis is basically like Apple Maps. Again, as someone who just switched\nactually from Google Maps to Apple Maps, I'm like, wow, this\nis actually in fact way better. Uh, but it was such a funny thing\n'cause I remember the initial reputation of it was terrible. So I didn't touch it for... It was terrible like,\nyou're driving the ocean. That's how I didn't touch it for, so, And it's, by the way,\nit's still like that. For anywhere except for\ntheir Apple offices. So we went to Japan and like, you know,\nApple Maps sends you into the forest. That's amazing. Um, Shobhit do you- do\nyou agree with that? It's kind of like, uh, it seems like Apple\nis the most disappointing, but it seems like what Volkmar is saying is like, give\nit time like they will eventually win just because you can't be you can't beat Apple. So if you look at the actually, uh, Apple\ndid a lot in the open source community last year and it's fairly impressive what\nthey did with their Ferret-UI models. They have these smaller adapter\nmodels can run on on device and things of that nature. The power envelope is pretty low so\nthey've done an incredibly good job and open source all a lot of that right. There are a few things where I\nthink Apple, uh, would, has a, has a lead over some of the other\nmobile manufacturers and stuff. Understanding of what's on\nthe screen, as an example. They have some brilliant work that\nthey've open sourced that lets you understand the different elements, so\nyou can then build on, on top of that and create apps that can take actions on\nthe screen, things of that nature, right? So they've done some really good\nfundamental uh, work in 2024 and I'm expecting that 2025 they're going to\nstart taking better use of the compute power going up as well as the fact that\nnow they've learned so much the challenge with a really really small model and As\nyou said earlier this year, the model, the small models will get better than\nwhere they were last year and so forth. So we're seeing that that'll\nget incrementally better. But the fact that you are picking a\nsmall model to cover news articles from every domain, that is a challenge, right? If you're asking a small model\nto do a bespoke piece of domain expertise, that works really well\nwhen we deploy this for our clients. But on an Apple phone, you're expecting\nit to understand the nuances of negation and things of that nature on a news\narticle that could be around biology or it could be around some politics or\nsports and things of that nature, right? It needs to have the understanding\nof every term that's used in golf. That's different from the\nway you talk about it when you talk about soccer, right? Soccer versus football. Things of that nature, right? So you do need a larger model to\nthe summary, but that's the balance there that they're trying to make. And I think they will catch up in\n2025, but 2024, fundamental work that they did was, was really, really good. I think, uh, I really\nagree with Shobhit here. Like the foundational work, how do you\nthink about UI integration, uh, how to think about on device processing\nand also the offload and then also, you know, how the, the, the cross. Um, Quest data domain integration, you\nknow, understanding maps, understanding your calendar, understanding your\nemail, all that foundational work. I think it's, it's\nincredible what they did. And so I, my expectation is that\nwe will get AI kit too, where also people can bring their own adapters. Right now you can't, but that's just the\nnext logical step because, you know, you 20 big models live on a phone because\nyou just don't have the memory capacity. And so. The next logical step is like, okay, I\ncan take the Apple model and I can fine tune it for my specific domain and I can\nload my adapter into it so that I can bring, you know, new AI capabilities on\ndevice and, but have shared base weights. And so this is where, where I think\nApple did this foundational work and by saying, hey, we are providing\nthis as part of the operating system that, you know, people can build on. And this is, I think, their strength. So they will, they will do that\necosystem play and give access to it. But, you know, Apple always starts with\nthe walled garden, you know, and nobody can do anything until they figured it\nout by themselves, until they enabled all the applications and then it will become\nkind of obvious how, how you build this. And then we'll run it on\nour DIGITS, uh, you know, Right.\nExactly. Yeah. Exactly. Yeah. Exactly. So for our last segment, let's do a\nlittle final round the horn, uh, Sam Altman on his personal blog, uh, put\nout a reflections blog post looking back at the last two years of ChatGPT. Um, there's a lot in it. It's a very long blog post. I think the big thing that came out\nof it for me, um, was really just The degree to which Sam still really\nbelieves in AGI as the mission of OpenAI. He hits on it multiple times,\nand it's still the big thing he's rallying the company towards. But I kind of wanted to get the view\nof all of you on the panel on, you know, what you thought was surprising,\nwhat you thought was interesting. Shobhit, I'm curious if you have any\nthoughts on, on the blog post, and if there's anything that you thought\nwas surprising or kind of worth it for people to pay attention to. Yeah, so he's, he talked a lot about\nAGI and I think, uh, we as a community have not agreed to what should be\nthe levels of defining what AGI is. So I think we need to do a\nbetter job before we can even evaluate people's opinions on\nwhether AGI is achievable or not. If we don't agree on a definition,\nof artificial general intelligence between even humans, right? How do you even evaluate\nhuman intelligence? Ten kids in a classroom or\nin high school or in college? It's very difficult for us to\nhave a good measure for that. So the community in 2025 needs to have\nbetter definitions, just like we did with autonomous driving, different levels and\nhear the scenarios, hear the test cases, we should do a little bit better job\nof defining that before we can evaluate if Sam is, is really telling truth\nabout where, how far we are from AGI. Yeah, for sure. Skyler, any thoughts? Uh, hot takes? Opinions?\nYeah, slightly humorous take. I had forgotten that he\nwas fired and hired back. So kudos to the PR team for that\nand it wasn't until reviewing the blog that I had that trigger again. You're like, Oh yeah,\nhe was briefly not CEO. Exactly. I had forgotten about that. And so I guess that was, if you're\ntalking for a hot take of reflection and reading that, that's probably\nwhat jumped out at me as it, it just triggered that memory again. So, uh, Um, yeah, that that's my\nthat's my hottest take of that. That's right. Yeah that's that's such a funny thing\nbecause that was such a big story and I had a very similar experience where I\nwas like Oh, yeah, that was last year. So, last but not least Volkmar\ncurious if you've got any takes. Yeah, I I think it's it's a mix of both. So I think you know having been\nin you know startups and venture capital for more than 10 years. Uh, you know, I can feel for the pain\nhe is going through and you know the ups and downs and um, you know, getting fired\nfrom your own company is really not fun. Um, but I think that um, it's really\ninteresting to see the um, the product evolution they are going through. And, you know, he is pointing this\nout, like, you know, we, we did ChatGPT and we released this thing into the\nwild and, you know, it's the fastest growing consumer product, uh, ever. Right? So that's really amazing to see\nthat, you know, how, how AI took off. I think in the end, open AI created. you know, this new wave, uh, they, they\ntook the risk, um, you know, they figured it out, you know, kudos to him, um, and\nnow it's, it's really the question, like, I mean, they have, they have this, um,\nreally big north star of, like, we want to get to AGI, and if you look at, uh,\nthe, like, you know, 2024 with o1, where they actually say we, we want to get to,\nyou know, human level reasoning, and they are still innovating, and it's really\nimpressive that, you know, if you look at OpenAI, um, they are clearly the leader,\nI think, in this industry right now. They are defining, you know, the next\nsteps, and I think it's part of Sam's vision to say we want to get to AGI in\na, you know, human scale time frame. So, and we, you know, every time\nthey're releasing a new product, it's like, wow, this is possible? You know, I think they are\nstill driving the industry and everybody else is a follower. So that's, that's really impressive. Yeah, I love that. Yeah, I think that was one big\nreflection on the blog post was just this guy who's running this company\nseems himself kind of surprised about how fast things are moving. You know, it's like, Oh, yeah,\nwow, like we're doing this thing. It's only been two years. And that's like, very fun to see\nthat even he is like, continually uh, confounded by how things are happening. So, um, well, great. Well, thanks for joining us. Uh, Shobhit as always, Volkmar\nas always, and Skyler as always. It's a pleasure to all\nhave you on the show. Um, and thanks for joining us,\nuh, all you listeners out there. If you enjoyed what you heard, you\ncan get us on Apple Podcasts, Spotify, and podcast platforms everywhere. And we will see you next\nweek on Mixture of Experts."
}