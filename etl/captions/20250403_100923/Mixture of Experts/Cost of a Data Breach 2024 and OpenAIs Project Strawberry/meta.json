{
  "video_url": "https://www.youtube.com/watch?v=L1_cLO4d_zE",
  "video_id": "L1_cLO4d_zE",
  "title": "Cost of a Data Breach 2024 and OpenAI's Project Strawberry",
  "upload_date": "20240816",
  "channel": "IBM Technology",
  "duration": "23:03",
  "caption": "Is AI going to save computer security? I think there's a balance. So while new tools are helping a lot,\nthen on the other side, we are also seeing new risks that arise with AI. There is no evidence that\nStrawberry is anything at all. OpenAI does need something\nthat is significantly better than where they are right now. So I do believe that they have to\nrelease something mega pretty soon. I'm Tim Hwang, and I'm joined today as\nI am every Friday by a tremendous panel of researchers, engineers, and others\nto hash out the week's news in AI. Today, Nathalie Baracaldo, who's a senior\nresearch scientist and master inventor, Kate Soule, who's a program director\nin generative AI research, and Shobhit Varshney, senior partner consulting on\nAI for US, Canada, and Latin America. So before we get into this segment, I want\nto do our usual around the horn question. Um, and I think it's a really simple\none, but I think teases up really well to kind of get into this topic. And the question simply is, um, data\nbreaches are very expensive today. Do we think in about five years\nthat the costs of an average data breach will be going up or down? Will it be greater than or lesser than\nthe kind of damage that we see nowadays? Um, Shobhit? More. Uh, Kate, how about you? I think down. All right, great. Going down. Okay, great. Well, we just got some disagreements,\nso let's get into this segment. So we've got a couple of news stories\nthat we really want to focus on today. First one is actually a story\nthat comes right out of IBM. Um, IBM released basically a few weeks\nback a report called Cost of a Data Breach, which is the latest edition of\nan annual report they do, estimating the the costs of data breaches. Um, and it has some fascinating\nimplications for AI and cyber security. Um, right now it estimates that\nthe average cost of a data breach is rising, um, 10 percent increase\nover last year, where the average data breach costs is about 4. 88 million. But I think one of the most\ninteresting things is that it estimates that there's an average 2. 22 million costs savings in the\nuse of security, AI and automation. So that's, that's a huge,\ncrazy, crazy difference. I want to kind of get into the\ndiscussion with, uh, Nathalie to bring you in first is that's like\na 50 percent difference, right? And I'm kind of curious how you\nthink about sort of the use of AI in the security space and how these\nkind of two worlds intersect and the world, uh, the implications I\nthink for AI in the security space. Thank you, Tim. So, um, actually, I read the report and\nI'm very, very happy to see that Gen AI and like AI in general really reduce the\ncost of, uh, incidents and help a lot. The teams are really\ninvolving the security. I think there's a balance. So while new tools are\nhelping a lot, Then idea. On the other side, we are also see\nnew risk that arise with the eye. Now, uh, the amount of benefits\nthat we have with these new tools. It's fantastic. So I'm very, very excited that we're\nheading in the right direction, but we cannot forget that we do\nneed to protect those tools against adversarial attacks and throughout\ntheir the pipeline of the system. So overall, I'm very excited\nto see the entire communities heading in the right direction. Definitely including AI for, uh, auto\nverification and, and helping humans. It's really helping out. And uh, so yeah, that's,\nuh, that's my thoughts. Yeah, for sure. That's really helpful. And Shobhit, I'm thinking when you\ntalk to clients, you know, you work with clients on a wide range of ai,\ndifferent implementations and you know, the security space is something\nwe actually really haven't covered very much on this show before. Um, and I'm kind of curious in\nthe market, do you see more and more enterprises wanting this? thinking about this intersection, um,\nand I guess if there are particular use cases that come to mind where you're\nlike, wow, that's, that's really making the difference, I think, in, in reducing\nthe impact of data breaches, preventing data breaches in the first place. Um, just curious about what you're\nseeing out there in the market. Yeah, absolutely. So a very, very hot topic for all of\nour clients, and it's a two way street. There is. AI that's helping you\ndrive better security. So pattern recognition and things\nof that nature to secure things. But there's also the reverse where\nthe security teams are doing a better job at protecting AI as well. So it's both directions. We are learning quite a bit. So we've gotten much closer\nto our security services within, uh, consulting as well. There are a few things\nthat you do in security. There is prevention. There is a making sure that you're\nbeing detected fast enough, you're investigating what happened, and\nyou're being able to respond, right? The whole life cycle of it. So across the whole platform, if you\nlook at what, from a tooling perspective, you're doing things like what's the\nattack surface, how do you manage that? How do you do red teaming around it? How do you do the do posture management,\nthings of that nature, right? So there's quite a few areas where Gen\nAI has been, or AI has been able to make a meaningful difference to it. The report that we're talking about,\nthat's a, that's a massive study. I'm just to give you the scale at\nwhich we did this, there are about 600 plus organizations that had\ndata breaches in the last year. 17 industries. We interviewed, this team interviewed\nabout, um, Close to 4, 000, uh, people, senior security officials who dealt\nwith the security breaches and stuff. And we looked at the entire\nspectrum of where AI is getting involved, is being applied, right? So when you start to look for patterns\nor looking at how do I do training, so the number one reason, number one was\nhuman error or human training that's needed to prevent these from happening. So small things like social engineering. I can use generative AI model to\ncreate a very, very plausible email that will be very tempted to click. So that click baitedness of how we\ngenerate content has been applied to social engineering attacks. Right, like using it for red\nteaming is kind of what you're talking about now, right? It's like, yeah, right. So red teaming, great use case. The second one, I'm working with\na large Latin American bank. We're working on cybersecurity,\nuh, uh, pattern detection. So we're saying, here's a\nset of things that happen. Can you, can you create an early alert? based on the pattern that you're seeing. And then the same information needs to\nbe assimilated at different levels and being able to send out as alerts, right? So we're being able to automate parts\nof what a human would have otherwise done in managing the whole life\ncycle from detection, education to detection, to managing the thing, right? On these SWAT calls, you join\na SWAT call and it's been running for the last six hours. And executives will jump in and\nsay, Hey, can somebody recap? Right? That's a very easy one for us. So now we've started to generate\nrecaps of what has happened so far. Actions that people have\ncommitted to taking. So those things show up on the right side. Anybody who joins the SWOT\ncall knows exactly where we are with trying to Get a sense of. That's really cool. Yeah.\nI never really thought about that. Yeah. I think that's kind of the funny\nthing is like when you think about like AI and security or like, Oh,\nthere's a, you know, hyper intelligent machine, you know, uh, system that\nwill just defend against hackers. But I think what's really interesting\nis like a show, but a lot of what you're talking about is just like,\nhow do we optimize like the human team that's doing a lot of this, which I\nthink is really, really important. Um, okay. Maybe a final question for you to\nkind of bring you into, and I'd love to kind of get the, the researchers\nsort of view on some of this is. You know, Shobhit talked about a big\npiece of this is defending AI systems, uh, against kind of subversion or manipulation\nor attack, which is a huge issue, right? I mean, you know, I was\njoking with a friend recently. I was like, there's probably a whole\nproduct you could build that's just around kind of manipulating open, you\nknow, chatbots that people have on people's websites and that kind of thing. Um, and I guess, I don't know if\nyou want to give our listeners a sense of like the kind of like,\nsort of like state of affairs there. Um, because it feels like, I mean,\nthere's certain things that just seem like very hard to defend, right? Like it's like within a few\nminutes of any model coming out, people have already extracted the\nprompt and the system prompt out. Like that's like just something\nthat's like hard to control. Um, and so, yeah, I guess on the technical\nside from this kind of perspective of defending AI systems, curious if you\nhave any thoughts or hot takes on sort of like where we are there and if the\nkind of state of the art is getting to the point where we feel like, yeah, we\ncan actually kind of handle some of these attacks when we these systems to the wild. Yeah, well, I want to make sure we\ngive Nathalie a chance to jump in there because Nathalie, I know you're doing\nsome really exciting work specifically in that space, so it'd be great to\nto get your perspective as well. You know, I think my where I've\nseen some really interesting research that we haven't. Quite touched on yet is\nactually on the data itself. So not that necessarily the\nlife cycle, but imbuing the data itself with different protection. So if it is leaked, maybe\nit's not as big a deal, right? So there's some interesting work going on\nthat we've done, for example, with some different financial institutions looking\nat, can we create versions of the data? That are privacy protected where\nwe actually create a synthetic version of a, you know, a\ncustomer bank transaction records. We extract and remove all PII. We try and make it, you know, so\nthat you could never identify the individual and we use that data set. to now go out into the business and\ndrive decisions and, you know, have a much broader reach across organizations. And that way, if that information is\nleaked, sure, there's, you know, maybe some business knowledge that's leaked, but\nthere's not actual customer information that's leaked to the same degree. So there's a whole area of\nresearch around kind of synthetic data and making that decision. data, um, private that I think is\ngoing to be really powerful as a tool. But Nathalie, you know, what\nare, what are your thoughts? You're, you're so ingrained in this space,\nreally eager to get your perspective. Yeah. Uh, this, this question, I really\nlike it because it really touches upon the entire life cycle of the model. In my perspective, risk\nis throughout the system. And right now I'm working on something\nthat it's really, really, uh, interesting. And it's the concept of unlearning. And, uh, a lot of people find it\ninteresting that it's not learning. Uh, but actually we're removing\nknowledge from a model. So let me, it's like, we're all about machine learning. You're like doing the opposite. it basically. Yeah. And if you watch a Star Trek, there's\nthis, uh, Yoda saying, you always need to unlearn or something like that. It's because actually sometimes we\ntouch upon certain topics that later on we'd really want to get rid of. And the reality is that when we have\na machine learning model, the way that we arrive to these very large models\nis by feeding lots and lots of data. So one of the things as Kate was\nmentioning is really trying to mitigate what data goes into the model. However, because the data is so huge,\nit is really, really difficult to make sure that you filter everything. So at some points in time, even after\nwe apply defenses like we're doing here at IBM, we filter, then we try\nto align the model and everything. At some point, we may realize that the\nmodel is spilling out data that's bad. And this is going to happen just\nlike in any security, uh, kind of, uh, area, we are going to\nsee things that happen way after. Now, what do we do? We have two options. Option number one is cry. No, I'm kidding. Option number one is actually retrain\nthe model, uh, which is not going to break the problem because Think\nabout how long it takes to, to train these models and how costly it is. So the idea of unlearning is rather\nthan retraining, can we create a way so that we manipulate the model\nand forget all the information? in retrospective. And that is one of the things that really,\nuh, has got me really excited to work on, uh, because it's a new angle towards\nsecurity and it's not only security, it's also life cycle management of the model. And that is a very, very, very, I\nthink it's going to be the future. And, uh, Tim, you were asking the first\nquestion about how do I see the future? I'd see having not only guardrails\nand not only filtering, but also having this way of going back to\nthe model, modifying the model, and then make it better for everybody. And we don't need to foresee\nevery single thing that will go wrong if we can do this. So that's, uh, uh, one of the things\nthat I think it's, uh, very trendy. Nobody knows how to fully\nsolve it, but we're there. And, uh, It's getting me really excited. That's so cool, yeah. I mean, you hear it here first, listeners. Uh, unlearning is the new\nhotness in machine learning, so. I call it the new black. So this week, and late last\nweek, rumors are swirling around a thing called Strawberry. Uh, and if you are too terminally\nonline like me, um, there's a large amount of discourse, uh, about this\npotential model that OpenAI is going to release, which is going to be, uh,\npromises a substantial increase in capabilities and reasoning ability. Uh, everybody's saying that it\nmight be the model that fits. finally brings the company into level\ntwo in their internal technology tiering, which is models that have much\nmore powerful reasoning capabilities. Um, this is a really bizarre story\nin some ways because open AI has not disclosed anything publicly. Um, and in fact, most of the discussion\nonline is being led by this completely weird anonymous account that showed\nup a few weeks ago, um, that goes by the handle, I rule the world Moe, um,\nwhich is this weird account that the Twitter algorithm just appears to love. right? Basically, it's just promoted into\neverybody's feeds all the time. And it promises that today, actually\nthe day of recording is going to be the day where we're going to\nsee this godlike model emerge. And now this, this account\nhas promised a lot. A lot of people have called it out\nfor basically just not actually providing any real detail and just\nkind of adding to the AI hype. Um, and so I think there's two\nquestions I want to cover here, but maybe let's just do the first one,\nwhich is, this is just hype, right? We have like no reason to believe\nthat open AI is going to release. anything at all, um, and I guess I\ndon't know which of you have kind of been watching this, this story. Maybe I'll start with Shobhit,\nbut like, Shobhit, like, this is, this is just hype, right? Like, we have no reason to believe\nthat anything is about to happen today. Yeah, so there's, there are, he, he earlier\nsaid it was coming out Tuesday at 10 p.t., right? So he's been, you know, like\nmoving it around as well. All kinds of conspiracy theories, whether\nthis particular Twitter account is just a shadow account for Sam Altman to\njust build some excitement and whatnot. There's just so much fan fiction in the space. I can't deal with it. I'm just like, I'm just trying\nto do machine learning here. So I think just, uh, overall\nthe arch of the reasoning capabilities, uh, is improving. It's not anywhere close to\nhuman, but it is starting. The models are starting to get better. I'm very encouraged by how enterprise\nfriendly features are being added. Uh, things like function calling or\nstructured outputs, things around, uh, observability and so forth. Right. So I think we're all moving\ntowards the right direction. OpenAI does need Uh, something\nthat is significantly better than where they are right now. They have enough competitors that\nnibbling, uh, on the, on all the benchmarks and so on and so forth. So I do believe that they, they have\nto release something mega pretty soon. Uh, Strawberry, all the rumors that I've\nheard so far, it's very encouraging. Uh, we've never seen any\nbenchmarks around it yet. The models that were showing up on LIMPSYS\nand others in shadow mode and stuff, those are revealed to be the new 4.0 model and so forth. But you've still not seen any\nactual validation that these models are going to be any better. Seeing that iPhone is going to, Apple\nis going to come up with the next best iPhone, of course that's going to happen. It's just a very obvious thing. I like that, yeah, like\na prediction is like OpenAI is going to release\nsomething big at some point. Yeah.\nIt's like, yeah, I guess that makes sense. And Tim, our clients, at least from an\nenterprise perspective, we're no longer jumping up and down with the latest\nreleases of models and stuff, right? Now you're at a point where, From an\nenterprise value perspective, right? There's so much to be done before and\nafter the LLM call, there's so many other things that non functional in nature. If my data is on a particular\ncloud, the security IP, what's the licensing agreement I have on? Can I actually commercially\nuse this model? How? How have I adapted that\nmodel to my own data? So on so forth and there's just so many\nmillions of things that happened before and after earlier that has been my team's\nfocus on creating the end to end workflows with the right evaluations and so on\nso forth for the business value unlock and the model itself we keep swapping\nthat out on a fairly regular basis so our clients are not at a point where,\noh my god, this beat the benchmark by 0. 1. They're not like texting you being\nlike, what's up with Strawberry? Can I, can I get Strawberry? I actually, I do want to also kind\nof like, so that's very interesting on the business side, right? Because there's so much hype about\non social media, sort of interesting on like the really day to day, like\ngetting the business done kind of angle, like clients are not asking about it. Um, Kate, Nathalie, I would love to\nkind of bring you into this kind of on the research side as well, right? Like having worked with a lot of\nresearchers in my time, what's kind of interesting is that a lot of\nthis kind of Twitter hype doesn't really impact the day to day. Like a lot of people are like,\nOh yeah, I know about it, but I'm not really paying attention to it. Is that your sense of it? Like there's kind of this like weird\nuniverse of discourse, which is about AI, but it's like not people\nwho are actually doing the research. I curious about how you, if you're a\nStrawberry believer, a, but just how you view this whole weird new cycle,\nI guess that we're in this week. Okay.\nThanks. I mean, I haven't been paying\ntoo much attention to it. You know, it's a waste of time. Yeah, we got more interesting\nproblems to solve than figuring out the meaning behind Strawberry. But I don't know, Nathalie,\nwhat are your thoughts? Yeah, uh, the first thing that I thought\nI was very, very curious about Project Q, which seems to be same as Project\nStrawberry, uh, but being really day to day working with these models. The thing that I first thought is\nlike, okay, now they are saying we are moving to the next level of AI\nwhen we cannot really fully measure the performance of the current chat\nbased model, a level where we are. So I meet it with a skepticism\nin that, uh, it may be. great answer certain questions\nand in certain scenarios. But when you dig deeper and try to\nchange a little bit the context, it may be possible that it's not working. And the reason is that right now we\nreally are not very good at measuring the performance of the models. There's tons of benchmarks out there. Uh, but if you throw the model\nto the wild, then you'll see stuff that is slightly different. So I meet it with a skepticism, really,\nI'm pretty sure it's going to be great. Uh, the other thing that I was thinking\nis that how do you know what is behind and the fact that it's closed\ndoors makes me wonder, what is it? Is it really intelligence or are\nthere like rules on top of a model? And, and maybe it is really, really\ntailored to this solution and the benchmarks that they are trying to beat. So we'll, we'll see. But that's, uh, my, my take on that. That's right. And it's a very interesting outcome,\nwhich is like, you know, OpenAI drops like the new big model. Um, but like because our evals are\nkind of so crude for evaluating model capability, it's actually kind of\nunclear how much of an improvement it is. Like I think that's actually\nalso really kind of potentially funny and interesting outcome. Yeah.\nI push back a bit on that, Tim. Okay. You think it'll be obvious? Like when they take\naction, it's going to be. Yeah.\nAnd it's very transparent. Uh, like we do this every\nday with our clients, right? So we'll go in and say, Hey. Everybody has some sort of a\nknowledge search use case and rack patterns and so forth, right? So we have our own, our entire benchmarks. We create golden records, truth,\ngrounding of truth and stuff. And we compare against those. We'll do a human evaluation. We will do an LLM as an,\nas a judge, whatnot, right? So we'll do this whole entire rubrics. for clients. We see a meaningful difference when\nyou're applying an OpenAI GPT 4. 0 model versus a smaller model. We do see a better response. It's crisper. We do see quality improvements over the\nlast, uh, 18 months to two years, right? So like I'm generally I'm very\nimpressed with how well the models work, as long as you do the before\nand after ridiculously well, right? If you form the question in the right\nway, and you're asking it, and you're getting the data, the answers are\ngetting better with these model upgrades. I still don't think that the\nsmallest model can come close to what the OpenAI models are doing. There are some bespoke use cases\nlike Cobalt to Java, right? Of course, IBM's model has to outperform\na general model because we have all of this first party data, we have a\nridiculously good set of talent around it, research, IBM tech can create that\nmodel and fine tune it really well. So those use cases, obviously\nit's not even a competition. But if you're looking at knowledge article\nuse cases, can I understand the nuances of what happened on this IT ticket? The ticket itself is 15\npeople have touched it. And each one had different updates. What's the root cause of what happened? The bigger, nicer models have\nbetter reasoning capabilities, do an exceptionally good job at picking\nout the needle in the haystack, which smaller models cannot, can't get to. But Shobhit, do you think we're at the\npoint where like, I can translate a 0.01 increase in MMLU or like\nthe degrees of which, you know, we're starting to see these model\nincremental changes are so small. into like, this will improve my\naccuracy and then reduce my cost by x. So I do see, uh, different\nweight classes, right? If you're just still in the\nOlympics frame of mind right now, different weight classes. If you're in the, in the, in the\ntop league of frontier models, you will not see that much of a\ndifference because there are other techniques that you're using that\nhave a higher impact on it, whereas just swapping out the model itself. But the same use cases, if I go\nfrom Gemini to OpenAI to Claude, I do see meaningful changes in the way\nthey're interpreting the data and how they're responding to it, right? But then once you pick a model, then\nthe way you're asking the question, the way you've created embeddings\nand things of that nature, you have to tie it a little bit to the model. You can't just swap out that,\nthat model for the new one and expect it to behave better. So it's, it's just not a\nvery plug and play right now. But if you find a model. You adapt the rest of the\nbefore and after to it. You see a fairly decent quality bump,\nbut again, different weight classes will give you different results. Yeah, yeah. So I think, uh, hearing show it, one\nof the things I thought is totally agree with you in that large language\nmodels have improved substantially the performance of smaller models. Uh, the comment was really towards more\nhow do we measure those big models, those large language models, and I think, uh, we\nstill some to have some more research to measure a nicely what's their performance. And I agree with Kate, uh, definitely\nhigher MMLU does not guarantee that the model is going to perform,\nuh, great in certain use cases. So yeah, lots of interesting\nchallenges to, to address there. We are unfortunately at time. Um, so Nathalie, uh, Kate, Shobhit,\nthank you for joining us as always. Um, and for all you listeners, if you\nenjoyed what you heard, you can get us on Apple Podcasts, Spotify, and\nbetter podcast platforms everywhere. Uh, we'll see you next week."
}