{
  "video_url": "https://www.youtube.com/watch?v=y4gm_4UFT28",
  "video_id": "y4gm_4UFT28",
  "title": "Deep Research, OpenAI inference chip, small VLMs, and AI agent job posting",
  "upload_date": "20250221",
  "channel": "IBM Technology",
  "duration": "45:51",
  "caption": "What was the last thing that\nyou had to do deep research on? Kate Soule is Director of Technical\nProduct Management for Granite. Uh, Kate, welcome back to the show. What have you been researching? I've been researching KV cache management. Volkmar Uhlig is Vice President,\nAI Infrastructure Portfolio Lead. Volkmar, welcome back to the show. What have you been looking into? Indices and vector databases. And last but not least is Shobhit\nVarshney, Senior Partner Consulting on AI for US, Canada, and Latin America. Uh, Shobhit, welcome to the show. What have you been looking into? Quantum computing, especially\nhow it intersects with AI. All right, all that and more\non today's Mixture of Experts. I'm Tim Hwang, and welcome\nto Mixture of Experts. Each week, MoE distills the\nbiggest stories in the world of artificial intelligence and\ngets you what you need to know. As always, we have a lot to cover. We're going to talk a little bit about\nrumors on OpenAI's inference chip. We're going to talk about\nsmall vision models. We're going to talk about a job listing\nfor an AI agent, but first, I really want to talk about Deep Research. Um, and it's kind of a funny\nphrase to use because, uh, it seems like nowadays, everybody has\na feature called \"deep research.\" Um, Google Gemini has a\ndeep research function. ChatGPT announced the\ndeep research feature. Even Perplexity announced a deep\nresearch feature and not to be left out, Grok has also launched a feature\nas of late called DeepSearch. And these are all kind of features\nwhere you can do a query to a model and get back what is effectively a\nvery long research report that's kind of in depth um, and you know, this all\nreally literally happened I think in the last month two months or so. Um, I\nguess, Kate, maybe I'll start with you. Why is everybody suddenly\nlaunching a deep research feature? Um, what are they trying to do and why\nis it suddenly all very competitive? Why is, why is deep\nresearch the new hot thing? Yeah. So I think it's helpful to understand\nsome of the broader context and when all of these features were released. So, you know, back in January, DeepSeek\ncame out with their R1 model demonstrating crazy reasoning capabilities, uh, OpenAI,\nyou know, maybe in a bit of a response to a way to show that they're, you know,\nalso innovating on reasoning and doing a lot of work in the space pretty much\nlaunched as a fast follow from what I've been able to tell their deep research\ncapability, which leverages the o3 reasoning model behind the scenes. And I think ever since that model\ncame out, we've been seeing a lot of, uh, following the market and a lot\nof other companies come out and yes, and and create their version to just\nto try and follow that broader trend and focus on reasoning models that\nhave really taken the world by storm. Yeah, for sure. And Shobhit maybe I'll bring you in here. You know, one of the questions I have is\nlike, how do you win in this competition? You know, it's like, it's like we're\nsuddenly living in a world where there's like four or five search engines. And it's like, you know, it's\nthe early two thousands again. And it's kind of really question\nabout like, if you see see any kind of differentiation between all the\ncompanies and how they're trying to win on this particular feature. Um, so I think Google came up with this\nfirst in December, followed by OpenAI and then followed by, uh, Perplexity\nand now, uh, with, uh, with Grok-3. Uh, the overall intention is, given a\ncomplex ask, I want you to go research this across a whole multitude of\ndifferent websites and then try to cluster them in different topics and\nthen go, when you find something, you go find other things that are\nrelevant, just the way humans do this. So you're trying to replicate the way\na human would have otherwise open up 20 different browsers and try to simulate all\nof that into a topic and research, right? Now when companies like Google are\ndoing this, they have a really good understanding of how web pages are\nstructured and how things semantically connected and so on and so forth. So most of these deep research models\nwill start first by creating a plan. They understand your query and\nthey'll have a plan generated. In case of Google, I can go and hit edit\nand I can go change the plan if I need to. In case of ChatGPT, it'll go\nask some follow up questions. So you can go modify that and understand\nthat here's how we're going to execute. There are certain queries that need\na little bit of disambiguation. Do you mean X versus Y? But I'd look for \"Transformer\"\nis that the movie or is that the model, things of that nature. In certain cases, you may need to narrow\nthe field a little bit and go very, very deep in a particular topic, right? So you're first establishing here's the goals and\nhere's the research plan. That's what a good research\nanalyst would have done for you. Then it fires off and starts to go\ncrawl the web and starts to find all the websites that are relevant, then extracts\neverything out of it and say, Hey, I found that this particular website\nwas talking about a new, for example, we were looking at quantum last night. Microsoft released some new, a new\nmodel with some new matter altogether. So now all of a sudden you have a\nnew topic that's coming up that I did not specify in my initial search. So that'll then spawn\nadditional queries and so forth. So it's going and crawling all of\nthat, bringing all the content back. So your question was around\nhow do you win in this? On the more B2C side, me as an end\ncustomer, the person who can connect the dots between the websites and the\ncontent the best is likely going to win. Speed matters to an extent, whether it\ntakes three minutes or four minutes. At the end of the day, I'm going to\nhave to come back to this tab anyway. So I think I'm okay on the\nspeed and the latency part. But understanding, grounding it, and\nbeing able to give the right citations. At least in my personal\nexperience using Perplexity, I've been using Pro for a while. Perplexity Pro Deep Research was a little\nmore hallucinating than I'm getting with Gemini and so far OpenAI has been\nthe best for at least my experience on the topics that I researched. But there'll be some nuances of\nhow you ground your content, you get all the responses back and\nthen do visual interpretation of all those different websites. In the enterprise space...\nThis is untapped right now. In an enterprise, when somebody gets a\nquestion saying: Why was my claim denied? Or if I want to say: Can I travel to X? And what will be my ATM charge? And what kind of benefits I have? There's a very unique set of\ndocuments that need to be looked at where a human researcher goes\nand looks at multiple systems. It's a combination of actually logging\ninto a third party SaaS, figuring out information there, reading\nsome documents, so on and so forth. We have not quite crossed the\nchasm yet on deep research coming to the enterprise space. I don't see a single vendor out\nthere that's enabling us to go add enterprise data to it, be able\nto model how the reasoning steps work based on the topic at hand. I think 2025 towards the end\nof end, we'll start to see these models getting more open. There should be other layers on top\nthat bring it into the enterprise space. I think that the company that's\ngoing to make billions of dollars. Versus somebody who's\ndoing a B2C, uh, view. Yeah, I mean, I think that's kind\nof the interesting thing, is like, I feel like the use cases that I've\nseen online so far have been people who have pretty niche needs, right? Like, I think it's like, you know,\nresearchers or kind of bloggers that need, you know, studies. Um, Volkmar, maybe I'll bring you in. I feel like over the last few episodes,\nyou've increasingly become like the loud skeptic on the MoE expert panel. Um, are you kind of impressed\nby stuff like Deep Research? Do you use it at all? Like, it does feel to me that\nthere's a bunch of problems. And I guess the question is like,\nalso whether or not it's like technically that impressive either. It's like kind of a just\ncombination of existing components. Is that the right way\nof thinking about it? I think it's a, it's a\nreally interesting approach. I think it's incremental. Um, you know, we already\nhad the ability to search. Now what we are doing is, you\nknow, we are just extending the search, you know, scope. Uh, I think we already had\nthe first iteration of, you know, go out, make a plan. Um, now I think what, and, you\nknow, like, uh, do longer reasoning. You stayed out of the model. I think what, what's, changing now\nis that we are saying, you know, we go multi step reasoning and\nmulti step document retrieval and extending, you know, the knowledge. I think the larger, uh, context\nwindow sizes allow it to do that. So that's what is one of the things\nwhich, you know, if you just have a 4k context window, you cannot do that. If you now have, you know, 128k, you can\nthrow lots and lots of documents at it and you can start reasoning about it. So I think we are at this point. junction that, um, the\naided data is available. So, I mean, this is also\nthe other thing, right? So, OpenAI started having, you\nknow, a call of the Internet accessible in a vector database. So, you needed search capabilities, you\nneeded the long context window sizes, and you need to have the multi step reasoning. And so, all these things now are at a\npoint where they are individually stable. And now we are getting into, okay,\nwhat can I build out of this? So, I think it's a really interesting uh,\napplication and it, I think, shows the direction where we are heading, right? It's like multi minute, you\nknow, processing with answers. Um, I think it also shows that we are at\na point that the models, we are willing to not just babysit the models every,\nyou know, a hundred characters, uh, but we are letting it run for a while. And, and, you know, the. The quality of the models is\nhigh enough that they don't just go off into a tangent. Totally. Yeah, it does feel like that's kind of\nlike almost like the biggest thing is like less a technical thing and more just like\na sociological thing is that like we just now have enough trust in these systems\nthat we're willing to let them run like this, which is like pretty interesting. So Tim, on one of the challenges\nyou see in deep research is you don't have a verifiable output\nto compare accuracy against. And we struggle with this\neven in organizations. So when you come back with a deep\ndocument on, say I'm looking at what are the, uh, milk regulations in Europe\nversus India versus the U. S., right? I don't know what good looks like, so it's\ndifficult for you to verify the output. And a lot of these companies are\nstruggling with the evaluations around these deep research, uh, files, right? There's some, uh, things that\nI can calculate, like how many, how many paths did you create? How long do you think? How many websites did you hit? And so forth. But there's not a good measure even in\nour real world, if I hire two different research companies to go research a\nparticular topic, they will come back with different documents, but I won't have\na good validation routine around that. So I think it's an order of magnitude\ntougher problem than say you are trying to write code or do some math\nwhere I can deterministically tell you whether the answer is correct or not. Yeah, no, I think it's like very evals\nquestion on this is like very hard. It's like how do you do good\nbenchmarks on this kind of feature becomes like very quick, tricky. Yeah, maybe a final question. Um, Kate, maybe I'll turn to you is,\num, you know, the four companies we have here, we have Google and OpenAI, right? Giants, you know, titans of the space. We have Perplexity, who has really\nspent a lot of time working on search. So it's no surprise\nthat they would do this. What's kind of interesting here is,\nis Grok, which really has only hit the scene fairly recently, um, and is as\nyet kind of launching these features that are very much kind of at parity. And I don't know how you read that. I mean, you know, there's one point\nof view, which is, well, the space is more competitive than ever. Anyone can just kind of get in and\nlaunch these cutting edge features. I think there's also a view, which\nis, well, you know, Grok is just executing in an incredible way,\nbut curious how you read that. It's like, is it easier and easier\nto launch some of these state of the art features, um, in, in\nwith teams that are way smaller? I, it's one of the questions I have. We're benefiting from having so\nmuch of the innovation starting to be put into the open source. which is allowing, you know, a\nrising tide to, to float all boats. It's allowing less traditional\nplayers to enter the market. Uh, and we're seeing just, you know, a\nreally rich ecosystem emerge from it. So it's exciting to see what, uh, you\nknow, Grok and others can come out with. And as we talk about, you know, Deep\nSearch and how that relates and Deep Research, you know, again, I really think\nright now, deep research is one of the more practical use cases for reasoning. Uh, if we're all innovating on\nreasoning and we're seeing a lot of that work in the open source, a\nlot of the benchmarks are on math. Like, I don't know that that is the\nkiller use case that, you know, is why I pay for a bunch of reasoning tokens. But research is certainly an area\nwhere we're seeing some benefit. And, you know, I think\nthis is just one of those. early use cases that we've\nidentified where there's some clear demonstrable value\nthat the reasoning is bringing. And so that's why we're seeing\nas new models come out, they're also coming out in parallel with a\ndeep research type of capability. Well, I want to move us\non to our next topic. Um, It's a story that I feel like we do\nevery few episodes to be totally honest with you, which is every few weeks or\nmonths there's always rumors that OpenAI is working on its own chip. And the story\nthis time was kind of a leak that you know OpenAI was kind of readying sort of an\ninference design with TSMC, which is kind of one of the lead, um, kind of chip fabs. And, uh, I think I wanted to kind\nof use it as a hook to talk a little bit and check in, I think, on\nsort of the state of like OpenAI's competition in the hardware space. And, you know, Volkmar, I guess\nyou're the natural person to turn to for this sort of thing. You know, it's sort of interesting\nto me that at least what has been reported in the news is that OpenAI\nis investing first in inference chips. And I guess for our listeners, do you\nwant to explain just like why this would be such a big priority for them? Because this is a very\nbig bet they want to make. Um, and I guess the question is\nlike what you think they, what you believe their upside to be, uh, in\ninvesting in this sort of thing versus using, you know, the established,\nuh, companies that are out there. OpenAI is building the chip\nnot by themselves, right? They are partnering with Broadcom\nand Broadcom is one of the giants in chip manufacturing. So that's a, that's expected, I\nmean, they had to pick a partner if they don't want to become a chip company. And I don't feel that OpenAI, you know,\nwants to, wants to get into that market as a, as a primary business model. Now, if you look at, uh,\ntraining versus inferencing, the requirements are very different. Um, so in training, you know, if\nyou, if you build a training cluster, it's a lot about, I mean, you have\nthe basic GPUs, but then a good chunk of the money goes into the\nnetworking infrastructure and goes into storage system and having, you know,\neffectively a high performance computing system. So it's like very, like if you\nlook at all the HPC people went, they all went into, into AI now and\nbuilding these training cost us. So that, that's a, a critical\ncategory of, you know, system design. Then you go into inferencing and that\nis usually a much smaller problem. Now we have very large models and\nthey don't fit on a single GPU. Uh, but often, you know,\nyou're on like maybe eight. Kind of on the maximum. If you have a really, really large\nmodel, which is not necessarily what you're, you know, using to do inferencing\nto an end customer, but maybe, you know, for model verification for\nyourself, then you may go to 16 GPUs. So let's say two boxes, but\nyou're not going much beyond that. And so, um, if you now look from a\nconsumption perspective, you know, training this, we talked a lot about\nthis huge training machines, but in the end, when you're scaling it to consumers,\nlike the ratios of consumer hardware or consumer consumption capacity you need\nto put down is orders of, or at least an order of magnitude larger, right? And at the very beginning, everything,\nall the investment went into, uh, into training because, you\nknow, we need to make the model. Now we have the model,\nnow we want to use it. And now the growth\nactually inferencing side. So it's a natural conclusion. Um, from an OpenAI perspective\nto control your destiny. Now, the easiest one is, you\nknow, you read the, uh, the profit statements of NVIDIA and\nthere are around 69, 68, 69%. And so if you want to get eight,\nyeah, they're doing well and it shows in the stock price. So if you want to take a\nlarger chunk of that revenue. Um, and of the profits, then,\nyou know, you partner with a chip manufacturer, you get an exclusive deal. I'm sure, you know, like, NVIDIA\nand OpenAI, they have very specific deals where OpenAI probably pays\nless than the rest of the world. But still, it's, if you can control\nyour supply chain for the product you're building, further, one step further down. You know, or two steps for that. And the first step is like,\nokay, I own my data center. The second step is I, I control the chip. Uh, now you can actually do chip\nmanufacturing and you can design a chip for your protocol model. So you can co design a model for the\nchip and that's where you can probably get another three, four acts in, in\ncost reduction and I think OpenAI now at that scale they're operating at are\ndoing the natural thing of any company, but just, you know, control your cost. Sure, but you want to talk a little\nbit about how this might impact kind of like the market for AI services,\nbecause it strikes me that in the past, you know, the way we've sold AI\nis that we go to customers and we say, look at this brand new shiny model. Look at all the things that\nit can do, um, work with us. Um, and presumably part of the pitch\nthat OpenAI has here in the future will be well, it's also running on our chips. And as a result, things are way fast,\nlike faster, or like way more performant. And kind of curious if you think\nthat's going to shape sort of the sales pitch in this space, like sort\nof moving from selling the underlying infrastructure to like being the kind\nof primary focus versus the model per se, which I think we're seeing is just\nbecoming more and more open source. Yeah, absolutely. And, uh, TSMC for the people who don't\nknow is the hundred pound gorilla. Like they have like 65 percent plus of 100,000 pound gorillas, you know, and the world comes to a grinding halt\nif something happens to TSMC and we're talking about chips, my, my Tesla\ndoor handle has two sensors, right? Imagine thousands of sensors\nacross the entire car. All of those are being, uh,\ncoming in from TSMC outside of, uh, like, outside of Samsung. I don't think there's any other\nbrand that, that comes in more than 10% of the marketplace. So TSMC is super critical. Everybody's designing the chips,\nbut TSMC is the heart of the entire industry at this point. Now, uh, if you look at, uh, Amazon,\nthat's a good, uh, a good analogy. Amazon has its own inference chips. They have built their own NOAA models that\nare super optimized for their own chips. So that combination, Anthropic\nis going to be using a lot of the Amazon chips as well. So when you optimize the architecture of\nthe hardware to work with the architecture of the software itself, that does magic. The total cost and the throughput\nthat you can get, the latency decreases, the cost of delivering\nthat comes down significantly. So in the enterprise world, Uh,\nwhen we, when you go do these large projects with clients, you're\nlooking at high volume use cases. For example, if I take a Llama\nmodel, the Llama 3 model could be running on, on Azure, AWS. It could be running on NVIDIA. It could be running on, on Watson. When you look at, uh, the inference stack,\nif you take that Llama model and you make a NIMS out of it and you put it on NVIDIA\ndirectly, that is particular now for that particular model, they have NIMIFIED it. It's going to run at 5x more throughput. You may pay 5 percent more extra cost,\nbut now you have 5x more throughput. So that, uh, that brings up use\ncases where you're doing some inferences at massive scale. So as an example, if you're doing,\nsay, fraud detection and you're looking at invoice coming in or you have\nan image that you're looking at and you want to do that today at scale. We've been using classical computing\ntechniques for doing those today. Because the volume is very high. You need a very quick latency. You need to have do this at\nmillions of times every day. So the cost would add\nup really very quickly. So this whole shift towards Amazon\ninference stack with their own models and Nvidia with NIMS on top. This is the trend that\nOpenAI is following as well. So that high volume use cases, the\ncost of doing this would go down. and be very effectively\nrunning in production at scale. So from an enterprise perspective,\nthe use cases don't change. But now we start to go after\nthe high volume ones where earlier the ROI didn't exist. So I'm generally very excited about\npeople looking at inference and optimizing it so I can take more AI to my clients\nand and infuse that into more processes at scale and deliver higher ROI. Yeah, for sure. Kate, do you think, um, one thing that\nkind of occurs to me is... Does, do these structural changes create any,\nlike, dangers, I think, for open source? So kind of what I'm thinking a little\nbit about is in a world where you've got models, but they run way better\nif you're using a particular kind of hardware, or they, they can only run\non a particular kind of hardware. It actually changes kind of the dynamics\nof open source, where I think the dream of open source is you can take\nyour model, you can run it everywhere. And that builds the largest possible\nopen source community around our models. Um, do you worry at all about kind of\nthis hardware fragmentation as like things get more sort of specialized\nand kind of like really optimized for particular families of models? I'm not sure that I worry so much. I mean, the model around open source\nhas always been there are open source versions of technology and then there\nare optimized enterprise supported versions that, you know, end up\nbeing what gets deployed, right? And so we always need\nto have a bit of that. Balance, um, as a whole. So, you know, it's not something that\nimmediately keeps me up at night, but I do think there's another really interesting\nthing that's going on here that, you know, 80 percent of the reason why, you know,\nanybody is doing this is probably exactly why, you know, Volkmar mentioned in terms\nof controlling costs, but I think there's uh, interesting part that also\nreflects that how these models are trained are is changing. And we're seeing a much larger emphasis\non techniques like reinforcement learning, which require a huge amount\nof inference of really big models. And so being able to control your\ninference costs no longer is just being able to serve your models\nat a lower cost point to customers and run larger and longer jobs. It also is now a critical part of training\nso much so that you could easily start to see Reinforcement learning costs starting\nto outweigh the cost of pre training. Yeah, that's wild to consider. I haven't thought about that. And I think, uh, Tim, this is in line with\nwhat Google has been doing forever, right? Their tensor processing units, the\nTPUs are just so well designed. They are, they do an amazing job at doing\nthis distributed across multiple centers. They don't need to build one big cluster. They're able to do this in a distributed\ncenter and connect them with very high fiber optic cables to do. Inferencing at scale. They have multiple products that are\nbillion users plus every day, right? They've been deploying these AI\nmodels, deep learning models, transformer based models at scale\nat an insane pace across the world. So you'll see more and more of this. Inference time, optimized\nmodels, they're delivering great ROI and the right cost point. I'm very excited about this space. Yeah, for sure. Yeah, I was also going to comment,\nI feel like MoE is one of the few podcasts you can go on where a panelist\nliterally does the chef kiss for GPUs. I'm going to move us on to our next topic. Um, you know, there's a joke that I\nalways used to make when I was kind of, um, working at Google where we'd\npresent, you know, oh, this is AI. And when we say AI, there's lots of\ndifferent techniques, but really what we're talking about is deep learning. Right. And this was, you know,\na decade plus ago now. Um, and I kind of feel like we've actually\ndone a very similar thing now where we say, oh, well, when we say AI, we mean large language models. Um, but it actually just turns\nout there's like lots and lots of things happening in AI. And I think one of the most\ninteresting things that's been popping off lately is kind of\ncompetition over vision models, right? Which I think, you know, have gotten\nshort shrift, even though there's lots of exciting things happening there,\nbut just because the LLMs have kind of taken up so much of the space. Um, and Kate, ideal for you to be on\nthe show for this episode, because I understand Granite is out with a\nnumber of new small vision models. And so first, do you want to kind of walk\nus through that and what's been launched? And then I kind of more\ngenerally want to talk about like how this space is evolving. Absolutely. So a couple of things, first,\nuh, VLM, a vision language model, it's a little bit different than\nwhat folks might be familiar with. If they played with some of the\nearlier like stable diffusion models and a lot of the image generation\nmodels that we've seen to date. A VLM, which are these smaller models\nthat are starting to get more popular, is all about image understanding. So it's a image and a prompt\nthat gets sent as an input. Text is normally then returned as an\noutput versus, you know, the, some of the original really popular DAL-E and\nother models that, um, you start with a prompt and you end up with an image. And the way these models work are you\ntake a standard large language model often that's already trained to do\nlanguage tasks and you do some additional training, uh, and to add a component on\ntop that allows for an image to be, you know, basically expressed as an embedding\nthat gets fed to your language model in addition to the embedding from the prompt\nand that information is together used from that language model to\nreturn the, the response. Um, so these are really,\nuh, becoming popular. We just saw a bunch of models drop. Uh, Granite released two\nweeks ago, our vision preview. Uh, and the full model\nis coming, uh, next week. So keep an eye out from the\nIBM Granite Hugging Face page. Uh, our model is only\n2 billion parameters. It's really small. You can run it locally. And what we're really excited about is\nwe've taken a very specific approach focusing on document understanding tasks. So think of images from the perspective of a\nchart in a PDF or a poorly scanned PDF document, um, or a GUI or a dashboard\nwhere you like take a screenshot and put that into the chat box and\nstart asking questions about it. So, you know, uh, Granite can\ndo all sorts of general vision understanding tasks, but we've really optimised the performance around this\ndocument understanding, uh, thinking through from our, you know, our enterprise\ncustomer perspective, that's going to be where there's a lot of really valuable use\ncases, particularly as we look at some of our other projects that are going on in\nthis space, like dockling, uh, and more broadly looking at use cases around areas\nlike multimodal RAG. So, yeah, so Granite, uh, preview released two weeks ago, the\nfull version is coming out next week. We just saw Qwen, uh, released, I think,\nearlier today or early last night. Their, uh, family of VLMs ranging\nfrom 3 billion to, I think, 72 billion parameters in size. And there's just a lot of other,\nuh, work going on in the space. You know, Pixtral, for example,\nis a common one that's been out for a little while. And we expect to see this\ntype of capability only grow. Shobhit, do you want to give a little\nbit of a picture of kind of how the competition around this is evolving? I think, again, it's kind of like,\nyou know, almost like a little bit like deep research, right? Which is, well, we've got this kind\nof interesting use case, and now people are trying to figure out where\nin the market it really belongs. For VLMs, it also seems similar,\nright, which is like suddenly you have this class of small vision models. What are enterprise people\nwanting to use it for? Absolutely. We've been working on vision models\nwith clients for a while now. Earlier, this was a lot of\nthe heavy lifting used to be done on a server on the cloud. So for example, if I take a Gemini  1.5 Pro model, it just chews through\na whole video and can understand exactly what happened and has\na really good understanding of what's, uh, what's going on. Those are very big, large models. There are a lot of use cases that\nwe've been delivering for clients. As an example, large consumer goods\ncompany, distribution company, you have things around planograms where you walk\ninto a store and you want to make sure the shelf has everything the right way. There are consumer goods companies\nwhere the label behind a particular product has to be, uh, relatively\ncompliant to each region, right? If it's food versus, uh,\nsome dresses and stuff. Then there are certain use cases around,\nuh, describing what's in the catalog. So for example, a large electronics\nmanufacturer or a clothing apparel company or retailer, they would take\nimages of what people are trying to sell. So when you upload a product, you\nwant to describe that product. If you look at a big furniture store, when\nyou take a piece of furniture, you need to create a lot of metadata so that it\nshows up in the research and stuff, right? Usually all of those tasks\nwere very human driven. Now we're at a point where the\nimages, as Kate was saying, they have evolved quite a bit, the VLMs, and\nthey have a better understanding. Earlier they were able to just identify\nwhat's in this particular image. I could do some, some correlations\nand say this looks like a cat, this looks like a dog. Now it has evolved quite a bit. So for example, one of my clients, we\nhave a camera that points at all the counters and you can see and tell which\ncounter is more busy because it's also doing a people counting on the fly, right? So it understands which product\nis getting more popular. It has a better understanding of temporal. If I give you a few screenshots\nor video, it understands what's happening in the video, right? So from frame 2 to frame\n19, what changed delta? So it's trying to\nunderstand that even better. So OCR was the first wave of use cases\nthat we found now we're getting into more and more semantic understanding\nof what's happening in the overall picture that starts unlocking even\nmore use cases and to Kate's point, the models are getting much, much smaller now. That allows us to do two things. One, I can now run these on a device while the person is running around. So the person in the field in a\nmanufacturing facility can take a picture of something can have\na small camera that's running and things are running on device. This was supremely important for security. A lot of these use cases, you don't want the images to being\nstreamed out for security reasons. You want to run these things near on prem. We're looking at defence use cases,\ndrones running around in territories where you don't have control over the\ncellular network and stuff like that. All of those required us to\ndo smaller models on the edge. The second category of things it's\nunlocking for us is high volume use cases. So, for example, the document processing\nthat Kate mentioned, those are being done millions of times every day. The incremental cost difference\nbetween a 7 billion parameter model and a 15 or or 30 billion, billion\nparameter makes a difference to the end ROI of that use case. So we are now coming to a point where\nthese small models deployed, either at scale or on device, are delivering the\nROI that's so critical for us. Yeah, that's great. Volkmar, I'm gonna give you an\nimpossible question for this segment, uh, to close out this segment. You know, to what Shobhit just\nsaid, right, there's these very interesting kind of pressures. And I don't think there is a clear\nanswer just yet as to like how much of AI workloads will happen at the edge versus\nlike, you know, in big data centers. But it does feel like kind of like the\nprominence of smaller models and the fact that they're actually like perfectly\nperformant for most industrial tasks, means that we have a world where this is\ngoing to be more and more on the edge. But I don't know if you think like the\ntrend is really going to be sort of 50/50 when this all kind of settles\nout, it's going to be all mostly on the edge, or just kind of curious\nabout how you size up like where you think the models will live ultimately. So I think it will be a bit of everything\nfrom a bandwidth perspective, right? It's very cheap to\ntransmit a couple of words. It's a very, very low bandwidth channel. The moment you go into vision,\nthat's a high bandwidth channel. So you have a bandwidth issue. And so this is a traditionally, if\nyou look at computation, it always has been the straight off between,\nyou know, do I bring the computation to where the data is, or do I bring\nthe data where the computation is? And so I think with, with text. Um, maybe ignoring latency, it was kind\nof in favor of bringing it to a data center so it can do the consolidation. It's like the argument of should I\nhave a nuclear power plant or should I have a generator in my backyard, right? And so I think we have the same, and,\nand, you know, nuclear power plant in the end because it gets consolidation\nis more efficient, but now I need to have a power distribution network. And so I think we are in a similar\nsituation here where, um, if I have a high bandwidth stream, uh, and I can\nactually solve this with a relatively small model at the edge that, you\nknow, the economics work in that favor. And if you look at the trend, um,\nuh, like, you know, we, we are, we are now making decisions on, on based\non the complexity of the question. Now for videos, that's really hard. Uh, but if you look at what, what\nthe iPhone did and, you know, and we'll see this probably in all\nthe phone manufacturers, like. You know, you have a model router\nat the beginning and the model router decides, this is an easy\nquestion or a complex question. If it's an easy question,\nI stay on device. If it's complex, I\noffload it into the cloud. And so I think from an, um, now flipping\nthis, the cheapest input device, um, is effectively a camera, right? If you think about it, you capture, you\nknow, 30 frames a second, uh, and you have millions of data points, right? And so now the, the, the entropy of\nmillions of data points over time is very low, but you can capture, you\nknow, a lot of information in one shot. Um, and, um, so the second one is audio. Would you want to transmit audio? That's probably more feasible. I think video is pushing it\nreally to, to a point where you, you know, you will just\nprocess on the edge. And then I think what will happen\nis that the models will specialize. So, you know, what Shobit said, you know,\nyou have these industrial use cases. Um, and if you're on a manufacturing\nplant, you may want to keep it as a record, but there's no reason for you to\nmove it, you know, into a different data center on the cloud because you already\nhave industrial scale installations, you already have data centers, et cetera. And so it makes much more sense\nto just do it, you know, locally. Now locally does not necessarily mean\ninside of the camera that may be required, you know, if you have a battery or so,\nbut it could be inside of a building and, you know, may run a cable, which\nis a couple of hundred feet long. Yeah, that's right. Yeah, it's a good reminder that\nlike where the edges is totally dependent on where you are. So... We kind of have this natural, you\nknow, tendency because everybody carries an iPhone around. That it's the phone, right? And so, you know, it needs to be a package\nof a battery, a camera, and a processor, but that's not necessarily true. Um, maybe just a final question. I guess, Kate, if folks want to learn more\nabout Granite's work, um, you know, where they should they go to get this new model. I know you said that there's a\nbig announcement and there'll be a release next week, but, uh, anywhere. You know, online people should be paying\nattention to or anything like that. Yeah, I mean, we always post everything\non our Hugging Face page, uh, under the IBM Granite org and then, uh,\nencourage folks to check out ibm.com/granite and you'll\nbe able to find all the latest there. Um, I'm going to move us on to our\nlast story of the day, really kind of more of a publicity stunt, if I can say\nthat up front more than anything else. But I think it is kind of an\ninteresting set of questions. The story is that Firecrawl, a Y\nCombinator startup, so very, very early company, got a little bit of\nattention on social media because they put out a job description. Um, and, uh, the job description\nwas basically looking for someone who could assist in their open\nweb, web crawler business. Um, but you know, the, the kind\nof listing specifically said, you know, humans need not apply. This is only for AI agents. Um, and you know, on interview, the\nfounders of the company admitted, right? Like this is just kind of conceptual. It's kind of a funny experiment. You know, this is sort of a publicity\nstunt more than anything else, but it did get me sort of thinking about. you know, how far agents are going to\ngo, particularly in the next year or so. Um, and whether or not for certain\ntypes of tasks, we really will start seeing kind of call for agents, right? To basically say, well, I could hire\na human to do this job, or I've got an open call if anyone wants to produce\nan agent that will do the same job. Um, and so maybe Kate, I'll\nthrow it to you first is like, are we living in that world? Are agents getting good enough,\nfast enough that, you know, we're going to start to see in 2025, 2026, some jobs really have\nlistings for agents specifically. Well, look, I think what\nthey did was clearly a bit of marketing tongue in cheek. Uh, but I think it's very realistic\nthat to have a near future where we have catalogs of agents and people\ncan also create, you know, specs for agents that types of behaviors\nthat they want so others can build it and sell those agents, right? Um, I think that's very\nmuch where we're headed. I don't know that, uh, it's going to be\na total job replacement, so to speak. I see a lot of opportunity for agents augmenting human roles and jobs. And I think that's much more realistic\nof, as we look at like, what will a job description look like next year? Having expertise and familiarity and,\nyou know, part of the job description is helping manage agents and work\nwith AI systems is I think, going to be increasingly a huge part of\nthe, the new workforce, so to speak. Yeah, I think that'll be sort\nof an interesting bit of it. It reminds me a little bit of back in the\nday where it was like, you know, skills, you know, Microsoft Office Suite, Excel,\nWord, you know, whether or not kind of agents will be or experience with agents\nwill be kind of like a relevant skill. Yeah, so I think this is not new. So if you look at what Dharmesh did\nfrom HubSpot CEO, he launched agent.ai last year. It's the largest network where you\ncan create just like you would have gone to Fiverr to go hire people. You can go to agent.ai and find a catalog people are\nrating and you can hire that particular agent for a particular task and you\npay by by different metrics, right? So I don't think this is new, having\naccess to a variety of different agents who specialize in a particular domain. The way enterprises look at multi agent\nworkflows, we spent the last 5, 10 years looking at structured, directional flows. We would go into an organization and\nsay, let me find the workflows that are yucky and I'll reverse engineer them. We'll all create a new way of doing it\nand we'll codify it in a fixed flow. This was the RPA era. We got to only 10, 15 percent\nof the, of the flows that were deemed worthy of automation. The challenge there was when a human\nstarts to trigger a flow, you go five, six steps in and you realize that, oh,\nthere was a, there was something that went wrong and now you have to take\nover and now you start from scratch. So the human expert would just rather\ngo into the whole step 10 steps by themselves so they have more control\nof what's going on so we could never go beyond 10-15 percent of the workflows\nthat were automated using RPA robotic process automation. In this agent world,\nwe have an opportunity for not having to define every deterministic step. So within very thin\nguidelines and guardrails. LLM can choose to figure out which LL-\nwhich API to call or which tool to, to choose, and how to pass in the parameters\nand create some sort of a plan, iterate through it, uh, and then make sure that we\nare heading towards the right direction. So very narrow task. Those will get automated with\nLLM agents fairly rapidly. You will see this from the native\ncompanies like Salesforce will have its own agent force, small pieces\nthat are automated, but then you'll have external third party tools like\nAzure, uh, has its own copilots. Watson Orchestrate and others that'll\nsit outside and they'll orchestrate work across a gamut of these different agents. The technology is solved,\nis maturing pretty rapidly. The thing that is missing in\nthe enterprises is ask to task. I should probably trademark this, but\nhumans are incredibly good at this. We go from an overall ask to a series\nof tasks in our head really well. As soon as I get a question about, hey,\nwhy is my bill higher than last month? In my head, I'll trigger\na few different tasks. Today, as a human, I'm\ndoing them in sequence. Tomorrow, as LLM agents, I can\njust trigger these off on our own. The companies who can create a\ngolden thread of ask to tasks are the ones who will win in this space. The agents themselves that are\nautomating a small step, those will get commoditized really well. Once you have this golden record of\nask to tasks, you can then create a planner agent that does that\nautomatically for you, and that unlocks the multi agent workflows then. In order to get to multi agent, you\nhave to solve for this ask to task. And the smaller LLM agents\nthemselves, they will become fairly commoditized and you'll be able\nto go to a network like agent. ai or and go find agents that are doing\nthat small task really, really well. Yeah, there's kind of a fun question\nhere about sort of like effectively, like what's the paradigm on which agents\nwill get integrated into organizations? Like I think one of the reasons why, you\nknow, a job interview or a job listing for an agent is sort of silly is that. You know, we have B2B SaaS. If like we want to use an agent,\nwe just like, you know, open up an account and click, click, click, and\nwe've integrated it into our system. I think the only kind of weird world that\nis opened up by some of the multi agent stuff, um, there's this Google paper on AI\nscientists, uh, that came out yesterday. We're essentially the paradigm was\nalmost like we're going to have an agent. That's the scientist. And then we're going to have some\nagents that run the experiment. And like, basically the, the kind of\nmodel for integrating agents was to basically create like a one to one\nanalogy with like a laboratory and then have the, have the agents put in. Um, and that's the kind of world\nwhere you might want to hire, but I guess Kate and I guess, well, why\nyou haven't spoken yet, but like, uh, we're not headed to that world. It just makes me cringe like this\nisn't preschool for agents, you know, like... There's got to be a better way Yeah, I think the the apis for that form\nof orchestration that is open, right? I mean we we went through centralized\nsoftware to you know, SAS services, um, and you can just, you know,\ninvoke an API, I think that is open. How that would work. I want to give it a different angle. We have something similar right now,\nwhich is Mechanical Turk at AWS, right? So I'm, I'm having micro\ntasks and I'm giving them out and then someone process it. So there's also an economic model of,\ndo I have compute capacity available? And I'm not selling you a\nGPU, but I'm selling you. you know the work product and I may\ngo to a centralized place picking up work items because I just have\nspare capacity or I have a model which is specialized in a particular\nway which produces better results. So right now you know this is more like\na work queue management thing at a meta layer like you know not not saying hey\nI'm, you know, produce me a bunch of tokens or llama, but, you know, solve an\nactual problem for me and post a result. And so I think this is where it could go. And the other one is APIs, you know,\nlike with the, with the baby agents, um, you could orchestrate something, but for\nexample, you may not have data access. So someone may, you know, so\nlet's say, you know, I'm going to run a chemistry experiment. I may not have all the data which is\nrequired to run the chemistry experiment. So I could imagine that, you know, I go to\na company which actually sits on the data store, which it doesn't want to share,\nbut it's happy to share the results of research or the summarisation of stuff. And then you may want to talk to an\nagent instead of talking to an API. So it's just the fact we're\nloving it one, one up. So your interface to that data set,\nmaybe, maybe the large language model. So Tim, one of the things I would like\nto highlight just from a really hands on keyboards perspective, when we're\ndeploying these large multi, multi agent networks for our clients, and we've had\ndone quite a few of these in the last six months. A large pharma company,\nwe're doing some content creation, authoring for compliance reporting,\nand there's another agent that will come audit it, so on and so forth. There's another healthcare client where\nwe are working on a customer facing member multi agent where you can understand\nall other nuances and secondary intents that get triggered and come back. There's a telco where we're creating\nsome software development, there's a BPO process where we're doing some three, four way matching quite a few of\nthese examples where we have multi agent frameworks in the last five, six months\nthat we've put in production for clients. One of the challenges we're running\nin, uh, is how do you describe the guidelines to these agents? We have as a society somehow figured\nout that English is the right way to talk to these LLM agents. Which I don't think will scale in\nenterprises when you get to agents, you're trying to go look at a complex\nworkflow and saying that, hey, if you have a question about the status\nof the ticket, go use this tool and you're giving it a few short learning. So the actual context that we give to\nthese LLM agents becomes 2, 3 pages for a small task because we have to add all\nthese bandages and if then statements that essentially you're codifying in\nEnglish and that's just one small agent. When you start to get to the planner,\nthis is completely breaks down. You cannot possibly give a\n30 page context to a LLM. The latency is very high, there\nwill be all kinds of overlapping rules, things of that nature. So I think as a community, we will need\nto make some progress, and I think IBM Research is doing quite a bit in this\nspace too, to get to a way in, like, just the way Mechanical Turk works. We have a very structured contract between\nhow you will go invoke a particular API or microservice or an agent. We'll need to get to a point where we can. We have solved this for\nsoftware engineering. We need to bring some of those principles. It will no longer, I think, be\nnational language in the way you go talk to these agents. There would have to be a little bit better\nsoftware design principles that will need to be put in place for large scale. Enterprise deployments. So the hallucinations are lower. There's better auditability,\nevaluations, and things of that nature. Well, and I think there's\ntwo key points, right? It's like, how do I, as a developer\nworking with an agent, express something in a very controllable,\nprogrammable fashion with very clear inputs and guarantees on the types\nof outputs I'm going to receive? And then, when we talk about agents\npotentially passing information back and forth and other ways to compress\ninformation and reserve it, there's no reason that has to be natural language. Or that is even efficient in any sense\nof the word and so what is the most effective way to actually bring that\ncommunication bridge some of that gaps. I again, I really hate the like nursery\nof agents all running around each with their own persona of I'm a critic agent\nand I'm a reflection agent and I'm a email writing agent and they all work together. Like how do we set this up? So it's much more of a program\nthat gets operated, right? They're not people. They're not personas. There's instructions with\nvery clear requirements. There's all sorts of agentic capabilities\nin this program, like reflection loops and validation loops and other\nthings that happen, planning loops. But at the end of the day, it's a very\nclear program where information is passed from one program to another,\nand eventually a task is executed. Yeah, it's almost like we've\ngotten so carried away by like the dream of the agent that we're\nlike, oh, it's a little person. But actually the optimal strategy is like,\nwait, is it just is it just programming? Like we just have to specify very clearly\nwhat we want the software to computer science is like we're saying,\npretty please, I was looking at a prompt and an agent and part of\nthe prompt said, be persistent. Like how is this how agent, like\nhow the state of computer science has evolved to like this is the, our\nprogramming instructions to a model. There has to be a better way. Yeah, for sure. Well, great. Well, that's all the time we have today. Kate, Volkmar, Shobhit,\nthanks for joining us. And thanks to all you listeners. If you enjoyed what you heard, you\ncan get us on Apple Podcasts, Spotify and podcast platforms everywhere. And we will see you next\nweek on Mixture of Experts."
}