{
  "video_url": "https://www.youtube.com/watch?v=G8lnNR1-rsw",
  "video_id": "G8lnNR1-rsw",
  "title": "NVIDIAâ€™s Nemotron-4 340B models, Safe Superintelligence Inc. and AI agents",
  "upload_date": "20240621",
  "channel": "IBM Technology",
  "duration": "43:53",
  "caption": "Are we just waiting for these,\nlike, agents to get better and better? And humans will have to do less\nand less in the loop. Or, you know, is it less dependent\non the model's capability and more dependent on kind\nof the use case, like where is it headed? Is there a limit to how much we can abstract away? Hello and good morning. From my hotel room in San Francisco, you're listening to Mixture of Experts,\nand I'm your host, Tim Huang. Each week, mixture of experts brings together\na stellar group from research, engineering, product sales,\nand more to tackle and discuss the biggest trends\nin artificial intelligence. So this week on the show,\nwe've got three stories. First, Nvidia announced\nthe launch of Nemotron-4 340B an LLM, specifically designed\nto aid in the creation of synthetic data. How big of a deal is it,\nand what does it say about the next stage of AI training? Second, we'll talk about recent\ndevelopments for agents in the enterprise. Our agents reality now. And what can we expect the biggest impacts\nto be. And third, and finally, just earlier\nthis week, former OpenAI chief scientist Ilya Sutskever launches a company, Safe\nSuperintelligence Incorporated. What is it? And does it have a chance\nof becoming a new contender in the space? As always, I'm joined\nby an incredible group of panelists that will help us navigate what has been\nanother action packed week in AI. So joining us for the first time today,\nI'm Maya Murad, product manager for AI incubation. Maya, welcome to the show. Thanks for having me. And then we've got two veterans who are joining\nthat we haven't seen for some time, but I'm very excited to have both of them\nback. Kate Souleis a program director\nfor generative AI research. Kate, Welcome back. Hey, everybody. Great to be here. And finally, Kush Varshney,\nwho was part of the very first episode. So he's the OG, for a mixture of experts. He is an IBM fellow working on issues\nsurrounding AI governance. Yeah, O.G.,\nI guess it's a great designation. Yes, it's some kind of distinction. So, Kush, welcome back. Well, great. So I think the first, story\nI really wanted to dive into was, you know,\nI caught in the kind of constant, you know, wave of papers and releases\ncoming out. a launch that happened from Nvidia late\nlast week on Friday. And because it was a Friday launch,\nI think it kind of got lost in the news cycle. But I did want us to kind of focus on it. Nvidia released a model,\na class of models, called Nemotron-4 340B\nand it's a set of models that are specifically designed\nfor synthetic data generation. and I think it's so interesting\nbecause if you're not familiar with the background here, right, the way we train LLMs,\nget them to do their magic. A lot of it relies on data. and in the past, the way people have done\nthis is literally getting lots and lots of real world text, to train\nand improve their models. And so, you know, the dream of synthetic\ndata, I think is very fascinating. It's kind of like the idea\nthat in the future, we actually won't even need text\nfrom the real world. It'll just be like stuff\nthat another AI model generates. and I guess, Kate,\nI want to throw it to you first, cause\nI know you've thought a lot about this area and been kind of\ndoing some work in the space. I'm kind of curious, just as a first cut. Like, why is Nvidia getting into this area? I'm really kind of curious about\nwhy this kind of hardware company is saying what we're going\nto be launching these models. And one of the models we're launching\nis, is a synthetic data model. So I guess the first question I'll throw to you\nis like, do you have any thoughts on that? Conspiracy theories. Maybe not conspiracy theories,\nbut just like why it is that they're investing\nin this space at all. So, I mean, I think there's, some more straightforward answers and then maybe some side answers on\nwhy they're working in this space. I mean, one example of what\nyou might think of why Nvidia is working, and released the 340B model into the open source, is for synthetic data specifically is\nbecause they're recognizing that no one wants to run inference\non a 340 billion parameter model for real tasks like the value of models\nthe size. I think\noriginally there was a lot of excitement. Everyone wanted to build as big a model as possible\nand see how far they could push the field, but the reality is like no one's actually\ngoing to go and deploy this in production and hit a 340 billion parameter model\nevery single time you want to run. Inference is just too costly. But there is a lot of value\nin running inference on this model. Once using the data that you create to train a much smaller model\nand then deploying that out in production. And so I think part of this might be just\nthe field is starting to find new ways to add great value to these really\nbig models they invested on early on. because they do take a while to train,\nbut it actually makes a ton of business sense for Nvidia to,\nif you think about it, because customers need\ntheir customers need to get models hosted on their compute\nrunning as soon as possible. And what we're seeing is the easiest,\nnot the easiest, but one of the most exciting. And, most powerful ways\nto start to take models, improve them, use them for their use case\nand get them out into the world and deploy them in production\nis to align them using synthetic data. So more and more customers are using\nand consumers are using synthetic data in order to actually take models\nand tune them for your use cases and for your task. And so if Nvidia can help customers\nwith that cycle and help get models out into production faster, you know,\nultimately that's going to create some some good drag\nfor their their products. Yeah, it's such an interesting dynamic. I think particularly in that first point\nthat you're making is, you know, Nvidia like the company that I identify\nwith like really big compute, right. Like, you know,\nevery release is like bigger and bigger. And the models that you can theoretically run on them\nare bigger and bigger and bigger. But I guess\nsometimes you're kind of saying here Nvidia is like conceding\nthe reality that like most people actually are not going to be doing that. It's just like such an expensive thing. And so almost they have to win\nthe synthetic data game, or in the very least kind of like support\nthat use case just because it isn't. It isn't what most people\nare doing to create like the biggest, biggest,\nbiggest models in the whole world. Yeah. I mean the model, if you look at the,\nsupported intended uses, it's not just intended for synthetic data\ngeneration. It's a perfectly reasonable chat model. It can be used for chat use cases. But if you look at how it's been marketed\nby Nvidia, every single press release and blog and paper\nis all about the synthetic data aspect. And so I think they're recognizing\nthat this really is the only viable way to try and get value\nout of models of size. And it's also a really exciting way. I mean, it's\njust where the field is headed and where people are seeing a lot of,\na lot of use and value. Yeah, for sure, Maya I'm kind of curious, So I can bring you into the discussion. So I think one of the reasons we want to bring you on\nis my understanding is that you've done a ton of work\nwith kind of enterprises, right? And getting them to integrate\nnew kinds of technology. I guess I'm curious about what you're sort\nof seeing out in the space as like synthetic data becoming,\nyou know, a bigger part of the discussion. Do people want it? I'm just kind of curious about like, what the market demand for, for\nthis is looking like over time. I think it's a very exciting space. And it's a it's a need that existed prior\nto the rise of generative AI. So customers are they have their own data,\nbut they're also limited to it. And it's costly to create task\nspecific data. So there is a really strong premise of\ncan I start with only a few examples and augment\nthat data set to for various use cases to train my own model\nto evaluate on a use case to red team? so there's a lot of value\nto be drawn out of it, and it's one of the top customer inquiries\nwe get. yeah, that's really interesting. Yeah. I wonder if, like in the future, I just I'm curious if any of the panel\nis kind of view on this as like, you know, we've we've so identified\nand I think this has actually been certainly something on the policy side. People have been like,\noh, you know, AI is so data hungry. And so therefore it is kind of privacy\ninvasive, right. Like it just needs all of this data\nto get working. Synthetic data for me has always\nbeen like, well, maybe in the future, like actually real world\ndata is actually not going to be that big of a deal\nbecause you have a few examples and then you scale up with synthetic\nand there you go. I mean, is that a fantasy\nor are we headed to that world? I mean,\nI think in terms of using synthetic data as a tool to protect privacy,\nit certainly has a lot of value. I don't think we have quite, you know, there's a lot of work to be done\nin that space. in order to really take advantage\nof the promise that it offers. But my I've mentioned that even before, like, LLMs became around,\nwe've been using, synthetic data. And I think that's very much true. Like we've looked at, for example, a couple of years ago, using synthetic\ntabular data just to create privacy protected versions of sensitive\ndata sets where you can mask information. so there's a lot of different\nkind of really cool applications that I think are going to start to converge with synthetic data around privacy,\nhow that impacts generative AI training. and maybe help drive the space forward\nmore. I know, Kush,\nyou do a lot of work in this space. Maybe you can comment. Yeah. No, I mean, I think the the privacy\naspects are an important part of it. I think the exciting thing\nis actually going back to the fundamentals of probability. I know in the first couple episodes\nwe covered Kolmogorov and other friends, from that time and just the ability\nto sample like really high dimensional, spaces,\nwith just a few examples is like mind blowing in some ways,\neven though it feels very normal. Like with five data points,\nyou can sample this like trillion dimensional space\nand cover it so well. I mean, I think that's pretty crazy. So, yeah, I mean, this is just changing\nhow work is done. I think, in the whole generative AI space. Yeah, for sure. I think it's really, yeah, it's a really interesting way\nof sort of thinking about it. And yeah, I think we want to do more\ncollaborative type sections, like it feels like that was actually\nreally good for us to go so, so wonky. but I think it's actually important,\nI think in terms of like, exposing, you know, what's\nactually really going on under the hood. I mean, I guess one question for you is, given that this has been a long term\nresearch objective, right. Do you see the Nemotron release\nthis kind of largely sort of incremental like,\nis this a big deal this launch, or is it pretty much just like the most recent\nsalvo in the race for synthetic, data? So honestly, I think what I what is the most exciting part about this release is the release of the model terms, where Nvidia is actually saying we want this model to be used to train\nother commercially viable models and make no claims over models\ntrained with synthetic data created by Nemotron which is very different\nthan almost any other model provider that's created, especially ones\ncreating their own custom model terms. If you look at like the Llamalicense,\nyou know, gamma license and others that are being created,\nthey all prohibit that type of use. So the model itself is exciting. And, you know, I think people\nwill start experimenting with it and seeing how far they can take it\nin the next couple of weeks. But what they did\nthat is really totally different from how anyone else is behaving is actually\nthe permissions in which it was released. it's like the legal terms\nare the real innovation here. I'm sure there's lots of other innovation too, but that's\nwhat I'm sure you're excited about it. yeah. Yeah, for sure. And I think it's kind of\nI mean, you know, Nvidia is kind of playing a dangerous game here\nthough, right? Because there's a bunch of people\nwho are making these models proprietary because they want to build businesses\nat the synthetic data layer. I guess Nvidia is kind of saying like,\nwe sort of don't care. Like we would rather just have\neverybody have access to this. And, you know, for us to sort of enable\nall the secondary commercial models rather than creating like a market around\nsynthetic data, specifically. I mean, you got to use Nvidia compute to, to generate synthetic data\non a 340 billion parameter model. Then you have to use compute\nto train it, and then you have to use compute to host it. So I think they're playing a good long\ngame there. yeah I saw this great tweet recently,\nwhich is, you know, the adage of like, oh, when there's a gold rush,\neverybody should be selling shovels, but it's almost kind of like Nvidia is\nlike when everybody's selling shovels, you need to be selling shovel\nmaking machines. And I was like, that's actually like such a good way\nof kind of capturing what it is that they're what\nthey're doing in the space. There's a very broad topic I think we want to cover, on this second\nagenda item, which is agents. Right. This is like\nthe jargon in the space shift so quickly. It's like it feels like a few months ago,\npeople were like, oh, agents are coming. And then now it's kind of like agents\nare here and everybody's working on it. And, I guess, you know, Maya,\nI think part of the first thing I want to kind of bring you in on is just if you want to explain to our listeners\nwhat are agents exactly? And like, why is it a jump\nfrom what we've had in the past? because I think the definition\nand the distinctions have not always been so clear. And so I think even for me,\nI don't really know. but we're just like,\nit's a good place to start. Like, what are agents\nand why are they different? Yeah, absolutely. So the way that I like to explain\nwhat our agents is to first contextualize where we are\nin terms of building applications with AI. So in 2022, all about foundation models,\nall about large language models. I think we've learned\nsince that simply inputs and outputs from a model is not going to unlock\nthose high impact, enterprise use cases. So if I want to understand\nwhat is my vacation policy or if I want to retrieve real data,\nI have to build a system around it. And actually, Berkeley\ncame out with a really good paper this year\ntalking about compound AI systems. and it's a reflection of the fact\nthat you need to go back to plain old systems\nengineering to build AI applications. So modular components\nthat are fit to solve certain problems. So you have a retrievers,\nyou have external tools, and then you have your model interacting\nwith those to solve a problem. And we're still in this world\nand agents still operate within the space of an AI system. the way most of AI systems are built,\nfor example, retrieval, augmented generation\nis one of the most popular kinds is me as a developer,\nI prescribe the flow. So take the user query,\nrun it through a search, retrieve the results, feed it a model,\nand then the model generates an answer. So this is a flow that has been pre\ndescribed and it's fixed. If I give it something else\nit's not going to work because it was pre described to solve problems\nmaybe related to a vacation policy. and agentic approach in a system is that the LLM can reason\nthrough how to solve the problem and understand what is the path to chart\nto to answer a query. And this is done through two capabilities\nand that are building on top of large language models. So one, large language models\nhave really great reasoning capabilities. And they're improving as LLMs are getting bigger and stronger\nand are seeing more data. And it's operating\nthe same way we as humans do it. So if I give you a complex question\nlike how many times can the UK fit in the US\nif you if you, I ask you to give me a an answer on the top of your head, you're not going to get\nyou're most likely won't get it right unless you're a\nreally great geography buff. but we as humans, the way we think about it is we break down\nthe problem into smaller parts. So let me find the size of the UK. Let me find the size of the US. What tools do\nI have at my disposal to find this? Maybe Wikipedia is a trusted source, and then let me do some math\nto divide the bigger by the smaller. And this is exactly how an agent\nwould reason about it. And there's no magic behind it. You're just literally prompting the model\nto say, think step by step, create a plan. And then for each part of the plan,\nyou have access to tools. so that's the other part of it,\nthe ability to act and to call on tools. So a tool could be an API\nthat interfaces with Wikipedia. It could be a calculator,\nit could be a piece of program that can run a script for you. And when you combine all of those\ntogether, you're actually giving a lot more autonomy to the model,\nto how to solve the problem. You're not scripting\nhow the solution would be. The model takes care\nabout how to solve it. And this is what we mean\nby an autonomous agent. So long answer. But I think this is helpful\nto contextualize that. It's a continuation\nof where we are with systems design. Yeah, that's really useful. It sort of feels like we've kind of moved\nthrough these three acts already. Right. And like lasts,\nyou know, 24 months, right. Where like my the way you kind of phrased\nit was the first act was everybody thought we'd have one big model and do inputs, outputs\nand like problem solved, right. Like we're done. And then it feels like act two was,\noh my God, that doesn't work at all. What we need to do is kind of like\nprescribe all of these steps and then kind of like\ninsert the model into the process. And then I think act three is kind of\nit sounds like a little bit of where we're going now with agents is, well, we\nkind of go back to that big model state. But the trick is, I guess, that\nwe're allowing the model to like, develop these step by step instructions\non their own and then I guess, enable them to kind of like reach out\nand interact with all of these systems. so it, it\nstrikes me that the big challenge here is can the model actually touch\nall these outside services? because, you know, we've had stuff like\nchain of Thought for a very long time. That interface seems to have been\nthe difficult one. Right? Which is like,\nhow do you actually get the model to go and interact\nwith all these services? Because I suppose, in fact, it's\nkind of like get the agent to write an API on the fly. Is that kind of how people are approaching\nit? Yeah. So models are being trained to be better\nat generating a correct output to interface with an API\nor to run a piece of code. There's also, actually good privacy and security considerations here\nwhen you move to the agentic space. So prompt injections would be quite scary. And then I think I pass it on to Kush to\nmaybe talk more about that. But the other part, as well as code\nexecution, we've heard from even like\nI think the founder of OpenDevin, said the first time we run it,\nlike all the systems in the file system were deleted\nbecause of like the agent behaving. I have to be like really careful\nand how you architect that. And yeah,\nI don't know if you want to comment more on like some of the security privacy\nconsiderations with agents. Yeah. so clearly when, these things are interacting with,\nvarious, cyber physical sort of systems, then there is a risk of,\nthe different types of, prompt injection attacks and what we call\nindirect prompt injection attacks that can either get corrupted\nbecause something out in the real world has, problem or that is the models\nthemselves are going out and, actually causing stuff,\nthat happen in the real world. So I think, yeah, I mean, it is a danger. but I think the promise is also really,\nreally great. And I think we can build in some of the,\nthis security privacy sort of guardrails into the way that these, decisions\nactually interact. And. Yeah, I mean, as Tim said,\nI mean, the code itself, like the API code is being generated on\nthe fly, is like the, the most exciting part about this,\nbecause, code has traditionally been written,\nfor like a fixed sort of thing, but here, if it's being generated,\nthen, you can compose things, as in a creative way. And, I think that's, a unique thing. Yeah. All of these things seem to point\nto kind of the struggle of deploying AI in the real world, where, you know, the act,\none that I described is almost like the AI person's dream, where it's like it's a complete vacuum,\nand we just have one model that does everything\ncompletely out of the box input output. And we're, you know, we're done. and, you know, each step of this seems\nto have been like, actually, you know, there's actually all these legacy systems\nyou need to deal with. my is it right from Kush's last comment\nthat this is one reason why agents are potentially really good\nin the enterprise? Because I guess what I imagine here is, you know, your main\nworry about agents on like a consumer side is like they're going out into the world\nand doing all sorts of things, and, you know, they're subject to all of these\ninputs from the public that might be attempts\nto manipulate these systems. But I guess in the enterprise, like, you control\na lot more of the variables, right? You could have an agent that just like operates within a business\nand that feels like it might like, constrain the problem more in a way\nthat makes agents more viable. do you agree that. Yeah. So I think in general, when\nyou're taking a new piece of technology, especially one that has more autonomy in\nhow it can operate, so starting with low risk areas,\nmaybe back end services in a company, I think that would be a good place\nto start. And then gradually exposing it more as you architected systems\nto safeguard it from threats. Or it's just guardrail, it's internal behavior\nso it doesn't end up wiping your system. So I think with all technology\nlike you start with the the less risky place and then you\ngradually add more risk to it. And I think that's a sound approach. and then, in this world. So I talked about compound\nAI systems. Right. And then there's kind of on the one\nhand side you have a programmatic way\nof doing things. And on\nthe other side is the agentic like way. I don't think it's going to be one\nor the other. The two are going to talk to each other. So for some problems,\nthere's a especially for narrow problems where there's a very specific way\nof solving the problem that's repeatable. You're not going to get something out of left field, a query\nout of the field that doesn't like fit the solution\nyou had in hand go for efficiency. You go for the programmatic approach, for problems\nthat there's many ways of solving it. especially like for example, how to solve\nsoftware engineering problems. this is where an agentic approach\ncan be easy because there's multiple ways\nof going through a path to solving it. And these two will come together\nto solve problems in the enterprise. It's not going to be one or the other, and you're going to always going to apply\na systems mindset of how can I solve the problem most cost efficiently\nwith the right guardrails around it? Totally. Yeah, it kind of there's like basically a real big question here\non, you know, for a given enterprise, if you if you broke down all the tasks\nthey need to do on a given day, right? Like what is what is structured\nand what is like requires an agent is actually kind of a big question\nabout how that market's going to separate. Like, I don't genuinely know, like if you\ntook an average enterprise in America and you say, let's map\nall your business processes, you know, do they tend to be quite routine\nin the way that like, you know, does this sort of like\nwe just need to do this in this kind of like very systematic,\nsystematic programmatic way? Or does it require like,\nyou know, like the agent to go out and kind of like exercise\nsome innovation? Do you have any early signal on that, like for the clients that you work with,\nthe people that you talk to? are enterprise is still mostly favoring\nkind of this very structured approach or is there like like\nare there particular kinds of businesses that are like, oh yeah, the agent,\nthat's exactly what we need. Yeah, I would say most enterprises\nare still in this systematic approach. RAGRis a very popular use case. Most enterprises have built\nprogrammatic RAG, there's degrees of agenticness\nyou can build into it. So maybe you could have\na self-reflection loop. so you could take the output,\nof what the RAG system, provided and have the model reason on\nis this actually solving the problem at hand and maybe loop one more time. So you we're seeing some companies dabble,\nbut not fully embracing the the full autonomy of an agent solving\na problem and for multiple reasons. one is we're trying to understand\nwhich sets of problems are better suited for a fully agentic approach\ncompared to the programmatic approach. I have a hypothesis of\nif you have a narrowly defined solution that can unlock a lot of business value,\nlet's say it's like you're interacting with a database and there's a narrow\nset of commands you want to apply. You can engineer all the fall back\nloops by hand. And if you solve this, you unlock\n$1 million for the company. Go for the programmatic way. And then if you have a system that's going to get\nso many different queries that there's so many, you need different\ntrajectories to solve it, and you can't even think through\nall of the different ways this could fail. And running a pilot program,\nyou could uncover some, but there's a lot more to figure out. I think this is where having more autonomy\nin the system could be useful. But we're still early days and we're still trying to thread this\ntension between the two. And and I think cost efficiency\nwill come into this equation as well. For some problems, you're willing\nto spend more for 1% more accuracy, and then for other problems, you're going to be a lot more tighter on\nwhat's the right solution to implement. Well, and isn't there, a third dimension\non all of this, which is like, what do you want to handle\nprogrammatically? What can you start to free up\nand allow more agentic approach? But then what\nwhat always needs a human in the loop, like humans and agents working together\nwith just humans separately. so I'm curious,\nMayaif you have any thoughts on that, on that other layer of it, I think great agent systems that where we are right now might need a human in the loop. just and it, it would be great to have\nthe system know when to plug in the human. And you could like with the genetic\nframeworks out there, you could prescribe for the model to call in a human for help. so I think it depends on like the,\nthe what is the risk of harm or the risk of failure\nand the availability of an expert. And I think that's what you would consider\nof how you would bake it in. I think most autonomous agents we've seen, have\na strong element of human in the loop. So if you're looking at the,\nthe GitHub workspace assistance, all of this require you to revise the plan\nof the agent before it executes it, and then you get to see\nevery step implemented. Then you have so many ways to, to, for recourse or changing the,\nthe how the problem is being solved. And I think right now we're leaning\nvery strongly in having human the loop because we're early days\nand there's many ways this could go wrong. And I don't know\nif this answer your questions. Well, well, I was just going to ask, are\nwe just waiting for these, like, agents to get better and better. And humans will have to do less\nand less in the loop or, you know, is it less dependent\non the model's capability and more dependent on kind of the use\ncase, like where where is it headed? Is there a limit\nto how much we can abstract away? Yeah, I think these are great questions. I'm always in favor of me personally. I'm always in favor to have people engaged\nand for safety. I it's an open question of,\nare these models going to get better at what they do and be able to implement\nwhat they need more autonomously? Where we are with the technology right now,\nyou definitely need a human in the loop. Yeah, I think we're also\nI mean, the meta here is that it feels like we're talking a little\nbit about trust, right. there's\na hypothesis I've been chasing after, which is whenever people talk about AI,\nthey're almost like the AI is coming. And like,\nwhat does the AI enterprise look like? but the fact of the matter is like this. This depends so much on, like,\nthe culture of a given company, right? And like the degree\nto which they trust autonomy. and what you could actually anticipate\nis that like the, the implementation of,\nlike the programmatic approach versus the agentic approach, like where you lie on that spectrum\nwill almost be like totally defined by like how much management\ntrust the technology and then what\ntheir general behavior is like. I'm curious, like if a company\nwho already is like very structured with its employees and it's like, here's\nthe big list of things you need to do. Like, it will be no surprise\nthat when they implement AI, it will be like programmatic\nin that way, right? Whereas you might see cultures of,\nyou know, enterprises that are just a lot more like, well, we just like give people the goals\nand they sort of figure out what happens. And it will similarly not be a surprise\nthat when they implement technology or AI that they'll say, yeah,\nactually working with agents, you know, so long as that there's enough\nkind of trust in the technology. Yeah, there's the risk appetite. So I think that we're seeing\nwith companies and larger enterprises, like we said earlier,\nyou might start in low risk places. So like your HR systems,\nyour back end systems. And then you would bring it\nmore consumer facing. and it's I think it will be interesting\nto see this year what would be the appetite of enterprises\nto adopt more agentic like behavior. Yeah. That'll be so fascinating to see, especially because I think some of those\ndeterminations at even what functions are low risk is going to depend a lot on\nlike what else is happening in the world. Like you can imagine like a very high\nprofile failure, basically totally lowering people's appetite to like\nimplementing agents for a long time. And there's almost kind of\nlike a Google Glass effect where it's like,\noh, you have one wearable, which is like an enormous failure\nin the market. And then it's like difficult to, like,\nconvince people to put anything on their face\nfor the next decade. and so there's, there's a little bit it's\nkind of like crossfade, which is like, all these companies\nare making their own decisions, but it's also in like a soup of like what\nthey're seeing out there in the press and in the open\nand what their competitors are doing. Well, great. So I think to bring us home,\nI want to go to our third topic. there is a company that was launched,\njust this week, I think like 48 hours ago, 24 hours ago, by the name of Safe\nSuperintelligence Incorporated. and if you haven't been following this soap opera,\nthe background is that Ilya Sutskever, who is formerly the chief scientist,\none of the key players in OpenAI, and played a role in the board\ndrama and kerfuffle. that happened not too long ago. was, left the company. The circumstances are still disputed. and has reemerged with a new company\nthat, promises to finally deliver\non the dream of superintelligence. and, Kush,\nI think, you kind of threw yourself, kind of in front of the bus\nto kind of talk about this topic. and I guess I'm kind of curious, just\nas a starter, how big of a deal is this? You know, there's my point of view,\nwhich is it's becoming increasingly clear that, like, in order to play in the AI space,\nyou need very, very, very deep pockets. And, you know,\nthere's kind of a part of me that this sort of despair\nis at competitiveness in this space where I'm like,\nI don't agree with Ilya. I'm very skeptical\nof all the superintelligence stuff. But like, man, if I don't hope that like,\na small startup can, like go up against the big folks,\nis that even a possibility, like, like, does this this company SSI have a chance\nto, like, become a player in the space? Yeah. I mean, I think this is just a round\nof like multiple rounds that are happening where you start with idealism  and you then like kind of face the reality. see the professionalism of its openAI\nstarted with an ideal sort of thing. Then anthropic started with some ideals\nsort of thing. And now this is kind of the, the next one. and I think what's happening is. Yeah, I mean, the deep pockets are the,\nthe important aspect of it. So, because you need so much sort of\ninvestment to even get off the ground and, at some point the question becomes so for where the money came from,\nwhat's the return that they're getting? It's hard to stay, like, isolated\nand only be like, a research organization or like have like that single, like, oh,\nwe only have one thing on our roadmap. We only have one sort of goal because, like,\npeople want to, they want to get paid to. They do. I think that that's right. And like there's this conflict\nbetween like scaling and caring. so you can like, scale, and that is what I mean,\ncapitalism is all about, you can care\njust focus on like, one person, one sort of issue\nand then really go deep in that. And like,\nyou can't really do both. And so, I think that's where\nwhere the conflict is. And is this going to be successful? Maybe, but I think it'll be\njust one more round of this. So for a couple of years, 2 or 3 years,\nwhatever it is, they'll kind of have their point of view,\nkeep it. But then, yeah. I mean, someone will be at the table,\nasking to be fed. So, I think we'll,\nsee we'll see what happens. Two forces interact. yeah. I've a friend who's observing, like,\nyou start out trying to deliver on AGI, and then you find yourself being like,\nwe got to do B2B SaaS. Like, actually,\nit's like you're eventually kind of dragged towards this,\njust even if you want to fund the dream, like basically like where the money's\ngoing to come from is like these kind of very like day to day\nkind of, applications. Kush, I actually we haven't talked\nabout this in the previous episodes. I mean, do you buy their mission? Like, is the goal of superintelligence\nlike something we should even be chasing after? Does it\nis it a coherent goal? Right.\nLike, I think there's, like, real kind of critiques\nthat people have made in that space. But as someone who thinks about\nAI governance, who's like researching these issues, how do you size up, I guess this ideathat, like, we're going to do a company where the promises, Yeah, like to an investor even you put money in and we deliver\non superintelligence, right. Like, it's like the old DeepMind mission,\nwhich is like we solve intelligence first,\nand then after that, we solve everything. yeah. You know, is that something that you think\nis like, the right way\nof approaching some of these problems? Yeah. No, it's a great question. And, let me like,\ngive somewhat of a historical perspective, at least for me, like the first time\nI heard about superintelligence at all, it was, December of 2015. so this was at the NEURIPS conference. there was this whole day, like symposium,\nwhich they don't do anymore, but there was one. It was \"the algorithms among us\" was the, the title of it. And, there were a lot of different things. It was about like\nthe societal benefits of AI. And I mean, things that I was thinking\nlike, oh, I would be really interested in. And then I show up at this thing and,\none of the, the presentations is, Nick Bostrom\ntalking about superintelligence and, and, like in this whole, like, sort of day. but the word safety kept\ncoming up again and again and again, and no one was defining what even safety\nis, what they mean by it. And I think, like, I came home, like I tried to figure out\nwhat safety means to me. kind of wrote something about it as well,\nwhich was, minimizing the probability of harms\nand risks and minimizing\nthe possibility of unexpected harms and, this sort of stuff,\nwhich then lends itself to, kind of more, like clear and present\nsort of harms clear and present things\nthat affect society now and then. at the same time, in 2016, there was this, now famous paper coming came out,\nwhich was this concrete problems in\nAI safety, Dario Amodei I was the first author on that one. And, that somehow, like, just caught the attention of people. it became kind of like this religious\nsort of thing that, this existential risk, this big sort of, sort of thing for like, 100 generations into the future, like,\nwhat is I going to do to to humanity? And I mean, to me, yes, we do need to think\na little bit into the future. so there's this, concept\ncalled the seventh generation principle. So this comes from the Haudenosaunee,\ntradition and, like. Yes. I mean, you can think\n150, 200 years into the future and think about what might happen,\nthe consequences. But, like, so for into the in advance is a little bit,\npretentious in my opinion. So, superintelligence. There are risks, of course. but I would much rather both\nfrom a personal, societal and enterprise perspective, focus\non, kind of. What can we do? where do we take things\nand where do we protect things? protect things now? Yeah. I think kind of what\nI'm trying to reconcile is, and it may just be that we end up, like,\ntalking a little bit about kind of like how the broad universe of AI\nis going to continue to diversify and look very different\nat very different places. It does seem like I know we've spent like\nmaybe the last 40 minutes kind of talking about trends that are like almost very opposite of,\nyou know, superintelligence, right? Or like what Illya working\non, which is like, turns out a lot of people don't really need\nhuge, huge, huge, gigantic models, right? Like, ergo, you know, like what\nthe synthetic data stuff is, is kind of pushing towards like similarly\nlike a lot of the issues that businesses are dealing with there, like, you know,\nhow do we query a database effectively. And so it kind of feels like maybe there's\nactually going to be this, like the these two worlds\nwere very similar for a while, which was, oh, LLMs are going to deliver\non superintelligence. And oh, by the way, we can also do B2B SaaS,\nbut it kind of feels like maybe over time those technical agendas\nare going to go further apart. Yeah, I think at least\nfrom my perspective, from the agent space, the types of agents that are coming out are very narrowly\ndefined to solve a specific task. And then you have instantiations\nof narrow agents. So an agent that focuses on data\nanalysis, that collaborates with an agent that can do reporting. And this is kind of the path forward. That's that is getting gaining\nadoption and traction. And for several reasons\nyou're able it's more democratic. You can work with open source models,\nsmaller models that you can self-post. So you have full control over the system\nthat you're building. And I think this is the opposite\nview of the big monoliths that are trying to take a stab\nat superintelligence. And I think we and IBM research,\nI think I know which camp we're in, and it's like betting on giving you\nas developer more autonomy, you being able to control which models\nyou use, trying to use your own data. And the way to go about\nit is more narrow applications. I have an open question\nin my head of like, how broad and general purpose\nwe can go, and there's a limit to what would be useful to society\nlike Kush mentioned, just echoing what Maya said,\nI don't think that they are going to be very well aligned, incentivize moving forward. you just look at where like, we're going\nto be incentivized to develop from the types of tasks. You don't need super general intelligence. but I do question and I question,\nyou know, whether this should be developed in a kind of proprietary closed\ncompany versus in a more like academic consortium and other groups\nthat might be better incentivized and, have maybe, better priorities as we talk about, like,\nwhat society could actually benefit from for developing this type of technology\nversus just like some man behind a curtain with some VC money going at it. Pay no attention to the man\nbehind the curtain. yeah. Yeah, it's it's so competitive. And I think in part like the this kind of divergence\nlike for some of these companies. Right. Like so OpenAI\nanthropic is another great example. Right.\nWhere it's basically like very much instantiated\nby people who like really believe in want to work on massively\na agentic systems. Right,\nthat are incredibly general purpose. You know,\nthey would call it superintelligence. but in practice they've also had to kind of just deliver on\nlike the day to day of being a business. And it's kind of like to what degree can\nthese businesses kind of keep these two objectives in line or like even work on\nthings that achieve both ends? And I think kind of the question\nwe're asking is, well, maybe at some point the research or product development\nagendas here become quite different. it producer\nHan's actually just dropped that anthropic just this morning\nlaunched a new model in the space. so they've just dropped, Claude 3.5 Sonnet, and, you know,\none of the things that everybody's kind of observing\nor chattering about this morning is that it's actually very similar\nto what Google and OpenAI I have done over the last few weeks,\nwhich is speed. They want to launch models\nthat are just very, very, very fast. And it is kind of interesting is like,\nwhy do you actually work on speed? Well, it's not necessarily\nbecause you think the superintelligence needs to be really fast, right? Like, you know, what you're doing for\nspeed is like, oh, it actually opens up all of these other\ninteresting consumer applications, like, you know, talking on the phone with your\nAI, why you like, walk along the street. and yeah, and it's a I think a really good example is like,\nif anything, the recent set of announcements\nthis summer have all been kind of pointed towards the more practical\nthan the more sort of speculative. but yeah, I don't know if the panel's\ngot kind of thoughts on that or if they saw the recent Claude release. I think it was literally\nlike 90 minutes ago. So no, I haven't seen the Claude released. But I think on that point maybe being counter what I said before,\nwhich could be interesting. You know,\nyou think about the types of models that we're working with today,\nthe class of technology with transformers. It's not they're not efficient at all. They require huge amounts of energy. Like there's a lot of question whether this is really\nthe type of technology that will actually drive\nsuperintelligence. So if we are being more incentivized\nto focus on things like efficiency and speed, will that unlock\nand maybe help us discover new types of models, new types of architectures\nthat could serve as a much better basis for potential\nin general intelligence systems? Yeah. That's right. So we actually kind of unexpectedly\nlike in chasing through B2B, SaaS, we actually end up\nlike coming out the other end and being like,\nI guess we have superintelligence now. It's sort of like,\nsounds like what you're saying. Well, I, you know, maybe it it just\nhelps us get a little bit more diverse in where we're, we're investing. But. Yeah. Right. And to go back to something\nwe said earlier about like we used to have all this promise\nand like monolithic models will solve all your problems\nand will achieve superintelligence. And then I spoke a little bit about how it's actually like systems\nthat are bringing a practical angle. There's some really interesting papers\nand studies coming out that if you compare like the best in class models\nlike GPT4o and and system approach, whether it's a genetic or not,\nusing smaller models, you're actually on like a burrito efficient curve,\nyou're actually achieving better accuracy than GPT4\nand a much more cost efficient way. And I think when you like,\nif you're talking to enterprise customers, when like, this is the selling\npoint of you're able to do more, more accurately and more cheaply. And I wonder if it's like in the future,\nare we going to get monolithic models that are going to be better\nand out of the box, or it's going to the power\nis going to come from a systems approach. Well, and even the definition of like\na monolithic model is, is different. Like we're seeing like, okay,\nthis model is actually, you know, a mixture of experts of like eight smaller\n8 billion parameter models either fuzed together, but sometimes\nthey can be more independent, you know. So what is a model or a monolithic\nmodel versus what is a system model? I think those lines continue to blur. Yeah, I think we're I think\nwe're very much past the monolithic model. Like I think we all can safely say,\nlike GPT4 is much closer to a system\nthan it is to a monolithic model. Yeah, like the era of the big model\nis actually already over. Like we're not actually\nin that world anymore. I it's it's also true to fact. I mean, if you're one of those people\nwho uses, like, the evolution of the human mind as kind of like a projector\nor forecasting for like where AI is going. Yeah, it's kind of a view that like\nthe human brain was kind of like clues together over many,\nmany like centuries and millennia, which is like one piece being bolted onto\nanother piece, bolted onto another piece. And so in some ways, it's\nkind of a surprise that if we're working\non general intelligence as something that resembles a human mind,\nthat you'd actually end up with a model, ultimately that is like a bunch of pieces\nrunning around. It's like a bunch of kids\nin a trench coat, actually, is like how we achieve\ngeneral intelligence. Yeah. And even the safety work that, Ilya,\nI mean, was leading on the super alignment,\nwhere it's, it's a smaller model that's kind of controlling to the bigger\nmodel and making sure it doesn't go haywire is, again,\nI mean, this sort of architecture, the sort of view that, I mean, you're going to have\na bunch of things working together. So, I mean, the way they described it,\nwas, like this weak to strong generalization. so you have a weak model\nthat's controlling the strong model, but I think a better way to think about\nit is like a wise model that's controlling the strong model. So there's some aspect of wisdom\nthat's coming in. You like different properties\nof different components. can actually,\nkeep things under control as well. So just like, our wise, host\nTim here keeps all of us under control. I think, the there's rules. I mean, there's reasons for, for all of us\nto exist and, kind of work together. So, that's great. Well, thanks all the listeners\nfor joining us again. please join us next week. And as always,\nif you enjoyed what you heard, you can get us on Apple Podcasts,\nSpotify, and, better podcast platforms everywhere. so, Maya, Kate Kush, thanks for joining us\nand hope to have you back on the show. at some point in the future. Thank you. Thanks, everyone. Thank you. That's great."
}