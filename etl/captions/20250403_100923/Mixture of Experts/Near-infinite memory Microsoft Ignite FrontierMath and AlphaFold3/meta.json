{
  "video_url": "https://www.youtube.com/watch?v=Z-6d1gnSOAI",
  "video_id": "Z-6d1gnSOAI",
  "title": "“Near-infinite memory,” Microsoft Ignite, FrontierMath, and AlphaFold3",
  "upload_date": "20241122",
  "channel": "IBM Technology",
  "duration": "43:07",
  "caption": "Should your AI assistant\nremember everything about you? Vagner Santana is a Staff Research\nScientist, Master Inventor, and Responsible Tech team member. Uh, Vagner, welcome to the show. What do you think? No, absolutely not. I think we should be able to,\nuh, tell to our agents what to remember and what not to remember. All right, a little bit of both ways then. Uh, Vyoma Gajjar is an AI\nTechnical Solution Architect. Vyoma, welcome back to the show. Um, what do you think? Thank you. And only what matters,\nonly purposeful things. And Shobhit Varshney, who I'm\ndeclaring is the MVP of MoE expert guests, Senior Partner Consulting on\nAI for US, Canada, and Latin America. Shobhit, what's your hot take? It should remember everything just like\nmy wife does, but I do need an incognito mode that I don't get with my wife. Okay, got it. All right.\nWell, lots to talk about there. All that and more on\ntoday's Mixture of Experts. I'm Tim Hwang, and welcome\nto Mixture of Experts. Each week, MoE aims to be the single\nbest place to get caught up on the week's news in artificial intelligence\nand sort out what it means to you. Today is jam packed. We're going to talk about\nMicrosoft's recent announcements at its Ignite conference, a\nnew math benchmark, AlphaFold3. But first, I want to talk\na little bit about memory. So Google Gemini is touting a new\nmemories feature available to premium subscribers, where the model basically\ncan recall facts about yourself. So you can say, I like prefer, um,\nyou know, apples over oranges, or you can say, I'm really interested in\nthese types of topics, and the model is, uh, allegedly able to recall and\nuse this for context of the future. Uh, at the same time, there was an\ninterview done with Mustafa Suleyman, who, formerly of Inflection AI, is now\nthe head of Microsoft AI, um, and he gave an interview basically claiming that\nthey are, uh, sort of on the brink of unlocking, quote, sort of near-infinite\nmemory for models coming soon. Vyoma, maybe we'll start with you. Why does memory matter? Is it just more context for these models? Or is this kind of like a\ngame changer in some ways? I do feel it's going to be a\ngame changer going forward. It's not just context, but It's also\nmaking it much more, uh, relevant to the users that are using it, which helps\npeople adopt, uh, these technologies much better, because now if you have an AI\nmodel, which kind of knows exactly what you want and how you want things, it makes\nme go back over and over again to it. Um, the other thing that I feel is,\nIf we keep looking and experimenting with AI models with more memory, we get\ninto a point where we come up with much more creative nuances of generative\nAI, which we've not seen as much yet. Yeah. And I think the user experience of\nthis is going to be so interesting. I mean, we even saw it in the first\nquestion is, you know, Vagner was like, well, it should remember what\nit should and shouldn't remember what it shouldn't, which is very much\nalmost like, I want to choose, right? Uh, and then I think Shobhit,\nyou were like, it's just remember everything, but I want the ability\nto opt out when I need to, right? And those are like two very\ndifferent ways of looking at it. Um, I guess Vagner, do you want to start? I'm curious about like why you\nanswered the way you did and what you're trying to preserve, right? When you're, when you give\nthat kind of response. I was thinking about situations in which,\nuh, imagine that we have agents that know almost everything about you, near-infinite\nmemory, and the consequences of that. Like imagine, um, advertisement or other\nthings that could be done with that kind of information, um, willingly or not,\nor not, uh, uh, considering your privacy or your, uh, uh, desires, so to say. So if, if there's a, um, let's say,\nuh, a business model behind of that using this near-infinite memory to\noffer things to you that I think that something that supposedly was to\nimprove the user experience thinking about things that is are good for you\nbut then with a business model behind that that may become a different thing. It's a little scary. I mean, I don't know, Shobhit, if\nthis is where you were going, but I mean, there's one counter argument,\nwhich is this is like the internet. You're like describing the internet. People are already\ntracking you all the time. You know, kind of why should\nmodels be any different? I don't know if, Shobhit, that was\nthe direction you were thinking about going down, but I think\nthat's like one question here. There's a different take if you're\nlooking at my personal day to day world. Uh, like if I need to go remember\nwhat I did in, uh, in Mexico, like six months back, where I stayed\nand stuff, I just expect to go into Gmail and ask that question to\nGemini and get a response, right? So I do expect that there's somebody\nwho's augmenting my long-term memory. We are really good at short term memory. I need somebody to\nmaintain that long-term. Uh, I have been very consistent in\nmy responses on this, on this podcast about enterprise focus, right? So for me, when we start to look at\nenterprise, I'm working with a very large healthcare client right now where\nwe're trying to build these, these virtual assistants that'll have infinite\nmemory because they are essentially, they're, picking up where you left off. Every time a conversation starts,\nit should be a hot, warm start. It should not be a cold start\nwhere we're asking information about what we should already know. So picking up from that moving toward\none on one personalization, that's the promise we've had for a decade. But ultimately, now we have\nthe right way of doing it. So if I do load all that context from\nback-end system passed on to a large language model, in the memory itself,\nit should be able to go look at what we've had the conversations around and\nthen be able to fine tune it and the conversation that they're having today. So it has to be more contextual. So for me, the memory plays a\nhuge, uh, has a huge impact when you're looking at enterprise. One-on-one personal relationships\nyou can build versus having a very generic goal you introduce\nyourself from scratch every time. Yeah, it'll be sort of interesting. I mean, I guess Shobhit, you're almost\narguing, and it is one of the questions I had is like how competition around this\nfeature is going to emerge over time. And you're almost kind of saying, well,\nlook, maybe if it's like a personal chatbot assistant, it'll have to just be\na lot more discriminating on these things. But in the enterprise setting, people,\npeople want access to everything. You know, I don't know if\nthat's what you're saying. I just wanted to add a little bit\nnuance there to Shobhit's point that there is an unexplored territory\nin the whole generative AI world, which I feel is called forgetfulness. So let's say, um, a lot of models,\nthey keep remembering things about you. Imagine if it keeps building\non some irrelevant data. As a human myself, I'd like to\nforget some data about myself, or I'd like to change myself. I might like Korean food today, but\ntomorrow I just might not like it. So how do you make these systems not\nto forget the long-lived data, but like use that more efficiently and make sure\nthat we're using those relevant parts such that the AI systems don't become\nbogged down with irrelevant information and they're not biased going forward. Yeah, I can imagine actually, there's\ngoing to be this period where, you know, we're all very excited about memory, we're\ngoing to have infinite memory features, but then it's going to be that funny\nthing where you like browse one thing on Amazon, and it just recommends that\nforever, you know, it's like, Oh, you really want to buy, you know, my friend\nbought like a toilet seat recently. And it was like, customers like\nyou also enjoyed, and it was just all more recommendations, not\nreally realizing that, like, that's kind of an incidental transaction\nversus a, you know, an ongoing one. That's what I meant by\nsaying incognito, right? Mm hmm.\nYeah. With chat GPT today. I do temporary chats quite a bit. I don't want to remember memory\nabout things that I'm asking it to do every time, right? So temporary chat with chat GPT,\nincognito with Chrome and Safari and things of that nature, those are meant\nfor those kind of use cases, right? This is a one off thing that I want to do. I don't want you to remember this. I don't have that option with my wife. I just want to have infinite memory. That's right. Uh, I'm a question for you for some,\nuh, you know, I'm sure some listeners will have this question, which is, Is\nmemory just kind of like RAG ultimately? Like it's ultimately just\nlike a document of facts that the model is retrieving from. So, you know, what is the difference\nhere if there is any difference? Is memory more of just like a\nmarketing phrase or is it, or are there actually kind of technically\ndifferent techniques going on here? Yeah, so it's a very nuanced\nworld now that we are in this whole multi-modal world right now. So when we- when we speak about memory,\nit might not just be text that you're giving out or prompts that you're\ngiving in about yourself or you want to ask it things about something else. Let's say there are clients\nthat I work with myself as well. I give it a picture that,\nhey, this is a picture. Can you help me evaluate what is in it? Can you develop this picture for\nme or can you like edit it for me? So it's not just a part\nof information that you're providing, but different facets. to it as well, about something that\nyou like or something that you dislike. So I don't feel that eventually it is a\nRAG because unlike humans, it will add all this information as a structured\ndata into its particular memory, but there might be different models to\nit that it also evaluates it against. So, um, RAG can be one use case for that\nparticular, uh, infinite memory, uh, ingestion, but there can be many other\nthings that can be done on top of that. Got it. Yeah, that's a really\nuseful clarification. Uh, Vagner, I think I'll give\nyou the last word on this one. Um, you know, I don't know\nif you buy all this, right? Uh, it seems like out of the\npanelists, you might be the most kind of privacy sensitive. Um, but do you think the future is\nkind of like optimal forgetting? You know, I guess you could even kind\nof fine tune a model, which is like it should forget the way I forget. You know, it's almost kind of like a\nreally interesting way of potentially thinking about it, but curious like\nwhether or not you think that there is this kind of nice distinction between\nenterprise and personal and and, you know, how these features will kind of\nevolve if we are sensitive to stuff like I think what you're raising,\nwhich are the privacy concerns. Yeah, and I think that only not for\nprivacy concerns, but also thinking about fairness and thinking about, uh,\ncertain, uh, business case as well. Um, in terms of fairness, if you\nthink that, uh, let's say credit score, uh, how long should we\nconsider in terms of, uh, data? to think about a fair credit score, right? How, uh, my life in the last five\nyears, my finance life is different from my life like 15 years ago, right? So, uh, that is an important distinction\nto, to make and how these things can be done to increase, uh, biases, right? And, and unfairness in certain\nalgorithms, in certain systems, in certain, uh, businesses. So I think that my concern is more\nto that direction, how, how to define this, uh, uh, interesting point,\nright, on how to, um, keep a context that is useful for the, the user and,\nand actually brings value in terms of business context, but also protect the\nuser and also people in communities that have historically been marginalized\nor penalized because of their, uh, uh, uh, characteristics that some, uh,\nunfair algorithms may, uh, distinct in when, when taking certain decisions. I think that that's the, uh, uh, uh,\nhard point Sweet spot to find, but it's something that we need to have in mind. Totally, yeah. I mean, you're talking about kind\nof a classic problem of, right, like I, uh, I commit a crime\nwhen I'm a young kid, right? And then that just follows me\naround for the rest of my life. And how do we want to manage that? It's like genuinely a pretty,\na pretty hard problem. And I actually wonder if this will almost\nbecome like a, uh, a kind of competitive thing in the market as we go forwards. Right now, everybody's\nlike, memory's a new thing. So, you know, Google says we have memory. Microsoft wants to come out and\nsay we have infinite memory. Um, but I actually wonder if after, after\nthese features become more commonplace, the, the reverse will be the case. It'd be like, I've, like, this\nproduct forgets in the right way. And like, that's why you should use it. Will be very, very\ninteresting to, uh, to see. So our next topic for the\nday is Microsoft Ignite. This is the Microsoft conference for I .T. professionals and developers\nthat happens just this week. Um, there is a large number of\nannouncements, but in particular was interested that the company made\na really big emphasis on safety. So among other things, the company\nannounced there was an event they announced called Zero Day Quest, which\nwill be an in person security event, and they announced a fairly large amount\nof money for $4 million in bounties to expose vulnerabilities in AI. Um, and I guess Shobhit, I\nwanted to turn to you first because I know you were there. I think you just got back, um, and\nyou were covering it on the grounds. Uh, curious from your\neyes what you saw there. What are the big trends? So we, Microsoft was a really big\npartner for IBM and I'm wearing that IBM Microsoft shirt right now. Yes, . Yeah. We got the logo. So we got, we got a couple big\npartner of the year awards for all the work that we do with Microsoft. We've had more than a 30 year\npartnership with Microsoft. It's been scaling tremendously, both on\nthe consulting side, as you would expect, but also on the IBM technology side. Within Ignite, this is one of the\nthings that they had to address very clearly around security. Uh, what happens with the copilots? Are they, is there any possibility\nof leaking data anywhere? How do you do access control\nand things of that nature? Uh, I believe out of all the companies\nthat we work with, all the ecosystem partners, understanding the ecosystem\nof SharePoint access, the email that was sent to somebody but not to the others,\nthat kind of a graph of access control is something that's very unique to Microsoft. So they're doubling down on that. So they are making sure that if I do. That's the create a agent to go to\na particular SharePoint, the access control automatically kicks in, right? So they've just made it natively\nembedded across everything. They also spent a lot of time and I\nspent the next two days with them on deep diving, technical deep dives on\nspecific capabilities around governance, all the partnerships that they've\ndone with showing what's happening across the pipeline, weights and\nbiases or with Kredo, with Arise, and all the other third party tools, that\ngives you the full gamut of what's happening, all the traceability, all\nthe evaluations, things of that nature. So I think they've addressed both the\ntransparency and evaluation frameworks, uh, governance, as well as the security\ncontrols in place very, very well. I was, I was genuinely pretty, pretty\nhappy coming out of the conference. I've done some hands on work with them. They've addressed this phenomenally well. Yeah, for sure. And Vyoma, maybe I'll turn to you,\nbecause I think one way of reading all these announcements is very clear, right? Like, Microsoft wants to be\nthe safest place, in some ways, to design AI products. And I think it's sort of a really\ninteresting kind of competitive edge on the market, right? You could, you could come to\nthe market and you say, we've got the biggest, baddest models. Uh, and I think Microsoft obviously\nwants that as well, right? But I think it's also now saying,\nwell, one unique thing about working with us is, is safety and security. Um, and I guess you work\nwith a lot of customers. How are you seeing customers kind\nof trade off against these things? Because I'm sure on one hand, they\nwant, the shiniest tool, right? But on the other hand, as they're\nworried about, you know, kind of like the security of the deployments they\nwant to work on and curious about how people are balancing that and whether\nor not you think this bid is really, it's where kind of the market is going. That's a great question. So the past one and a half year,\neveryone experimented with Gen AI. And they've done a lot of POCs, et cetera. But now rubber is hitting the road. Now things are going into production. Once they go into production, then\ncome the different issues of privacy concerns, security concerns, et cetera. And I have always seen Microsoft\nas a leader, innovator, and now a steward of responsible AI as well. So the way that they are augmenting\nit into everyday, Uh, products such as Microsoft 365 and their\ndifferent OS systems as well. That kind of showcases and instills\nthat faith in users who use those products on a day to day basis. All of us I feel at some point,\nuh, use Word or like Excel, etc. And I feel that kind of is the right way\nto go to get everyone talking about it. And then when you say about\nlike, uh, you asked me about how clients are looking into it. Each client wants security measures on\ntop of it, if not any success metric that is available out of the box. They want to bring\ntheir custom metrics in. They also want to create their own\nmetrics based on the kind of information that is coming out before, um, it-\nthat particular product or like chatbot in our case or rack system\ngoes into full fledged production. They want all security guidelines\nadhered to because now no longer there's just like the AI tech team sitting\nin a boardroom making the decisions. Now there's a finance team sitting\nthere, a legal team sitting there, and then the entire tech team too. So there are so many different, uh, minds\nat play here that all of them will feel much more, uh, secure if there are like\nguardrails, uh, defined around them. Yeah, that's great. And Vagner, maybe I'll turn to\nyou next because I think that there's this one question I've been\npondering a lot in this space is when we say safety for AI products? That's, that's very broad, right? It's everything from, is your model\ngoing to leak the data that, you know, you've put into the system, um, to can\nhackers, you know, get access, right? Can they manipulate what the system\ndoes to even the bias questions? I think that you raised on the last\nsegment, and, you know, so I think safety is kind of the shifting\ncategory where who's responsible for it and what you're working on is,\nis always kind of moving over time. And I think it does feel like here at\nleast, right, there's a lot more of emphasis on what you might call kind\nof like this technical safety, kind of like cybersecurity in some sense. Um. Do you think that, like, you know,\nultimately these teams will also be responsible for the types of\nbias questions that you raised earlier, or is that going to live\nkind of elsewhere in the enterprise? It's interesting that we see\ndifferent approaches to this. There are some, um, some companies that\nput everything on developers shoulders. Like, oh, you're responsible\nfor taking care of the safety. There's a lot of discussion\nright now on defining what. is exactly safety nowadays, because when\nwe think, uh, we look back for, let's say, in aviation or other systems, it\nwas, it had, it had a different flavor right now with stochasticity of these\nmodels, it's really hard to define and, uh, with, uh, synthetic data as well. So we are. touching a really unknown territory\nin terms of how to even how to define how what is safety in\nthis world that we're living. Um, so I don't know how\nto answer your question. No one knows is the answer. I brought more questions because it's\nsomething that things, for instance, our group here is touching on and how to think\nabout safety in these new terms, right? It's hard even to define it and, and, uh,\nto define the boundaries of that as well. Who's responsible for, for that? Well, we know that, uh, when we\ncreate a technology and when we deploy that, we have this, uh, um,\nentanglement with the technology, right? We are actually responsible for, for\nthat technology that we're delivering. But when we look down for downstream\nimplications, that becomes really more complicated, especially in a B2B settings. With enterprises, we look at\nit at three different levels. There's the security of the infrastructure\nitself, the network, the hardware, access, things of that nature. The next level from there is security of\nthe data, who has access to what controls, things of that nature, any breaches. The third level on top is the security of\nthe application itself, that includes the actual AI and the model and the LLM app. And things of that nature, right? So the three different levels,\nthere's a varying degree of how much of that is the network security team\ninvolved, or those are the classic security teams in the companies. And as you go up to the application\nlayer, you start to think more about the responsible use of how you want\npeople to use this application. Let me just, uh, let's pick an example. One of the big Fortune 50 companies that\nwe're working with in the CPG space, they just launched a massive campaign\naround um, their water bottles where you can go on the website and go create\nan image with Adobe Firefly and others, be able to print it on that bottle. And that's something that's very unique. It's a unique design that you have built\nand that gets shipped to you directly. So we are running that in that\nplatform for them end to end. So in that platform, we need to make\nsure that model itself that we're using has its own red shirting and can go\nreject things like if you want to go create an image that's not appropriate. One level up from there is the\nactual cloud vendor, right? If you're building this on Azure,\nIBM, Google's of the world, and each one of those cloud vendors\nhas on filtering processes, some policies you can set for in and out. One layer out is this platform\nthat we have built for for them. That platform can have some rules\nas specific to that company. And across any third party tool\nor any third party cloud they use, we'll filter it out there. And ultimately, you have\nthe application level. So for example, if you if you are\nif you have a lot of points with that particular company because of\ninteractions, you may have unlocked some more premium images that you can create. But on the platform side, we may\nneed to say creating an image of an astronaut is okay, but an\nastronaut on that bottle holding a competitor's drink, that's not okay. Or wearing clothes that are\nnot appropriate is not okay. So all that has to be filtered in. And we're getting thousands of these every\nday when people are starting to use this. These things go viral very quickly. So we need to have enough of\nfiltering mechanisms for the safe use of that particular product. But on the infrastructure side,\nit's more around cyber security. So the three layers as you go further\nup, it's a blend of security and the responsible use, I think, and\norganizationally, they'll start to get, get tied into one organization. Yeah, that's really interesting because\nI think, I mean, you know, Vagner's response kind of corresponds to what\nI've experienced this right now. It's a little bit of\na free for all, right? It's like everybody knows they want\nthese models to be safe, but there's maybe 10 different organizations inside\na company, for instance, that are tasked with different aspects of the problem. And it ultimately has this effect\nof making the security posture very, often very incoherent. Right, and I think it's interesting\nto hear your prediction that, like, as things get a little bit more mature,\nthese will start to kind of coalesce into maybe a single org, or maybe a single\nperson will sort of be responsible, managing teams that are looking at\nthis from a number of different angles. So Shobhit, maybe I'll ask you\nthis, is before we leave Ignite, any announcements that you're excited about? Things to look forward to? So I think it's really reflects the\nmaturity that we are that we are seeing. There were a bunch of gaps with the real\nworld deployments and we might mention a couple, but as we have scaled these\nout with Microsoft and like we have all kinds of offerings around their copilots,\nwe do a ton of custom copilots for clients at scale across each industry. We do a lot of Azure\ntransformations, their OneLake fabric on the data side and stuff. Across each one of them, there was a lot\nof incremental progress they've made. Uh, the really cool thing, some of the\nreally cool things that stood out for me personally was their Azure Foundry. They've done a good job of bringing all\nof their AI tools under one umbrella. So it's just on the studio,\nthey've governance, they've models. There's a lot of talk about industry\nspecific models, how to make this easy for you to fine tune with your own data. Azure Foundry.\nA lot of talk about security and stuff. And there are a few different\nfeatures like side-by-side comparison of LLMs on the same topic. Google has been doing\nthis for a while now. We need to go have a set of\nlearnings from each other as well. So I'm trying to, we're seeing\nall the different vendors catch up to the kind of Things that are\nneeded to put LLMs into production. In 10 days, we'll be at the AWS reInvent. Under NDA, we've seen quite a bit\nof really cool things that they're bringing out in the next year. It's really exciting to see all\nthe different vendors catch up with each other and one up each other. And the great thing is they're\nall in the service of enterprises. So, data, their fabric, what they're doing\nwith the data landscape on the Azure AI Foundry side, they've done quite a bit. They did have a lot of\nthings around hardware. They are ensuring that the whole\nstack works, works very well. Uh, both from their own\nproprietary hardware, plus all the partnerships that they've built. Uh, they, we do a lot of work with\ncompanies like NVIDIA and Azure together. So there's a lot of clients where you're,\non the infrastructure level, there's a lot of, uh, good, uh, synergy between\na lot of our vendors working together. It's, it is, it was a great,\ngreat event and I'm just very, very excited coming out of it. Especially after the keynote, when you\nstart to go hands on and you work with the product leads, the, the research\nteams and stuff, they've done a really good job at piecing everything together. A few episodes ago, we were\nlike, we're just done with the summer announcement season. We've got a little bit of a break\nand it feels like basically like the gas is being revved up again as we\nkind of get into the end of the year with these final few conferences. Our third segment of today is going\nto focus on a new benchmark that's on the scene called FrontierMath. We love benchmarks here at Mixture\nof Experts, it's one of the things that we cover almost as ferociously\nas we cover new product features. And this one's particularly\ninteresting because it was released by a research group, uh, Epoch AI. Um, and what's interesting about\nFrontierMath is that in contrast to a lot of benchmarks that you may\nbe familiar with, this benchmark specifically contains unpublished,\nexpert level mathematical problems that specialists spend days solving. So in contrast to like an MMLU\nor other benchmarks you might be familiar with where you yourself as\na human could go through and evaluate them and do the test yourself, um,\nthis is specifically designed to be the ultra hard benchmark on math. Um, and, uh, Vagner, maybe we'll start\nwith you on this particular topic, you know, we've talked a lot about how the\nbenchmarks are getting increasingly gained in the AI space, you know,\nwhen a new model comes out and they're like, look, we beat all the benchmarks. I think everybody kind of just like\ncollectively rolls their eyes now and says, I'll just kind of, you know, load\nit up and test it out myself and see whether or not I think it's good or not. Um, But this one's really interesting,\nand I guess I'm curious if you think that this is indicating a kind of new sort\nof meta in, uh, AI benchmarking, where the new trend is now going to be like\nthe benchmark that's so hard that you need to be a world class human expert\nto even deal with it, um, and, uh, and what you think that means for the space. I think that you already mentioned\na really key aspect that they are novel and unpublished. So then I think that if you think\nabout the challenge for mathematicians, it will be like, okay, you think\nthat that thing reasons, let me show you what is a real hard problem. So I think that it was a really\ninteresting approach to that. And I think that at the\nend, it shows us that. Uh, there's not much reasoning, right? It is word prediction and the, the,\nthe technology that we are, uh, talking about and, uh, and, and trying to, uh,\nput things on the ground and discuss how the technology actually works. And I think that this, uh, benchmark is\ntrying to expose more of the capabilities and also limitations of this technology. I think that it's an interesting aspect,\nand, and, and the interesting thing in the report is that, is that it was saying\nthat, uh, only 2 percent of the problems from the benchmark were solved, right? And so, uh, uh, maybe, and, and\nI think the interesting thing is, Uh, what happened in these 2%? I think that is an interesting\ndiscussion as well. If these are novel and unpublished\nproblems, how do this technology, uh, solve actually 2 percent of these\nproblems that were unseen for that? I think that is the most interesting\naspect for me, but, uh, yeah, I think that the, uh, it is an interesting, um,\napproach to show novel things and actually is not part of the training data, right? And how these technology is tackling that. And, and, and connecting to, uh,\nwhat we always talk here about, uh, enterprise business case, right? Uh, what will this technology do if it's\ntouching for the first time, uh, to, uh, some enterprise, uh, uh, data that this\ntechnology, uh, had never seen before. So I think that that's an interesting\ndiscussion to, to, to bring up that, uh, we sometimes talk a lot about the\ntraining, the data used for training, and what would happen if, uh, let's\nsay, uh, Gen AI or LLM specifically, uh, uh, interact with new data. And I think this is an\ninteresting example of that. Yeah, for sure. It's really interesting. I mean, there's a fascinating point\nhere about how, you know, because these benchmarks are unpublished, you start\nto see the real edges of AI capability. And so kind of like the apparent\nsuccess of models against a bunch of these benchmarks may just be\nbecause we've been lazy, like no one wants to spend the money and time\ncreating entirely novel test sets. And so you do end up having a lot\nof kind of like repeating from training data, you know, being the\nreason why there's a lot of success. Um, I don't know, I guess maybe\nVyoma, Shobhit, I don't know if either one of you want to take this one. So this is very fun from a\nresearch standpoint, right? Like what's that 2 percent mean\nand what is this model doing and how does it succeed here? Um, I guess on the commercial side,\nlike, do we, do we feel that enterprises are like, we need better benchmarks? Like we need harder benchmarks because\nit's clear that the benchmarks that we have aren't giving us enough signal into,\nyou know, whether or not these models can do stuff that's more than just kind\nof search and retrieval effectively. So, um, This is my, my\nquick two cents on this. Um, when we are recruiting a human for a\nparticular job in an enterprise, we have some good, uh, expectations on what we\nexpect them to do out of the box, right? From day one, they've had a training\nin psychology or accounting and things of that nature, right? I think we need to have some level\nof entry-level domain expertise that we need to judge each model by. So in that sense, the corollary\nwill be we need to have some benchmarks that are domain specific. And then within that, as the AI model\nstarts to do better, just like we do performance reviews of our own team\nmembers on specific topics, a hard call came in about a tax issue, were\nyou able to solve it or not, right? So we then start to differentiate and say\nthis person is a subject matter expert. Not just in the domain, but in the\nexpertise in this industry, in our specific company, in our specific,\nuh, uh, tools and the documents, things of that nature, right? So I think there has to be some\nlevel of gradation of what kind of benchmarks we go through, and then\nyou give them scores accordingly. And that should be the way you charge for\nthese benchmark, for these models, right? So if I'm hiring somebody who has a\ngeneric accounting degree, I may pay X dollars for them, but as they start\nbecoming an expert and as they go through different tests, we, we know that\nthey're, that they're doing a better job. There's also a continuous\nevaluation piece to it. So just, I'm not sure how many people\nrealize this, but as a physician, my wife has to take an exam every X number\nof years to recertify herself that she knows pulmonary, she knows critical care\nwell, and so on and so forth, right? We do need to have some sort of a\nContinuous benchmarking that's needed over time, the kind of problems that we need to\nsolve or what we are seeing will change. So we do need to have a starter\nset of benchmarks for enterprise, and you keep evolving them based\non the kind of question that we are really, really getting. There'll be very few people\nthat we need in our organization that can humanly solve the math\nbenchmarks that they just created. PhDs in this area were not able to\ndeliver that kind of accuracy themselves. So is it working to stumping both humans? As well as AI. So you really need to be doing, writing\na PhD in that particular domain to be able to answer that question in one shot. So this whole one shot getting\nto the answer versus using tools and agents giving to the answer. So there has to be a mix of all of that. And there's a sense of\nconsistency of the response. If you look at Claude's\ndesktop use, right? In the benchmarks they released,\nthey had a benchmark at the very end about tool usage. That says that the consistency\nof the response, I ask you to go book me a flight to London. I asked you that in 10,\nin 10 times in a row. Today, the models getting, getting it\nonce right out of 10 is very, very high. Getting it 10 times in a row\ncorrect is very, very low. It's embarrassingly low how bad these\nagentic frameworks are in getting the same thing correct in a row, right? So I think there's a consistency\nbenchmark, there's some levels of benchmarks that are needed, and the\nbenchmarks need to evolve based on the kind of usage we are seeing. Vyoma, you'd add something to that? Yeah, I was just going to add right\nexactly to what Shobhit said if we pivot into it, unlike the generic\nbenchmarks that we have, the one that FrontierMath focuses on is the\nmathematical reasoning behind it. So imagine whatever Shobhit said\nthat we need, um, uh, like the same answer not being consistent, imagine\nFrontierMath starts, uh, spitting out something like a cognitive residue. So every time it spitted out an answer,\nit gives you exactly how it did it. That's one of the ways and that\nhas been pitched in this new research that has been coming up. And then the agents and the models\nstart becoming more and more intelligent and understanding their own patterns. That, okay, whatever I said before\nis now based on this parameters, and then we go keep adding this. So I feel there's a whole avenue that\nis going to be released in the form of research in AI, mathematical AI. The moment I read FrontierMath,\nI'm like, oh, did they solve the NP hard problem yet? So that's, that's not, I\ndon't see it in the future. But one of the things that I feel\nis it will at least help you, you know, understand some computations,\nwhich mathematicians or statisticians do to solve a particular problem\nand then at least rule that out. Imagine the amount of time and\nenergy that is being put in creating validation test sets or test data sets. So all of that being done by some\nother technology to expedite the process would be a good avenue as well. Yeah, for sure. Um, yeah, and I think that's like\none of the questions that I think is interesting that's kind of presented by\nFrontierMath is, you know, to Shobhit's point, these models are really bad at\nconsistency, but you know what's really important with consistency is like\nmath, like when you, when you add two numbers together, you're always supposed\nto get the same number every time. And so it almost kind of begs\nthe question of like, is the technology good for math at all? Um, and I think, you know, these\nbenchmarks are helping us kind of like think through the problem. I feel that. There's a, there's an adjacency\nthat comes with being good at math. Your reasoning skills, the way you think\nthrough a problem, you break it down in your head, I think that helps LLMs do a\nbetter job at reasoning elsewhere as well. We see this with code as well. When you add code to the training\ndata, You see them do a better job even on the tech side. So consistently if you look at models that\nhave added more code, even for text-only LLM kind of responses and stuff, it does a\nbetter job at reasoning and understanding. So if I give you a problem where I need to\nfigure out what is the real root cause of what the customer was complaining about,\nyou do need to figure out that, oh, so far I've seen that there are three things\nthat they talked about, the first two kind of got resolved, the third one did not. But this is what they really\nwere complaining about, right? So the fact that you can reason and\nthink through a particular model, and the word reasoning is not very well\ndefined, uh, like in our industry yet. You'll see a lot of exuberance around\nthe word reasoning versus some others will just dunk on it saying that, no,\nthis is just smartly regurgitating. But across that spectrum,\nI think they get better. When they get better at math as well, Shobhit, I do really love the idea\nof thinking about this almost as kind of like recruiting and like a human\nsense or in the future, you're going to be like, I just need to get like\nthree, I got to staff like three entry level models and maybe like\none senior model to run the team. It will like start to feel\nlike that as these evals really become kind of like the way we. We see whether or not we wanna\nwork with these models at all. I'm gonna move us on to our final\ntopic of the day, which is AlphaFold3. Um, AlphaFold3 is the technology\nthat drove the Nobel Prize Award- uh, Award-winning work in using\nAI to predict protein and its interactions with other models. And it's potentially this major\ntechnology for using AI to advance, uh, scientific research, pharmaceutical\ndevelopment, and, and otherwise. And, uh, if you haven't been\nfollowing the twists and turns on this story, it's been very interesting. So, so DeepMind, uh, originally\nreleased its paper and it said, look, if you're a researcher, you can get\naccess to the model, but it's going to be on our servers and it's going\nto be under very specific kind of licensing constraints, essentially. Um, there was this big outcry in the\nresearch community saying, well, if you do that, we can't reproduce the\nresearch and, you know, it's kind of offensive from a research standpoint. And after a lot of\npressure, DeepMind relented. Right? And so the big move of kind of the week\nis that DeepMind decided to take this major kind of groundbreaking technology\nand open source it to the world. And so I think maybe\nVyoma, I'll start with you. You know, I think one way of looking at\nthe story is this is super valuable stuff. Right. Like AlphaFold3 is like core technology\nthat you could imagine building this enormous new business on. Um, and DeepMind, uh, I guess apparently\nwas sort of bullied, you know, into releasing this model open source. And so, you know, I guess maybe I'll\njust kind of present the question was like, why would a company like\nDeepMind, um, want to give up this incredibly valuable trade secret? Like what, what is, what is\npressuring them to do that? What does that tell us about the space? Yeah, it's, it's press-. It's not pressuring, I feel, but\nit does, um, kind of pivot people to use AI in their industry. That's one of the key things that I've\nbeen saying, like when everyone says that, oh, there are, uh, researchers who said\nthat AlphaFold shouldn't, uh, do this. But there is another caveat to this as\nwell, which they say that they wanted, researchers wanted to get a hand, uh,\nhands on with this, why is open source technology so prevalent in this world? Why does everyone, um, like it? Because it kind of opens new\navenues, it helps create, uh, more IP because I'm pretty sure when you\nhave a strong technology like this, there'll be so many different creative\naspects which can be added to it. Imagine synthetic, uh, data\ngeneration for, um, pharmaceutical companies, et cetera. So that is one thing as well. But. I feel, uh, when AlphaFold did this\non their own servers or depended it on their own servers, it helps reduce the\ncomputational, uh, resource need as well. There might be researchers or\nlike universities or students who might not have that\ncomputational power to work on it. So I feel it is a blessing in disguise\nbecause now it's on their own servers. I know, um, it's a little bit of\nan advanced topic around IP, et cetera, but it does help everyone. It helps the future. Uh, to be, uh, at the future industry\nin this case because everyone will get a chance to build something and there\nare no stopping criteria such as not enough resources or computational needs. Yeah, I think that's a really good point. And I guess Vagner, I mean, you know,\nDeepMind had a number of reasons why they didn't kind of want to release\nthis to the public, you know, one of them was, well, we want to balance\nthe ability to open up new research, as you were saying, with our ability\nto kind of pursue this commercially. I know some people were also\ncommenting like, this technology could also be potentially used\nfor some, like, bad purposes. Like, once you start getting AI and\nbio, you know, you start to worry about, like, well, if a, you know,\nbad actor gets access to this stuff, what could they possibly do with it? Do you think those kinds of\nrisks are overblown here? Or do you, do you kind of worry\nthat this is kind of one trade off as we get more and more powerful\nmodels into the open source? Well, it's, it's interesting that, um,\nIt is related to another prize, right? Winning a project. And, and I think that we're going to\nsee more and more, uh, AI projects, AI technologists and their creators\nbeing, uh, uh, awarded this prize. And I think that the reproducibility\nand the transparency here, uh, play a key aspect, right? Because people want to know, okay, why\nthis is so valuable, why this is, uh, uh, But to your point, I think that it's. It's also a challenge because, right,\nit's open source, and then what, what, what can be done with that technology? Um, this is, is really hard to define,\nand, uh, well, to one aspect, they only open, uh, for, uh, researchers. So I think that that And he's a better\nway to deal and and you're making it transparent for other researchers, but\nuh, it's not open to everyone I think this is an interesting uh approach\nbecause uh to your point, I think that yeah I agree that could bring high risks\nto you other uses of this technology. That's right. Yeah, i'm looking for I don't\nknow kind of like it feels like everybody's struggling for a good\nlike answer to this question, right? Which is we, we know we have all\nthese benefits of open source. We want to preserve all these\nbenefits of open source. And it feels like companies have\nbeen like, well, we'll give you a license that you need to sign. And maybe that's one\nway we'll deal with it. I know the other one has been, oh,\nwell, we'll fine tune these models. So they'll be necessarily safe. But you know, the history of jailbreaks\nis that these models get jailbroken. And Vyoma maybe I'll give\nyou the last word here. Uh, it's fascinating to think that like,\nwe may already be moving from one era of open source and AI to another, right? Like maybe 2025 is this inflection point. point. Um, do you have predictions? Like where, where does this\nall go for 2025 in open source? First thing, like DeepMind doing this,\nbut then we, I think we spoke in this, uh, about this in a previous podcast\nabout watermarking, uh, technology called SynthID by Google as well. So imagine all the information that\nhas been coming out from AlphaFold and that starts getting watermarked. So I feel Google's thinking\nleaps and bounds ahead. They have already created a structure\nwhich is going to help them. Stop these processes in which\nIP is being misused or sensitive information leaking out. So I feel that is happening. And one of the great caveats that I see\nhappening in the future is how do you govern each aspect of a generative AI\nor a machine learning production flow? Not only a prompt, not only\na model, but even your agent. Each information coming out of an agent\nbeing monitored by a particular governance model or a governance structure, I\nfeel there's going to be a multifaceted governance structure or a platform coming\nup, which will not only have rules, regulations, ethics, responsible AI,\nbut success metrics for each different task, which I feel someone or some place\nwill come up like a company or like a board, which will have the final say\nin token on these processes needs to be governed with these different structures\nand these different, uh, policies. The actual evaluation of each step\naround the agent governance around how an agent is picking up tool\nand stuff, that is available today. We, we do when we are rolling out\nproduction agents, uh, flows for clients. We do have enough measures in place. I think the, the, the extension that\nyou're, that you're recommending is these will become, will make their\nways into the AI regulation as well. Today, AI regulations are\nlooking at overall, uh, overall application and the use of AI. And yes, you're thinking that AI\nregulations will become much more precise on going further, further\ndown into the, into the how. Mechanics and stuff, right? Exactly. So the question is that we follow\nand the one that we feel sure with is like, is your model going to be biased\nagainst a particular group, et cetera? Very, very, um, generic. They are not specific to the\ntask or specific to the problem that we're trying to solve. Exactly what you said. That's something that I envision\nhappening in the future. Well, as always, a lot to\nlook forward to in 2025. I don't think we will\nbe short of any stories. Um, and even the kind of close of\nthe year is going to be a little bit crazy with all these conferences. Um, so I think that's all\nthe time we have for today. Thanks for joining us,\nVagner, Vyoma, Shobhit. As always, it's great having\nyou all on the show and hope to have you back sometime. And for all you listeners, if you\nenjoyed what you heard, you can get us on Apple Podcasts, Spotify,\nand podcast platforms everywhere. And we'll see you next week for\nanother episode of Mixture of Experts."
}