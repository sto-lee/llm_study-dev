{
  "video_url": "https://www.youtube.com/watch?v=MluoD8Z1ARQ",
  "video_id": "MluoD8Z1ARQ",
  "title": "Responsible AI: why businesses need reliable AI governance",
  "upload_date": "20240514",
  "channel": "IBM Technology",
  "duration": "26:58",
  "caption": "Malcom Gladwell: Hello, hello. Welcome to Smart \nTalks with IBM, a podcast from Pushkin Industries,   iHeartRadio and IBM. I’m Malcolm Gladwell.\nThis season, we’re continuing our conversation   with New Creators— visionaries who \nare creatively applying technology in   business to drive change—but with a focus \non the transformative power of artificial   intelligence and what it means to leverage AI as \na game- changing multiplier for your business.  Our guest today is Christina Montgomery, IBM’s \nChief Privacy & Trust Officer. She’s also chair of   IBM’s AI Ethics Board. In addition to overseeing \nIBM’s privacy policy, a core part of Christina’s   job involves “AI governance”—making sure the way \nAI is used complies with the international legal   regulations customized for each industry.\nIn today’s episode, Christina will explain   why businesses need foundational principles when \nit comes to using technology, why AI regulation   should focus on specific use cases over the \ntechnology itself, and share a bit about her   landmark congressional testimony last May.\nChristina spoke with Dr. Laurie Santos,   host of the Pushkin podcast The Happiness \nLab. A cognitive scientist and psychology   professor at Yale University, Laurie is an \nexpert on human happiness and cognition.  Ok! Let’s get to the interview. Laurie Santos: So Christina, I'm so excited to talk to you today. So let's start by talking \na little bit about your role at IBM. What does   a chief privacy and trust officer actually do? Christina Montgomery: Yeah, it's a really dynamic profession. And it's not a new profession, but the  role has really changed. I mean, my role today is   broader than just helping to ensure compliance \nwith data-protection laws globally. I'm also   responsible for AI governance. I cochair our AI \nEthics Board here at IBM on—for data clearance,   and data governance as well, for the company.\nSo I have both a compliance aspect to my   role—really important on a global basis—but also \nhelp the business to competitively differentiate,   because really, trust is a strategic advantage \nfor IBM and a competitive differentiator, as a   company that's been responsibly managing the most- \nsensitive data for our clients for more than a   century now, and helping to usher new technologies \ninto the world with trust and transparency. And   so that's also a key aspect of my role. Laurie Santos: And so—you joined us here on Smart Talks back in 2021, and you chatted with us about IBM's approach of building trust   and transparency with AI. And that was only two \nyears ago, but it almost feels like an eternity   has happened in the field of AI since then. And \nso I'm curious: How much has changed since you   were here last time? The things you told us \nbefore, are they still true? How are things—  Christina Montgomery: You're absolutely \nright. It feels like the world has changed,   really, in the last two years. But the same \nfundamental principles and the same overall   governance apply to the IBM program, for \ndata protection and responsible AI that we   talked about two years ago, and—not much \nhas changed there from our perspective.  And the good thing is, we've put these practices \nand this governance approach into place,   and we have an established way of looking at these \nemerging technologies as the technology evolves.   The tech is more powerful, for sure. Foundation \nmodels are vastly larger and more capable,   and are creating, in some respects, new issues, \nbut that just makes it all the more urgent to   do what we've been doing and to put trust and\ntransparency into place across the business—to   be accountable to those principles. Laurie Santos: And so our conversation today is really centered around this need for \nnew AI regulation. And part of that regulation   involves the mitigation of bias. And this is \nsomething I think about a ton as a psychologist,   right? I know, my students and everyone who's \ninteracting with AI is, is assuming that the,   the kind of knowledge that they're getting \nfrom this kind of learning is accurate, right?  But of course, AI is only as good as \nthe knowledge that's going in. And   so talk to me a little bit about, why \nbias occurs in AI and the level of the   problem that we're really dealing with.\nChristina Montgomery: Yeah. I mean—well,   obviously AI is based on data, right? It's trained \nwith data, and that data could be biased in and of   itself. And that's where issues could come up. \nThey come up in the data. They could also come   up in the output of the models themselves.\nSo it's really important that you build bias   consideration and bias testing into your product \ndevelopment cycle. And so what we've been thinking   about here at IBM, and doing—we had—some of our \nresearch teams, uh, delivered some of the very   first tool kits to help detect bias years ago \nnow, right? And deployed them to open source.  And we have put into place for our \ndevelopers here at IBM an “ethics by   design” playbook that's a sort of a step-by-step \napproach, which also addresses very fully bias   considerations. And we provide not only, \nlike, “Here’s a point when you should test   for it and you consider it in the data.”\nYou have to measure it both at the data   and the model level or the outcome level. And we \nprovide guidance with respect to what tools can   best be used to accomplish that. So it's a really \nimportant issue. It's one you—you can't just talk   about. You have to provide, essentially, \nthe technology and the capabilities and   the guidance to enable people to test for it.\nLaurie Santos: Recently you had this wonderful   opportunity to head to Congress to talk about \nAI. And in your testimony before Congress, you mentioned that it's often said that innovation \nmoves too fast for government to keep up.  And this is something that I also \nworry about as a psychologist,   right? Are policymakers really understanding \nthe issues that they're dealing with? And so I'm   curious how you're approaching this challenge \nof adapting AI policies to keep up with the   sort of rapid pace of all the advancements \nwe're seeing in the AI technology itself.  Christina Montgomery: It gets really critically \nimportant that you have foundational principles   that apply to not only how you use \ntechnology, but whether you're going   to use it in the first place and where you're \ngoing to use and apply it across your company.  —and then your program, from a governance \nperspective, has to be agile. It has to be   able to address emerging capabilities, \nnew training methods, et cetera. And   part of that involves helping to educate and \ninstill and empower a trustworthy culture at   a company so you can spot those issues—so you \ncan ask the right questions at the right time.  If we talked about, during the Senate hearing, \nand—and IBM's been talking for years about   regulating the use, not the technology itself, \nbecause if you try to regulate technology, you're   very quickly going to find out, um, regulation \nwill absolutely never keep up with that.  Laurie Santos: In your testimony to Congress, \nyou also talked about this idea of a “precision   regulation approach” for AI. Tell me more about \nthis. What is a precision regulation approach,   and why could that be so important?\nChristina Montgomery: It's funny,   because I was able to share with Congress our \nprecision regulation point of view in 2023,   but that precision regulation point of view was \npublished by IBM in 2020. So we have not changed   our position that you should apply the tightest \ncontrols, the strictest regulatory requirements,   to the technology where the end use and \nrisk of societal harm is the greatest.  So that's essentially what it is. There's \nlots of AI technology that's used today   that doesn't touch people—that's \nvery low risk in nature. And even when you think about AI that delivers a movie \nrecommendation versus AI that is used to diagnose   cancer, right? There's very different implications \nassociated with those two uses of the technology.  And so essentially what precision regulation \nis “Apply different rules to different risks,”   right? More-stringent regulation to the use \ncases with the greatest risk. And then also   we build that out, calling for things like \ntransparency. You see it today with content,   right? Misinformation and the like.\nWe believe that consumers should always   know when they're interacting with an AI system. \nSo: be transparent. Don't hide your AI. Clearly   define the risks. So as a country, we need to have \nsome clear guidance, right? And globally as well,   in terms of which uses of AI are higher risk, \nwhere we'll apply higher and stricter regulation,   and have sort of a common understanding \nof what those high-risk uses are,   and then demonstrate the impact in \nthe cases of those higher-risk uses.  So companies who are using AI in spaces \nwhere they can impact people's legal rights,   for example, should have to conduct an impact \nassessment that demonstrates, you know, that   the technology isn't biased. So we've been \npretty clear about “Apply the most-stringent   regulation to the highest- risk uses of AI.”\nLaurie Santos: So far, we've been talking   about your congressional testimony in terms \nof, you know, the specific content that you   talked about. But I'm just curious on a personal \nlevel, what was that like, right? Like right now,   it feels like at a policy level, like there's a \nkind of fever pitch going on with AI right now.  You know, what did that feel like, to kind \nof really have the opportunity to talk to   policymakers and sort of influence what \nthey're thinking about AI technologies,   like in the coming century, perhaps?\nChristina Montgomery: It was really an   honor to be able to do that, and to be one of the \nfirst set of invitees to the first hearing. And   what I learned from it essentially is, really \ntwo things. The first is really the value of   authenticity. So both as an individual and as \na company, I was able to talk about what I do. I didn't need a lot of advance prep, \nright? I, I talked about what my job is,   what IBM has been putting in place for years now. \nSo this isn't about creating something. This was   just about showing up and being authentic. And we \nwere invited for a reason. We were invited because   we were one of the earliest companies in the AI \ntechnology space. We're the oldest technology   company, and we are trusted, and that's an honor.\nAnd then the second thing I came away with was   really how important this issue is to society. \nI don't think I appreciated it as much until,   following that experience, I had outreach from \ncolleagues I hadn't worked with for years.  I had outreach from family members who \nheard me on the radio, my mother and my   mother-in-law and my nieces and nephews and \nmy— friends of my kids were all like, “Oh,   I get it. I get what you do now. Wow. That's \npretty cool.” You know, so that was really,   the best and most impactful takeaway that I had. Malcom Gladwell: The mass adoption of generative AI happening at breakneck speed has spurred societies and governments   around the world to get serious \nabout regulating AI. For businesses,   compliance is complex enough already. But \nthrow an ever-evolving technology like AI   into the mix and compliance itself \nbecomes an exercise in adaptability.  As regulators seek greater accountability in \nhow AI is used, businesses need help creating   governance processes that are comprehensive enough \nto comply with the law but agile enough to keep up   with the rapid rate of change in AI development.\nRegulatory scrutiny isn’t the only consideration,   either. Responsible AI governance—a business’s \nability to prove its AI models are transparent   and explainable—is also key to building \ntrust with customers, regardless of industry.  In the next part of their conversation, Laurie \nasks Christina what businesses should consider   when approaching AI governance. Let’s listen.\nLaurie Santos: So what's the particular   role that businesses are playing in AI \ngovernance? Like, why is it so critical   for businesses to be part of this? Christina Montgomery: I think it's really critically important that businesses understand the impacts that technology can have,   both in making them better businesses—but \nthe impacts that those technologies can   have on the consumers that they are supporting.\nBusinesses need to be deploying AI technology   that is in alignment with the goals that \nthey set for it and that can be trusted.   I think for us and for our clients, a \nlot of this comes back to trust in tech.  If you deploy something that doesn't work, \nthat hallucinates, that discriminates,   that isn't transparent, where decisions can't \nbe explained, then you are going to very rapidly   erode the trust—at best, right?—of your clients. \nAnd at worst, for yourself; you're going to create   legal and regulatory issues for yourself as well.\nSo trust in technology is really important.   And I think there's a lot of pressure on \nbusinesses today to move very rapidly and   adopt technology. But if you do it without \nhaving a program of governance in place,   you're really risking eroding that trust. Laurie Santos: And so this is really where I think a strong AI governance comes in. \nYou know—talk about, from your perspective,   how this really contributes to \nmaintaining the trust that customers   and stakeholders have in these technologies. Christina Montgomery: Yeah, absolutely. I mean, you need to have a governance program because  you need to understand, that the technology,   particularly in the AI space that you \nare deploying, is explainable. You need   to understand why it's making decisions and \nrecommendations that it's making, and you   need to be able to explain that to your consumers.\nI mean, you can't do that if you don't know where   your data is coming from; what data you're using \nto train those models; if you don't have a program that manages the alignment of your \nAI models over time to make sure—as   AI learns and evolves over uses, which is in \nlarge part what makes it so beneficial—that   it stays in alignment with the objectives \nthat you set for the technology over time.  So you can't do that without a robust governance \nprocess in place. So we work with clients to share   our own story here at IBM in terms of how we \nput that in place, but also in our consulting   practice, uh, to help clients work with these new \ngenerative capabilities and foundation models and   the like, in order to put them to work for their \nbusiness in a way that's going to be impactful to   that business, but at the same time be trusted. Laurie Santos: And so now I wanted to turn a little bit towards watsonx.governance. And—so IBM recently announced their AI platform, watsonx,   which will include a governance \ncomponent. Could you tell us a   little bit more about watsonx.governance? Christina Montgomery: Yeah. I mean, before I do that, I'll just back up and talk about  the full platform, and then lean into watsonx,   because I think it's important to understand \nthe delivery of a full suite of capabilities.   To get data, to train models, and then to govern \nthem over their life cycle—all of these things   are really important. From the onset, you need \nto make sure that you have—for our watsonx.ai,   for example; that's the studio to train \nnew foundation models and generative AI   and machine-learning capabilities, and we are \npopulating that studio with some IBM-trained   foundation models, which we're curating and \ntailoring more specifically for enterprises.  So that's really important. It comes back to the \npoint I made earlier about business trust and   the need,to have enterprise-ready technologies \nin the AI space. And then the watsonx.data is   a fit-for-purpose data store or a data lake, \nand then watsonx.gov. So that's a particular   component of the platform that my team and the \nAI Ethics Board has really worked closely with   the product team on developing. And we're using \nit internally here in the chief privacy office   as well to help us govern our own uses of \nAI technology and our compliance program   here. And it essentially helps to notify \nyou if a model becomes biased or gets out   of alignment as you're using it over time. So \ncompanies are going to need these capabilities. I mean, they need them today \nto deliver technologies with   trust. They'll need them tomorrow to comply \nwith regulation, which is on the horizon.  Laurie Santos: I think compliance becomes even \nmore complex when you consider international   data-protection laws and regulations. Honestly, \nI don't know how anyone on any company's legal   team is keeping up with this these days. But my \nquestion for you is really, “How can businesses   develop a strategy to maintain compliance and to \ndeal with it in this ever- changing landscape?”  Christina Montgomery: Increasingly more \nchallenging. In fact, I saw a statistic   just this morning that the regulatory obligations \non companies have increased something like 700   times in the last 20 years. So, it really is \na huge focus area for companies. You have to   have a process in place in order to do that.\nAnd it's not easy, particularly for a company   like IBM, that has a presence in over 170 \ncountries around the world. There's more   than 150 comprehensive privacy regulations. \nThere are regulations of nonpersonal data.   There are AI regulations emerging. So you \nreally need an operational approach to it,   in order to stay compliant.\nBut, but one of the things we do   is we set a baseline—and a lot of companies do \nthis as well. So we define a privacy baseline,   we define an AI baseline, and we ensure, then, \nas a result of that, there are very few deviances   because it incorporates that baseline.\nSo that's one of the ways we do it. Other   companies, I think, are similarly situated in \nterms of doing that. But, again, it is a real   challenge for global companies. It's one of the \nreasons why we advocate for as much alignment as   possible—in the international realm as well as \nnationally here in the U.S.—as much alignment as   possible to make compliance—easier—and not just \nbecause companies want an easy way to comply,   but the harder it is, the less likely \nthere will be compliance. And it's   not the objective of anybody—governments, \ncompanies, consumers—to have to set legal   obligations that companies simply can't meet. Laurie Santos: So what advice would you give to other companies who are looking to rethink or strengthen their approach to AI governance? Christina Montgomery: I think you need to start \nwith, as we did, foundational principles. And   you need to start making decisions about \nwhat technology you're going to deploy,   and what technology you're not, what are you \ngoing to use it for and what aren't you going to   use it for. And then when you do use it, align \nto those principles. That's really important.  Formalize a program. Have someone within \nthe organization—whether it's the Chief   Privacy Officer, whether it's some other \nrole, a Chief AI Ethics Officer—but have   an accountable individual, an accountable \norganization. Do a maturity assessment,   figure out where you are and where you need to \nbe, and really start, putting it into place today.   Don't wait for regulation to apply directly \nto your business because it'll be too late.  Laurie Santos: So as Smart \nTalks features New Creators,   these visionaries like yourself who are creatively \napplying technology in business to drive change,   I'm curious if you see yourself as creative. Christina Montgomery: I definitely do. I mean, you need to be creative when you're working in an industry that evolves so very quickly.   So you know, I started with IBM when \nwe were primarily a hardware company,   right? And we've changed our business \nso significantly over the years. And   the issues that are raised with respect to \neach new technology—whether it be cloud,   whether it be AI, now, where we're seeing a \nton of issues, or you look at emergent issues,   in the space of things like neuro technologies \nand quantum computers—you have to be strategic   and you have to be creative and thinking about \nhow you can adapt agilely, quickly, a company to   an environment that is changing so quickly. Laurie Santos: And with this transformation happening at such a rapid pace, \ndo you think creativity plays a   role in how you think about and implement, \nspecifically, a trustworthy AI strategy?  Christina Montgomery: Yeah. I absolutely think \nit does. Because again, it comes back to   these capabilities. And, there are ways, I guess how \nyou define “creativity” could be different,   right? But I'm thinking of creativity in the \nsense of, sort of agility and strategic vision and   creative problem-solving. I think that's really \nimportant in the world that we're in right now,   being able to creatively problem solve with \nnew issues that are rising sort of every day. Laurie Santos: And so how do you see the role \nof Chief Privacy Officer evolving in the future   as AI technology continues to advance? Like, \nwhat steps should CPOs take to stay ahead of   all these changes that are coming their way? Christina Montgomery: So the role is evolving, in most companies, I would say, pretty rapidly. Many companies are looking to   chief privacy officers, who already understand \nthe data that's being used in the organization   and have programs to ensure compliance with laws \nthat require you to manage that data in accordance   with data-protection laws and the like.\nIt's a natural place and position for,   AI responsibility. And so I think what's \nhappening to a lot of chief privacy officers is   they're being asked to take on this AI-governance \nresponsibility for companies,—and if not take it   on, at least play a very key role working with \nother parts of the business in AI governance.  So that really is changing. And if Chief \nPrivacy Officers are in companies who maybe   haven't started thinking about AI yet, \nthey should, so I would encourage them   to look at different resources that are available \nalready in the AI-governance space. For example,   the International Association of Privacy \nProfessionals—which is the 75,000-member   professional body for the profession of chief \nprivacy officers—just recently launched,   an AI- governance initiative on—an AI-governance \ncertification program. I sit on their advisory   board. But that's just emblematic of the \nfact that the field is changing so rapidly.  Laurie Santos: And so, speaking of rapid \nchange—when you were back here on Smart Talks   in 2021, you said that the future of AI will \nbe more transparent and more trustworthy.What   do you see the next five to 10 years holding? \nYou know, when you're back on Smart Talks in,   you know, 2026, you know, 2030, you know, \nwhat are we going to be talking about   when it comes to AI technology and governance? Christina Montgomery: So I try to be an optimist, right? And I said that two years ago, and I think we're seeing it now come into   fruition. And there will be requirements, \nwhether they're coming from the U.S.,   whether they're coming from Europe, whether \nthey're just coming from voluntary adoption by clients of things like the NIST risk-management \nframework, a really important voluntary framework,   you're going to have to adopt transparent \nand explainable practices in your uses of AI.  So I do see that happening. And in the next \nfive to 10 years, boy, I think we'll see more   research into trust in, in techniques, because we \ndon't really know for example, how to watermark.   We were calling for things like watermarking; \nthere'll be more research into how to do that.  I think you'll see regulation that's specifically \ngoing to require those types of things. So I   think—again, I think the regulation is going \nto drive research. It's going to drive research   into these areas that will help ensure that we can \ndeliver new capabilities, generative capabilities   and the like, with trust and explainability. Laurie Santos: Thank you so much, Christina, for joining me on Smart Talks to talk about AI and governance.  Christina Montgomery: Well, thank \nyou very much for having me.  Malcolm Gladwell: To unlock the transformative growth possible with artificial intelligence, businesses need to know   what they wish to grow into first. Like Christina \nsaid, the best way forward in the AI future is for   businesses to figure out their own foundational \nprinciples around using the technology,   drawing upon those principles to apply AI in \na way that’s ethically consistent with their   mission and complies with the legal frameworks \nbuilt to hold the technology accountable.  As AI adoption grows more and more widespread, \nso too will the expectation from consumers and   regulators that businesses use it responsibly. \nInvesting in dependable AI governance is a way for   businesses to lay the foundations for technology \ntheir customers can trust, while rising to the   challenge of increasing regulatory complexity.\nThough the emergence of AI does complicate   an already tough compliance landscape, \nbusinesses now face a creative opportunity to set a precedent for what accountability in \nAI looks like and to rethink what it means   to deploy trustworthy artificial intelligence.  I’m Malcolm Gladwell.\nThis is a paid advertisement from IBM.  Smart talks with IBM will \nbe taking a short hiatus,   but look for new episodes in the coming weeks.\nSmart Talks with IBM is produced by Matt Romano,   David Zha, Nisha Venkat, and Royston \nBeserve, with Jacob Goldstein. We’re   edited by Lidia Jean Kott. Our engineer is \nJason Gambrell. Theme song by Gramoscope.  Special thanks to Carly Migliori, Andy Kelly, \nKathy Callaghan, and the EightBar and IBM teams,   as well as the Pushkin marketing team.\nSmart Talks with IBM is a production   of Pushkin Industries and Ruby Studio at \niHeartMedia. To find more Pushkin podcasts,   listen on the iHeartRadio app, Apple \nPodcasts, or wherever you listen to podcasts."
}