if i input myself into an autoencoder it can create a pretty good al though not perfect reconstruction of me why don't i uh well i mean you explain sure so look auto encoders are an unsupervised neural network and they consist of two parts let's let's take a look there is first of all an encoder that's the first layer and that takes in some input and learns how to efficiently compress and encode that data into something that we call the code then we have a decoder that learns how to reconstruct that encoded data representation so decoder and that creates output and that output is as similar to the original input data as possible effectively an autoencoder learns to recognize which aspects of observable data are relevant and limit noise in data that can be discarded separate the signal from the noise okay great so is this is this all about creating smaller file sizes like the way i'll zip up some documents or compressor video no not at all so let's talk about some examples convolutional autoencoders have a variety of use cases related to images so for example i can draw an image like this number three and then through a process called feature extraction i can derive the required features of the image by removing noise something that looks a bit more like this and then i'm able to generate an output that approximates the original it's not exactly the same but it's pretty close now i can use this part called the code to do other things like create a higher resolution version of the output image or i can colorize an image so black and white input full color output now in this case the input and the output they look much the same which well that's what autoencoders are all about but they don't have to be we can provide input to an auto encoder in a corrupted form like the noisy image of a of a 3 and then train a denoising auto encoder to reconstruct our original image from the noisy version of it once we've trained our auto encoder to remove noise from a representation of a number or a picture or park bench we can apply that to all sorts of objects within an object element that displayed the same noise pattern so let's take a closer look inside an auto encoder the encoder itself compresses the input into a latent space representation so we have multiple layers here that represent the encoder each one a little smaller than the other so this part here that's the encoder the most compressed version we said that's called the code that's also known as the bottle neck because i suppose much like the neck of a bottle that's the most compressed part then we have the decoder which is reconstructed from that bottleneck that's the decoder and it's reconstructed from the latent space representation to generate the output by learning what makes up the signal and what makes up the noise we also have the ability to detect when something is not part of the status quo and that's anomaly detection anomalies are a significant deviation from the general behavior of the data and auto encoders are very good at telling us when something doesn't fit as a result auto encoders are widely used in anomaly detection in things such as fault fraud and intrusion detection so auto encoders are a great way of extracting noise recognizing relevant features and detecting anomalies a pretty handy toolbox for dealing with all sorts of data and eerily similar clones if you have any questions please drop us a line below and if you want to see more videos like this in the future please like and subscribe thanks for watching