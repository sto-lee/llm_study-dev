{
  "video_url": "https://www.youtube.com/watch?v=fJ40w_2h8kk",
  "video_id": "fJ40w_2h8kk",
  "title": "What is PyTorch? (Machine/Deep Learning)",
  "upload_date": "20231011",
  "channel": "IBM Technology",
  "duration": "11:57",
  "caption": "PyTorch has emerged as the de facto standard for \nmachine learning and deep learning. And I know a little bit about PyTorch, but I've brought in an \nexpert, Sahdev Zala, to teach us all more about PyTorch. So, Sahdev, what is PyTorch? Hi Brad! So it's a framework for machine learning and deep learning. And what I mean by that is you can use PyTorch to build your models because it provides you all of the building blocks. It provides you all the functionalities\nto run faster training on that model. And it's an open source project under PyTorch Foundation, \nwhich is part of the Linux Foundation. So there is a dynamic community behind the project. Oh, great! So it's got an ecosystem and it's in the Foundation. So that means you're going to have open governance\nand a level playing field. That's wonderful. Well, Sahdev, can you tell \nme about the key features of PyTorch? Yeah, sure. That's a great question. So let me just mention \nthe common steps of model training. So first, you need to prep your data, your data set for training. And, ideally you also want to do it for testing. And then, the other steps is you're \ngoing to build your model. And you're going to train it. And as I mentioned, you're going to test. Okay. So those look like some\npretty straightforward features. Why don't you tell me about the first one? \nWhat do you mean by prepping the data? Right. So, the data says are you going to use \nfor your model, maybe small as you're learning it, but for larger models, these data sets can \nbe huge--10 terabytes, petabytes wide. So how do you use this data\nto train your model and then test it? So PyTorch provides you two things here. Data sets and data loader classes that help \nyou to easily feed this data for your training and testing. Okay. How does this help me?\nDoes it speed things up? Well, that's a good question. So, it helps you to download \nthe data to make it accessible for your training and testing. And this data loader, it provides you iterator over this \ndata so that you can use them to train in a batch. Because you're not going to just feed one data \nat a time. You're going to train using the batch sizes that you want. It also provides you other things like shuffling the data. You don't want to just feed the data in an order so that your model, \nit's only memorizing the data versus versus it's learning. So this will shuffle for you as well. \nIt has other features as well. Very nice. Well, it also helps you to build models?  Absolutely. So, once you think you're ready with your preparation using PyTorch because it takes you all the takes \ncare of all the complexity, the next step would   be and building the model to define your models \nand for that what you need is layers because it's   a deep learning, it's made of multiple layers. \nSo you need different layers like linear layer or   combination layer. And there are many others that \nare provided by partners to you. And that are also   things like, besides layers, that are activation \nfunctions that you'll be using to add nonlinearity   to your model-- that's also provided to you by \nPyTorch. So you don't have to do anything but   just to call those functions.   What do you mean by nonlinearity? So, in general, when you train the model and then-- it's a \nmathematical term, right, linear as well,   but it will if you don't get nonlinear, you \nbasically just get like a one straight line.   And in real life, not everything is just \nchanging in X will be same as changing your Y   output. So it adds you that nonlinearity for \nyou. And the next step would be training and there   basically I can talk more about the training side, Brad.  Well, so tell me about features-- \nwhat  does it do to help you train?  So training will require to use the loss function. And loss function   is basically to find out the loss that you going \nto have. As when you run this model like   a forward pass from the input and you get some \noutput. Well, I'm not going to have the correct output   every time magically, there's no magic there. So \nyou can have lots of parameters in between, you are just going to randomize them in the beginning, \nyou got some output, but then you're going to   have a loss function to calculate the loss from \nthe desired output. So your want your model to reach a certain expectation. \nAnd typically during the training process that model's falling short   and you're seeing how much it's falling short \nfrom where you want it to be. Exactly. So that loss functions, there are multiple loss\nfunctions and PyTorch provides it to you again. You again   you call them according to your need for \nthe model. Once you have the loss function used,   the next big thing is finding the gradient of this \nloss with regards to your parameters. So PyTorch   provides the backward propagation for you, or, \nauto-grade features. That is by far one of the   most popular feature of PyTorch, that it will \ncalculate the gradient for you.  So if we all think back from our calculus days, gradients are \nthis piece that helps you to tweak and   get the model the way you want it and it's got \nit built-in for you.  Exactly. So once you got the gradient, you basically\nrun the optimizer function just to step over, which is again   provided by PyTorch to you. And like you exactly \nsaid, you're going to tweak the parameters, you're   going to optimize it to reach to a level in a \nnumber of iterations. So that you basically define   those iterations. But the number of iterations \nyou're going to reach to a level where we're like,   you know what, that should be enough training. \nI do like 3x, 5x iterations, and at that point   you are ready to test it.  And is that a big deal for these models to have to do testing or I just  test once I'm done? Or is it more more than that?  Yeah. So the next step would be no. From here to the test side. You need to test it. Ideally it's optional. But as part of testing, PyTorch provides   a function, an eval evaluation. So you can \nevaluate your model. And at that point, you're not   going to calculate the gradient, you're not going \nto find the loss function. You basically just do   the forward pass. You see what you're getting. And \nif you're happy with it, then pretty much ready   to use the model. If you're not, then you're \ngoing to do the further training. And again,   this data sets, which I mentioned earlier, that \nwould be used for training useful test, white or   black are two different datasets. So as part of the testing. I'm getting to decide, hey, \nis my model good enough? I think I'm ready to go with it. Pretty much, yeah.  Well, it all seems a little complicated to me.\nIs PyTorch really easy to use?  Well, yes,  that's one of the best things I love about PyTorch. \nIt's easy to get started. It's easy to install.   It's easy to use because it's Pythonic; the \"Py\" \nin PyTorch is for Python. So you know how much   data scientists just love Python. Absolutely. \nPyTorch is in Python. And it's been easily I use   by data scientists. And if someone if they don't \nknow Python, they can learn it quickly as well.   PyTorch.org provides a lot of good documentation, \ntutorials that will help you to get started very   quickly and it's also flexible. So I mentioned \nthe training on your right. You can run training,   you can run your PyTorch on CPU just \nusing the tensor that PyTorch uses in   data structure (multi-dimensional \narrays). They can be run on CPUs,   they can be run on GPUs, they can do the training \non multiple CPU and GPU on a single machine,   you can do that on a distributed environment on \nmultiple machines, multiple GPUs. And you can   like say part of that you can just run PyTorch on \nyour laptop and play with it. There is also like   a mobile development going on to to help PyTorch \non your mobile devices.  So yeah, it's a lot of  options. Supports a lot of platforms. GPUs. \nCPUs. Well, what if I want to be a contributor?   Well, that's great question. Something I love as \nan contributor myself, so it's actually very easy.   PyTorch is part of PyTorch Foundation as I \nmentioned. There's a dynamic community behind it,   very friendly. Lots of people are going to help \nyou to get started, to contribute. As long as you   sign the CLA, follow the code of conduct, these \nare things to do. You are ready to contribute.   The community also provides weekly office hours. \n  Office hours, that's huge. I can come in as\na new person and say, hey, could you help me out or can you give me an easy first item to work on?\nI could do that in an office hours.  Yes, exactly. And there are things like you can easily\nfind the good first issues. You can find the document issues to \n  get started with and you can ask questions. \nAnd the office hours, through their Slack channel is another one.  And one of the classic tips is when you join a new project, \nask for a mentor and ask them to put you to work on something. Because   when they put you to work on something, they're \ngoing to be very interested in what you're doing   and they're going to give you timely reviews and \nanswer all your questions. So tell me more about   how IBM is contributing to PyTorch. Yeah, sure. \nWell, IBM is contributing to PyTorch in a big way, like IBM always do. By using PyTorch, so we are \ngoing to contribute to help the community, grow   the community. And a part of that, we working on \nmany different things, something called like FSDP,   Fully Sharded Data Parallel, well, an advanced topic, \nbut it helps you to shard the model parameters   across multiple GPUs across multiple machines\nfor fast training and for your   large models they may not fit in like a single GPU \nor CPU. And so we are contributing there. There's   really good blog posts out there. Just search \nfor it, \"IBM FSDP PyTorch\" wiil find it quickly.   Highly recommend to read it. We also provide \nimprovements in the storage site for training,   compiler optimizations. And besides that benchmarking,\n test side improvements and documentation.   And we have multiple developers working in the \ncommunity.  So it sounds like there's lots of nice features to help it\nsupport those large foundation models, supporting multiple GPUs   and running in a distributed fashion. And \na lot of work being done for benchmarking,   seeing how fast things are running and obviously \na lot of work in the documentation to help others   get started. It's a fabulous. It is. It's amazing. I'm so glad to be part of the community.  Well, thank you, Sahdev. I've learned a lot today. This \nis fabulous. We hope that you've learned a lot   about PyTorch and we encourage you to come \njoin the community. We really enjoy working   on PyTorch and pushing forward with your deep \nlearning/machine learning initiatives.   Thanks for watching our video. And don't forget, if you \nliked it, remember to hit like and subscribe."
}